[["index.html", "Bienvenue", " Notes de cours GMQ405 : Modélisation et analyse spatiale Philippe Apparicio, département de géomatique appliquée, Université de Sherbrooke Bienvenue Version : 20-07-2023 Auteur : Philippe Apparicio Département de géomatique appliquée, Université de Sherbrooke Notes de cours GMQ405. Modélisation et analyse spatiale Baccalauréat en géomatique appliquée à l’environnement Microprogramme de 1er cycle en géomatique appliquée "],["préface.html", "Préface", " Préface Ce livre vise à décrire une panoplie de méthodes de modélisation et d’analyse spatiales dans le logiciel ouvert R. Il a d’ailleurs été écrit intégralement dans R avec rmarkdown. Le contenu est pensé pour être accessible à toutes et tous, même à celles et ceux n’ayant presque aucune base en statistique ou en programmation R. La philosophie de ce livre est de donner toutes les clefs de compréhension de méthodes de modélisation et d’analyse spatiale abordées dans le cours CMQ405 et de mise en œuvre dans R. La présentation des méthodes est basée sur une approche compréhensive et intuitive plutôt que mathématique. Bonne lecture! "],["sect001.html", "Un manuel sous la forme d’une ressource éducative libre", " Un manuel sous la forme d’une ressource éducative libre La licence de ce livre, CC BY-SA (figure 0.1), oblige donc à : Attribuer la paternité de l’auteur dans vos versions dérivées, ainsi qu’une mention concernant les grandes modifications apportées, en utilisant la formulation suivante : Apparicio, Philippe . 2023. Notes de cours GMQ405 : Modélisation et analyse spatiale. Baccalauréat en géomatique appliquée à l’environnement. Département de géomatique appliquée, Université de Sherbrooke. CC BY-SA (4.0). Utiliser la même licence ou une licence similaire à toutes versions dérivées. Figure 0.1: Licence Creative Commons du livre "],["sect002.html", "Comment lire ce manuel?", " Comment lire ce manuel? Le livre comprend plusieurs types de blocs de texte qui en facilitent la lecture. Bloc packages. Habituellement localisé au début d’un chapitre, il comprend la liste des packages R utilisés pour un chapitre. Bloc objectif. Il comprend une description des objectifs d’un chapitre ou d’une section. Bloc notes. Il comprend une information secondaire sur une notion, un élément, une idée abordée dans une section. Bloc pour aller plus loin. Il comprend des références ou des extensions d’une méthode abordée dans une section. Bloc astuce. Il décrit un élément qui vous facilitera la vie : une propriété statistique, un package, une fonction, une syntaxe R. Bloc attention. Il comprend une notion ou un élément important à bien maîtriser. Bloc exercice. Il comprend un court exercice de révision à la fin de chaque chapitre. "],["sect003.html", "Comment utiliser les données du livre pour reproduire les exemples?", " Comment utiliser les données du livre pour reproduire les exemples? À compléter "],["sect004.html", "Structure du livre", " Structure du livre À compléter. "],["sect005.html", "Pourquoi faut-il programmer en géomatique appliquée et utiliser R?", " Pourquoi faut-il programmer en géomatique appliquée et utiliser R? À compléter. "],["sect006.html", "Remerciements", " Remerciements De nombreuses personnes ont contribué à l’élaboration de ce manuel. À compléter. Nous remercions les membres du comité de révision pour leurs commentaires et suggestions très constructifs. Ce comité est composé de trois étudiantes et deux professeurs : À compléter. Finalement, nous remercions Denise Latreille, réviseure linguistique et chargée de cours à l’Université Sherbrooke, pour la révision du manuel. "],["chap01.html", "Chapitre 1 Manipulation des données spatiales dans R", " Chapitre 1 Manipulation des données spatiales dans R Dans ce chapitre, nous regardons comment importer, manipuler et cartographier des données spatiales dans R. Pour une description plus détaillée du langage de programmation R – objets et expression, opérateurs, structures de données (vecteurs, matrices, arrays, DataFrame), importation et manipulation de données –, lisez le chapitre intitulé Prise en main avec R (Apparicio et Gelb 2022). Dans ce chapitre, nous utilisons les packages suivants : Pour importer et manipuler des fichiers géographiques : sf pour importer et manipuler des données vectorielles. rmapshaper pour simplifier des géométries en conservant la topologie. terra pour importer et manipuler des données raster. gpx pour importer des coordonnées GPS au format GPS eXchange Format. Pour cartographier des données : tmap est certainement le meilleur package. ggplot2 est un package pour construire des graphiques qui peut être aussi utilisé pour visualiser des données spatiales. RColorBrewer pour sélectionner une palette de couleur. ggpurb pour combiner des graphiques et des cartes. Pour importer des tables attributaires : foreign pour importer des fichiers dBase. xlsx pour importer des fichiers Excel. Ce document comprend de nombreuses notions sur l’importation, la manipulation et la cartographie de données spatiales dans R, soit des opérations que vous avez l’habitude de réaliser dans ArcGIS Pro ou QGIS. Pas de panique! Prenez le temps de lire ce premier chapitre à tête reposée et assurez-vous de bien comprendre chaque notion avant de passer à la suivante. L’objectif n’est pas de maîtriser parfaitement la syntaxe R pour toutes les opérations dès la première semaine! Vous allez manipuler de nombreuses données spatiales avec R dans les prochaines séances de cours. Par conséquent, n’hésitez pas à revenir sur ce chapitre lorsque nécessaire; considérez-le comme un aide-mémoire. References "],["sect011.html", "1.1 Importation de données géographiques", " 1.1 Importation de données géographiques Quels packages choisir pour importer et manipuler des données spatiales? Pour les données vectorielles, il existe deux principaux packages (équivalent d’une librairie dans Python) : sp (Pebesma et Bivand 2005; Bivand, Pebesma et Gomez-Rubio 2013) et sf (Pebesma 2018). Le package sp est progressivement délaissé par R, il est donc fortement conseillé d’utiliser sf. Pour les données raster, il est possible d’utiliser les packages raster (Hijmans 2022a) et terra (Hijmans 2022b), dont le dernier, plus récent, semblerait plus rapide. En résumé, privilégiez l’utilisation de sf et de terra. Il convient d’installer les deux packages. Notez que l’installation d’un package requiert une connexion Internet car R accède au répertoire de packages CRAN pour télécharger le package et l’installer sur votre ordinateur. Cette opération est réalisée avec la fonction install.packages(\"nom du package\"). Notez qu’une fois que le package est installé, il est enregistré localement sur votre ordinateur et y reste à moins de le désinstaller avec la fonction remove.packages(\"nom du package\"). Autrement dit, il n’est pas nécessaire de les installer à chaque ouverture de R! Pour utiliser les fonctions d’un package, vous devez préalablement le charger avec la fonction library(\"Nom du package\") (équivalent à la fonction import de Python). Pour plus d’informations sur l’installation et le chargement de packages, consultez la section suivante (Apparicio et Gelb 2022). 1.1.1 Importation de données vectorielles La fonction st_read de sf permet d’importer une multitude de formats de données géographiques, comme des fichiers shapefile (shp), GeoPackage (GPKG), geojson (GeoJSON), sqlite (sqlite), geodatabase d’ESRI (FileGDB), Geoconcept (gxt), Keyhole Markup Language (kml), Geography Markup Language (gml), etc. 1.1.1.1 Importation d’un fichier shapefile Le code R ci-dessous permet d’importer des couches géographiques au format shapefile. Notez que la fonction list.files(pattern = \".shp\") renvoie préalablement la liste des couches shapefile présentes dans le dossier de travail. ## Chargement des packages library(&quot;sf&quot;) library(&quot;terra&quot;) library(&quot;tmap&quot;) library(&quot;ggplot2&quot;) library(&quot;ggpubr&quot;) library(&quot;foreign&quot;) library(&quot;xlsx&quot;) library(&quot;rmapshaper&quot;) library(&quot;RColorBrewer&quot;) ## Obtention d&#39;une liste des shapefiles dans le dossier de travail list.files(path = &quot;data/chap01/shp&quot;, pattern = &quot;.shp&quot;) ## [1] &quot;AbidjanPtsGPS.shp&quot; ## [2] &quot;AbidjanSegRue.shp&quot; ## [3] &quot;Arrondissements.shp&quot; ## [4] &quot;IncidentsSecuritePublique.shp&quot; ## [5] &quot;Installations_sportives_et_recreatives.shp&quot; ## [6] &quot;Pistes_cyclables.shp&quot; ## [7] &quot;PolyX.shp&quot; ## [8] &quot;PolyY.shp&quot; ## [9] &quot;Quebec.shp&quot; ## [10] &quot;Segments_de_rue.shp&quot; ## Importation des shapefiles avec sf Arrondissements &lt;- st_read(&quot;data/chap01/shp/Arrondissements.shp&quot;, quiet=TRUE) InstallationSport &lt;- st_read(&quot;data/chap01/shp/Installations_sportives_et_recreatives.shp&quot;, quiet=TRUE) PistesCyclables &lt;- st_read(&quot;data/chap01/shp/Pistes_cyclables.shp&quot;, quiet=TRUE) Rues &lt;- st_read(&quot;data/chap01/shp/Segments_de_rue.shp&quot;, quiet=TRUE) Regardons à présent la structure des couches importées. Pour ce faire, nous utilisons la fonction head(nom du DataFrame, n=2); notez que le paramètre n permet de spécifier le nombre des premiers enregistrements à afficher. Les informations suivantes sont ainsi disponibles : 6 fields : six champs attributaires (TYPE, DETAIL, NOM, SURFACE, ECLAIRAGE, OBJECTID). Geometry type POINT : le type de géométrie est point. Bounding box: xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536 : les quatre coordonnées définissant l’enveloppe de la couche. Projected CRS: WGS 84 / Pseudo-Mercator : la projection cartographique. Ici, une projection cartographique utilisée par Google Maps et OpenStreetMap. La géométrie est enregistrée dans le champ geometry. Pour le premier enregistrement, nous avons la valeur POINT (-8001939 5686891), soit un point avec les coordonnées géographiques (x,y) entre parenthèses. head(InstallationSport, n=2) # Visualisation des deux premiers enregistrements ## Simple feature collection with 2 features and 6 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536 ## Projected CRS: WGS 84 / Pseudo-Mercator ## TYPE DETAIL NOM SURFACE ECLAIRAGE OBJECTID ## 1 Aréna &lt;NA&gt; Aréna Eugène-Lalonde &lt;NA&gt; &lt;NA&gt; 1 ## 2 Aréna &lt;NA&gt; Aréna Philippe-Bergeron &lt;NA&gt; &lt;NA&gt; 2 ## geometry ## 1 POINT (-8001939 5686891) ## 2 POINT (-8009681 5696536) names(InstallationSport) # Noms de champs (colonnes) ## [1] &quot;TYPE&quot; &quot;DETAIL&quot; &quot;NOM&quot; &quot;SURFACE&quot; &quot;ECLAIRAGE&quot; &quot;OBJECTID&quot; ## [7] &quot;geometry&quot; View(InstallationSport) # Afficher l&#39;ensemble de la table attributaire Explorons les types de géométries et la projection des autres couches avec le code ci-dessous. En résumé, les types de géométries sont : Des géométries simples point : un seul point. linestring : une séquence de deux points et plus formant une ligne. polygon : un seul polygone formé par une séquence de points pouvant contenir un ou plusieurs polygones intérieurs formant des trous. Des géométries multiples multipoint : plusieurs points pour une même observation. multilinestring : plusieurs lignes pour une même observation. multipolygon : plusieurs polygones pour une même observation. Une collection de géométries (Geometrycollection) qui peut contenir différents types des géométries décrites ci-dessus pour une même observation. head(PistesCyclables, n=2) ## Simple feature collection with 2 features and 3 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -8010969 ymin: 5666202 xmax: -7997972 ymax: 5697954 ## Projected CRS: WGS 84 / Pseudo-Mercator ## NOM OBJECTID SHAPE__Len geometry ## 1 Axe de la Massawippi 1 13944.09 MULTILINESTRING ((-8010969 ... ## 2 Axe de la Saint-François 2 19394.28 MULTILINESTRING ((-8001909 ... head(Rues, n=2) ## Simple feature collection with 2 features and 16 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -8013896 ymin: 5681299 xmax: -8008810 ymax: 5695980 ## Projected CRS: WGS 84 / Pseudo-Mercator ## ID TOPONYMIE NOROUTE NUMEROCIVI NUMEROCI_1 NUMEROCI_2 NUMEROCI_3 ## 1 1 Rue Oliva-Turgeon NA 0 0 0 0 ## 2 9 Rue Melville NA 0 0 0 0 ## NOMGENERIQ TYPERUE TYPESEGMEN VITESSE TYPESENSUN MUNICIPALI ## 1 OLIVA-TURGEON Rue Locale 50 Pas de sens unique 43027 ## 2 MELVILLE Rue Locale 50 Pas de sens unique 43027 ## OBJECTID SHAPE__Len CIRCULATIO geometry ## 1 1 114.7781 Interdite en tout temps MULTILINESTRING ((-8008810 ... ## 2 2 114.4441 Interdite en tout temps MULTILINESTRING ((-8013782 ... head(Arrondissements, n=2) ## Simple feature collection with 2 features and 2 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -8027109 ymin: 5668860 xmax: -8000502 ymax: 5704391 ## Projected CRS: WGS 84 / Pseudo-Mercator ## NUMERO NOM ## 1 1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville ## 2 4 Arrondissement des Nations ## geometry ## 1 POLYGON ((-8005013 5702777,... ## 2 POLYGON ((-8005680 5690860,... Visualisons quelques couches importées avec ggplot(). ## Arrondissements et rues ggplot()+ geom_sf(data = Arrondissements, lwd = .8)+ geom_sf(data = Rues, aes(colour = TYPESEGMEN)) ## Arrondissements, pistes cyclables et installations sportives ggplot()+ geom_sf(data = Arrondissements, lwd = .8)+ geom_sf(data = PistesCyclables, aes(colour = NOM), lwd = .5)+ geom_sf(data = InstallationSport) 1.1.1.2 Importation d’une couche dans un GeoPackage Pour importer une couche stockée dans un GeoPackage (GPKG), vous devez spécifier le fichier et la couche avec respectivement les paramètres dsn et layer de la fonction st_read. Le code ci-dessous permet d’importer les secteurs de recensement de la région métropolitaine de recensement de Sherbrooke pour l’année 2021. ## Importation d&#39;une couche dans un GPKG SR.RMRSherb &lt;- st_read(dsn = &quot;data/chap01/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;SherbSR&quot;, quiet=TRUE) ## Affichage des deux premiers enregistrements head(SR.RMRSherb, n=2) ## Simple feature collection with 2 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 7762066 ymin: 1271201 xmax: 7765357 ymax: 1274082 ## Projected CRS: NAD83 / Statistics Canada Lambert ## IDUGD SRIDU SRNOM SUPTERRE PRIDU SRpop_2021 SRtlog_2021 ## 1 2021S05074330001.00 4330001.00 0001.00 3.1882 24 5637 2918 ## 2 2021S05074330002.00 4330002.00 0002.00 0.8727 24 1868 1169 ## SRrhlog_2021 RMRcode HabKm2 geom ## 1 2756 433 1768.082 MULTIPOLYGON (((7764998 127... ## 2 1063 433 2140.484 MULTIPOLYGON (((7763361 127... ## Visualisation rapide des secteurs avec ggplot ggplot()+ geom_sf(data = SR.RMRSherb, lwd = .5) 1.1.1.3 Importation d’une couche dans une geodatabase d’ESRI La logique est la même qu’avec un GeoPackage, nous spécifions le chemin de la geodatabase et la couche avec les paramètres dsn et layer. AffectDuTerritoire &lt;- st_read(dsn = &quot;data/chap01/geodatabase/Sherbrooke.gdb&quot;, layer = &quot;AffectationsDuTerritoire&quot;, quiet=TRUE) ## Visualisation des affectations du sol avec ggplot ggplot()+ geom_sf(data = AffectDuTerritoire, aes(fill = TYPE), lwd = .2) 1.1.1.4 Importation de données GPS En géomatique appliquée, il est fréquent de collecter des données sur le terrain avec un appareil GPS. Les données ainsi collectées peuvent être enregistrées dans différents formats de données dépendamment de l’appareil GPS utilisé : GPS eXchange Format (GPX), Garmin’s Flexible and Interoperable Data Transfer (FIT), Training Center XML (TCX), etc. Une personne ayant collecté des données sur le terrain pourrait aussi vous les transmettre dans un fichier csv (fichier texte délimité par des virgules). 1.1.1.4.1 Importation de coordonnées GPS longitude/latitude au format csv Il convient d’importer le fichier de coordonnées GPS dans R dans un DataFrame (avec la fonction read.csv). Une fois importé, nous constatons qu’il comprend trois champs : id : un champ identifiant avec des valeurs uniques. lon : longitude. lat : latitude. Les points sont projetés en longitude/latitude (WGS84 long/lat, EPSG : 4326). ## Importation du fichier csv PointsGPS &lt;- read.csv(&quot;data/chap01/gps/pointsGPS.csv&quot;) head(PointsGPS) ## id lon lat ## 1 1 -71.99985 45.36010 ## 2 2 -71.99096 45.37535 ## 3 3 -71.98444 45.46964 ## 4 4 -72.09873 45.37126 ## 5 5 -72.04880 45.41035 ## 6 6 -71.95000 45.32570 Pour convertir le DataFrame est un objet sf, nous utilisons la fonction st_as_sf en spécifiant les champs pour les coordonnées et la projection cartographique. ## Importation du fichier csv PointsGPS &lt;- st_as_sf(PointsGPS, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) Les points ainsi créés sont localisés dans la ville de Sherbrooke. ## Affichage des points avec la librairie tmap tmap_mode(&quot;view&quot;) ## Mode actif de tmap tm_shape(PointsGPS)+ tm_dots(size = 0.05, shape = 21, col = &quot;red&quot;) 1.1.1.4.2 Importation de coordonnées GPS au format GPX Le format GPX est certainement le format de stockage et d’échange de coordonnées GPS le plus utilisé. Les informations géographiques (x,y) et temporelles (date et heure) sont respectivement enregistrées en degrés longitude/latitude (projection WSG) (WGS84, EPSG : 4326) et en temps universel coordonné (UTC, format ISO 8601). Pour importer un fichier GPX, nous utilisons le package gpx. S’il n’est pas installé sur votre ordinateur, lancez la commande install.packages(\"gpx\") dans la console de R; n’oubliez pas de le charger avec library(\"gpx\")! Ensuite, importez le fichier GPX avec la fonction read_gpx, enregistrez la trace GPS dans un DataFrame et convertissez-la en objet sf. library(&quot;gpx&quot;) ## Importation du fichier GPX TraceGPS &lt;- read_gpx(&quot;data/chap01/gps/TraceGPS.gpx&quot;) ## Cette trace GPS comprend trois listes : routes, tracks et waypoints ## Les points sont stockés dans tracks names(TraceGPS) ## [1] &quot;routes&quot; &quot;tracks&quot; &quot;waypoints&quot; ## Pour visualiser les données, il suffit de lancer la ligne ## ci-dessous (mise en commentaire car le résultat est un peu long...) #head(TraceGPS) TraceGPS &lt;- TraceGPS$tracks$`ID1_PA_2021-12-03_TRAJET01.gpx` ## Conversion du DataFrame en objet sf TraceGPS &lt;- st_as_sf(TraceGPS, coords = c(&quot;Longitude&quot;,&quot;Latitude&quot;), crs = 4326) ## Visualisation des premiers enregistrements head(TraceGPS, n=2) ## Simple feature collection with 2 features and 4 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -4.022827 ymin: 5.327383 xmax: -4.022825 ymax: 5.327387 ## Geodetic CRS: WGS 84 ## Elevation Time extensions Segment ID ## 1 40.8 2021-12-03 08:38:49 NA 1 ## 2 40.6 2021-12-03 08:38:50 NA 1 ## geometry ## 1 POINT (-4.022827 5.327387) ## 2 POINT (-4.022825 5.327383) La trace GPS correspond à un trajet réalisé à vélo à Abidjan (Côte d’Ivoire) le 3 décembre 2021. Cette trace a été enregistrée avec une montre Garmin et comprend un point chaque seconde. tmap_mode(&quot;view&quot;) tm_basemap(leaflet::providers$OpenStreetMap)+ tm_shape(TraceGPS)+ tm_dots(size = 0.001, shape = 21, col = &quot;red&quot;)   La structure de la classe sf. La classe sf est composée de trois éléments (figure 1.1) : L’objet simple feature geometry (sfg) est la géométrie d’une observation. Tel que vu plus haut, elle est une géométrie simple (point, linestring, polygon), multiple (multipoint, multilinestring, multipolygon) ou une collection de géométries différentes (Geometrycollection). Pour définir chacune de ces géométries, nous utilisons les méthodes st_point(), st_linestring(), st_polygon(), st_multipoint(), st_multilinestring(), st_multipolygon() et Geometrycollection(). L’objet simple feature column (sfc) est simplement une liste de simple feature geometry (sfg). Elle représente la colonne geometry d’une couche vectorielle sf. L’objet data.frame correspond à la table attributaire. Une simple feature correspond ainsi à une observation (ligne) d’un objet sf, soit une entité spatiale comprenant l’information sémantique (attributs) et l’information spatiale (géométrie). Figure 1.1: Structure de la classe sf Voyons un exemple concret : créons une couche sf comprenant les trois entités spatiales décrites dans la figure 1.1. ## Création des géométries : simple feature geometry (sfg) point1 = st_point(c(-8001939, 5686891)) point2 = st_point(c(-8009681, 5696536)) point3 = st_point(c(-7998695, 5689743)) ## Création d&#39;une liste de géométries : simple feature geometry (sfc) ## avec la projection cartographique EPSG 3857 points_sfc = st_sfc(point1, point2, point3, crs = 3857) ## Création de la table attributaire : objet data.frame table_attr = data.frame(TYPE = c(&quot;Aréna&quot;, &quot;Aréna&quot;,&quot;Aréna&quot;), NOM = c(&quot;Aréna Eugène-Lalonde&quot;, &quot;Aréna Philippe-Bergeron&quot;, &quot;Centre Julien-Ducharme&quot;), OBJECTID = c(1, 2, 3)) ## Création de l&#39;objet sf Arena_sf = st_sf(table_attr, geometry = points_sfc) ## Le résultat est bien identique à celui de la figure ci-dessus head(Arena_sf) ## Simple feature collection with 3 features and 3 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -8009681 ymin: 5686891 xmax: -7998695 ymax: 5696536 ## Projected CRS: WGS 84 / Pseudo-Mercator ## TYPE NOM OBJECTID geometry ## 1 Aréna Aréna Eugène-Lalonde 1 POINT (-8001939 5686891) ## 2 Aréna Aréna Philippe-Bergeron 2 POINT (-8009681 5696536) ## 3 Aréna Centre Julien-Ducharme 3 POINT (-7998695 5689743) 1.1.2 Importation de données raster La fonction terra::rast permet d’importer des images dans différents formats (GeoTiff, ESRI, ENVI, ERDAS, BIN, GRID, etc.). Nous importons ci-dessous quatre feuillets de modèles numériques d’altitude (MNA) à l’échelle du 1/20000 couvrant la ville de Sherbrooke. La figure 1.2 présente l’un d’entre eux. ## Liste des fichiers GeoTIFF dans le dossier list.files(path=&quot;data/chap01/raster&quot;, pattern = &quot;.tif&quot;) ## [1] &quot;f21e05_101.tif&quot; &quot;f21e05_101.tif.aux.xml&quot; &quot;f21e05_201.tif&quot; ## [4] &quot;f21e05_201.tif.aux.xml&quot; &quot;f21e12_101.tif&quot; &quot;f21e12_101.tif.aux.xml&quot; ## [7] &quot;f31h08_102.tif&quot; &quot;f31h08_102.tif.aux.xml&quot; &quot;f31h08_202.tif&quot; ## [10] &quot;f31h08_202.tif.aux.xml&quot; ## Importation des fichiers f21e05_101 &lt;- terra::rast(&quot;data/chap01/raster/f21e05_101.tif&quot;) f21e05_201 &lt;- terra::rast(&quot;data/chap01/raster/f21e05_201.tif&quot;) f31h08_102 &lt;- terra::rast(&quot;data/chap01/raster/f31h08_102.tif&quot;) f31h08_202 &lt;- terra::rast(&quot;data/chap01/raster/f31h08_202.tif&quot;) f21e12_101 &lt;- terra::rast(&quot;data/chap01/raster/f21e12_101.tif&quot;) ## Visualisation des informations sur l&#39;image f21e05_101 f21e05_101 ## class : SpatRaster ## dimensions : 1409, 2798, 1 (nrow, ncol, nlyr) ## resolution : 9e-05, 9e-05 (x, y) ## extent : -72.00095, -71.74913, 45.24907, 45.37588 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## source : f21e05_101.tif ## name : f21e05_101 ## min value : 143.4273 ## max value : 423.5806 # Visualisation de l&#39;image terra::plot(f21e05_101, main=&quot;Modèle numérique d&#39;altitude à l’échelle de 1/20 000 (f21e05_101)&quot;) Figure 1.2: Modèle numérique d’élévation au 1/20000 (feuillet f21e05_101) References "],["sect012.html", "1.2 Manipulation de données vectorielles", " 1.2 Manipulation de données vectorielles Le package sfest une librairie extrêmement complète permettant de réaliser une multitude d’opérations géométriques sur des couches vectorielles comme dans un système d’information géographique (SIG). Notre objectif n’est pas de toutes les décrire, mais d’aborder les principales. Au fil de vos projets avec sf, vous apprendrez d’autres fonctions. Pour ce faire, n’hésitez pas à consulter : Une belle Cheatsheet sur sf. Allez y jeter un œil, cela vaut la peine! Sur le site CRAN desf, vous trouverez plusieurs vignettes explicatives (exemples de code documentés). La documentation complète en PDF. La syntaxe methods(class = 'sfc') renvoie la liste des méthodes implémentées dans le package sf. Pour accéder à l’aide en ligne de l’une d’entre elles, écrivez simplement ?Nom de la fonction (ex. : ?st_buffer). methods(class = &#39;sfc&#39;) ## [1] [ [&lt;- ## [3] as.data.frame c ## [5] coerce format ## [7] fortify identify ## [9] initialize ms_clip ## [11] ms_dissolve ms_erase ## [13] ms_explode ms_filter_islands ## [15] ms_innerlines ms_lines ## [17] ms_points ms_simplify ## [19] obj_sum Ops ## [21] print rep ## [23] scale_type show ## [25] slotsFromS3 st_area ## [27] st_as_binary st_as_grob ## [29] st_as_s2 st_as_sf ## [31] st_as_text st_bbox ## [33] st_boundary st_break_antimeridian ## [35] st_buffer st_cast ## [37] st_centroid st_collection_extract ## [39] st_concave_hull st_convex_hull ## [41] st_coordinates st_crop ## [43] st_crs st_crs&lt;- ## [45] st_difference st_geometry ## [47] st_inscribed_circle st_intersection ## [49] st_intersects st_is ## [51] st_is_valid st_line_merge ## [53] st_m_range st_make_valid ## [55] st_minimum_rotated_rectangle st_nearest_points ## [57] st_node st_normalize ## [59] st_point_on_surface st_polygonize ## [61] st_precision st_reverse ## [63] st_sample st_segmentize ## [65] st_set_precision st_shift_longitude ## [67] st_simplify st_snap ## [69] st_sym_difference st_transform ## [71] st_triangulate st_triangulate_constrained ## [73] st_union st_voronoi ## [75] st_wrap_dateline st_write ## [77] st_z_range st_zm ## [79] str summary ## [81] type_sum vec_cast.sfc ## [83] vec_ptype2.sfc vect ## see &#39;?methods&#39; for accessing help and source code 1.2.1 Fonctions relatives à la projection cartographique Les trois principales fonctions relatives à la projection cartographique de la couche vectorielle sont : st_crs(x) pour connaître la projection géographique d’un objet sf. st_transform(x, cr) pour modifier la projection cartographique. st_is_longlat(x) pour vérifier si les coordonnées sont en degrés longitude/latitude. ## Importation d&#39;un shapefile pour la province de Québec ProvinceQc &lt;- st_read(&quot;data/chap01/shp/Quebec.shp&quot;, quiet = TRUE) ## La projection est EPSG:3347 - NAD83 / Statistics Canada Lambert, ## soit la projection conique conforme de Lambert st_crs(ProvinceQc) ## Coordinate Reference System: ## User input: NAD83 / Statistics Canada Lambert ## wkt: ## PROJCRS[&quot;NAD83 / Statistics Canada Lambert&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;Statistics Canada Lambert&quot;, ## METHOD[&quot;Lambert Conic Conformal (2SP)&quot;, ## ID[&quot;EPSG&quot;,9802]], ## PARAMETER[&quot;Latitude of false origin&quot;,63.390675, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8821]], ## PARAMETER[&quot;Longitude of false origin&quot;,-91.8666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8822]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,49, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;Latitude of 2nd standard parallel&quot;,77, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8824]], ## PARAMETER[&quot;Easting at false origin&quot;,6200000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8826]], ## PARAMETER[&quot;Northing at false origin&quot;,3000000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8827]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Topographic mapping (small scale).&quot;], ## AREA[&quot;Canada - onshore and offshore - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon.&quot;], ## BBOX[38.21,-141.01,86.46,-40.73]], ## ID[&quot;EPSG&quot;,3347]] ## Reprojection de la couche en WGS84 long/lat (EPSG:4326) ProvinceQc.4326 &lt;- st_transform(ProvinceQc, crs = 4326) ## longitude/latitude? st_is_longlat(ProvinceQc) ## [1] FALSE st_is_longlat(ProvinceQc.4326) ## [1] TRUE La figure 1.3 démontre bien que les deux couches sont projetées différemment. Figure 1.3: Deux projections cartographiques 1.2.2 Fonctions d’opérations géométriques sur une couche Il existe une douzaine de fonctions d’opérations géométriques sur une couche dans le package sf dont le résultat renvoie de nouvelles géométries (voir la documentation suivante). Nous décrivons ici uniquement celles qui nous semblent les plus utilisées : st_bbox(x) renvoie les coordonnées minimales et maximales des géométries d’un objet sf. Pour créer l’enveloppe d’un objet sf, il suffit donc d’écrire st_as_sfc(st_bbox(x)). st_boundary(x) renvoie les limites (contours) des géométries d’un objet sf. st_convex_hull(x) crée l’enveloppe convexe des géométries d’un objet sf. st_combine(x) regroupe les géométries d’un objet sf en une seule géométrie sans les réunir ni résoudre les limites internes. st_union(x) fusionne les géométries d’un objet sf en une seule géométrie. st_buffer(x, dist, endCapStyle = c(\"ROUND\", \"FLAT\", \"SQUARE\"), joinStyle = c(\"ROUND\", \"MITRE\", \"BEVEL\")) crée des zones tampons d’une distance définie avec le paramètre dist. Cette fonction s’applique à des points, à des lignes et à des polygones. st_centroid(x) crée des points au centre de chaque géométrie d’un objet sf. Elle s’applique donc à des lignes et à des polygones. st_point_on_surface(x) crée un point au centre de chaque polygone d’un objet sf . st_simplify(x, dTolerance) simplifie les contours de géométries (lignes ou polygones) avec une tolérance exprimée en mètres (paramètre dTolerance) d’un objet sf . st_voronoi(x, bOnlyEdges = TRUE) crée des polygones de Thiessen, appelés aussi polygones de Voronoï pour des points. Attention, le paramètre bOnlyEdges = TRUE renvoie des lignes tandis que bOnlyEdges = FALSE renvoie des polygones. 1.2.2.1 Enveloppe et union d’une couche Les lignes de code ci-dessous créent une enveloppe (en bleu) et un polygone fusionné (en rouge) pour les arrondissements de la ville de Sherbrooke. La couche résultante de l’opération st_as_sfc(st_bbox(x)) est ainsi l’équivalent des outils emprise de QGIS et de Minimum Bounding Geometry (Gemeotry Type = Envelope) d’ArcGIS Pro. ## Enveloppe sur les arrondissements de la ville de Sherbrooke Arrond.Enveloppe &lt;- st_as_sfc(st_bbox(Arrondissements)) ## Fusionne les géométries en une seule en résolvant les limites internes Arrond.Union &lt;- st_union(Arrondissements) 1.2.2.2 Centroïdes et centre de surface Les lignes de code ci-dessous extraient les centres géométriques, c’est-à-dire les centroïdes (en bleu) et les points à l’intérieur des polygones (en rouge) pour les arrondissements de la ville de Sherbrooke. Ces deux opérations correspondent aux outils centroïdes et Point dans la surface de QGIS et Feature to Point (avec l'option Inside) d’ArcGIS Pro. ## Centroides et points dans les polygones sur les arrondissements Arrond.centroide &lt;- st_centroid(Arrondissements) Arrond.pointpoly &lt;- st_point_on_surface(Arrondissements) 1.2.2.3 Zone tampon (buffer) Une simple ligne de code permet de créer des zones tampons (équivalent des outils Analyse vectorielle/Tampon dans QGIS et Buffer dans ArcGIS Pro). Une fois les zones créées, utilisez la fonction st_union pour fusionner les tampons en un polygone. ## Zones tampons de 1000 mètres autour des installations sportives et récréatives InstSports.buffer &lt;- st_buffer(InstallationSport, dist = 1000) ## Si vous souhaitez fusionner les zones tampons, utilisez la fonction st_union InstSports.bufferUnion &lt;- st_union(InstSports.buffer) ## Zones tampons de 500 mètres autour des lignes PistesCyclables.buffer &lt;- st_buffer(PistesCyclables, dist = 500) PistesCyclables.bufferUnion &lt;- st_union(PistesCyclables.buffer) Notez que pour des polygones, il est possible de créer des polygones intérieurs comme suit : st_buffer(x, dist = - Valeur). Par exemple, le code ci-dessous crée des polygones de 200 mètres autour et à l’intérieur du parc du Mont-Bellevue de la ville de Sherbrooke. ## Importation de la couche des aires aménagées de la ville de Sherbrooke AiresAmenag &lt;- st_read(dsn = &quot;data/chap01/geodatabase/Sherbrooke.gdb&quot;, layer = &quot;AiresAmenagees&quot;, quiet = TRUE) ## Sélection du parc du Mont-Bellevue MontBellevue &lt;- subset(AiresAmenag, NOM == &quot;Parc du Mont-Bellevue&quot;) ## Création d&#39;une zone tampon autour du parc MontBellevue.ZTA500 &lt;- st_buffer(MontBellevue, dist = 200) ## Création d&#39;une zone tampon à l&#39;intérieur du parc MontBellevue.ZTI500 &lt;- st_buffer(MontBellevue, dist = -200) 1.2.2.4 Simplification de géométries La simplification ou généralisation d’une couche de lignes ou de polygones permet de supprimer des sommets tout en gardant le même nombre de géométries dans la couche résultante. Cette opération est réalisée dans QGIS avec l’outil simplifier et dans ArcGIS Pro avec l’outil Generalize. Deux raisons principales peuvent motiver le recours à cette opération : La réduction de la taille du fichier, surtout si la couche est utilisée pour de la cartographie interactive sur Internet avec des formats vectoriels comme le SVG (Scalable Vector Graphics), le KML ou le GeoJson. L’utilisation de la couche à une plus petite échelle cartographique nécessitant la suppression de détails. Les lignes de code suivantes permettent de simplifier les contours des arrondissements de la ville de Sherbrooke avec des tolérances de 250, 500, 1000 et 2000 mètres. Plus la valeur de la tolérance est élevée, plus les contours sont simplifiés. Notez que l’algorithme de Douglas-Peucker (Douglas et Peucker 1973) a été implémenté dans la fonction st_simplify. Bien qu’intéressant, cet algorithme ne conserve pas les frontières entre les polygones. ## Simplification des contours avec différentes distances de tolérance Arrond.simplify250m &lt;- st_simplify(Arrondissements, preserveTopology = TRUE, dTolerance = 250) Arrond.simplify500m &lt;- st_simplify(Arrondissements, preserveTopology = TRUE, dTolerance = 500) Arrond.simplify1000m &lt;- st_simplify(Arrondissements, preserveTopology = TRUE, dTolerance = 1000) Arrond.simplify2000m &lt;- st_simplify(Arrondissements, preserveTopology = TRUE, dTolerance = 2000) Pour remédier au problème des frontières non conservées, utilisez l’algorithme de Visvalingam et Whyatt (1993) avec la fonction ms_simplify du package rmapshaper, tel qu’illustré dans le code ci-dessous. À titre de rappel, pour l’installer et le charger sur votre ordinateur, tapez dans la console : install.packages(\"rmapshaper\") et library(\"rmapshaper\"). Le paramètre keep permet de définir la proportion de points à retenir : plus sa valeur est faible, plus la simplification est importante. ## Algorithme de Visvalingam–Whyatt Arrond.simplifyV.005 &lt;- rmapshaper::ms_simplify(Arrondissements, keep = .005, # proportion des points à retenir (entre 0 et 1) method = &quot;vis&quot;, # Algorithme Visvalingam keep_shapes = TRUE) tm_shape(Arrond.simplifyV.005)+tm_borders(col=&quot;red&quot;) 1.2.2.5 Enveloppe convexe (convex hull) Finalement, le code ci-dessous permet de créer l’enveloppe convexe pour des points (figure 1.4). Notez que cette fonction s’applique également à des lignes et à des polygones. Elle correspond aux outils Enveloppe convexe de QGIS et Feature to Point (avec l'option Convex hull) d’ArcGIS Pro. ## Convex hull autour des points GPS PointsGPS.Convexhull &lt;- st_convex_hull(st_union(PointsGPS)) Figure 1.4: Enveloppe convexe autour de points 1.2.2.6 Polygones de Thiessen (polygones de Voronoï) Le code ci-dessous comprend la fonction st_voronoi qui permet de créer des polygones de Thiessen pour les parcs de la ville de Sherbrooke. Elle correspond aux outils Polygones de Voronoï de QGIS et Create Thiessen Polygons d’ArcGIS Pro. 1.2.3 Fonctions d’opérations géométriques entre deux couches Les opérations entre deux couches sont bien connues et largement utilisées dans les SIG. Bien entendu, plusieurs fonctions de ce type sont disponibles dans sf et renvoient une nouvelle couche géographique sf : st_intersection(x, y) génère l’intersection entre les géométries de deux couches. À ne pas confondre avec la fonction st_intersects(x, y) qui permet de construire une requête spatiale. st_union(x, y) génère l’union entre les géométries de deux couches. st_difference(x, y) crée une géométrie à partir de x qui n’est pas en intersection avec y. st_sym_difference(x, y) crée une géométrie représentant les portions des géométries x et y qui ne s’intersectent pas. st_crop(x, y, xmin, ymin, xmax, ymax) extrait les géométries de x comprises dans un rectangle. En guise de comparaison, toutes ces fonctions sont disponibles dans la boîte à outils de traitement de QGIS (dans le groupe recouvrement de vecteur) et les outils de la catégorie Overlay du Geoprocessing d’ArcGIS Pro. Le code ci-dessous illustre comment réaliser des intersections et des unions entre deux couches polygonales. ## Importation des deux couches polysX &lt;- st_read(&quot;data/chap01/shp/PolyX.shp&quot;, quiet = TRUE) polysY &lt;- st_read(&quot;data/chap01/shp/PolyY.shp&quot;, quiet = TRUE) ## Intersection des deux couches ## Les géométries récupèrent les attributs des deux couches Inter.XY &lt;- st_intersection(polysX, polysY) head(Inter.XY) ## Simple feature collection with 2 features and 2 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -8006904 ymin: 5684822 xmax: -8006602 ymax: 5685184 ## Projected CRS: WGS 84 / Pseudo-Mercator ## X_id Y_id geometry ## 1 X.a Y.d POLYGON ((-8006753 5684838,... ## 2 Y.b Y.d POLYGON ((-8006788 5684908,... ## Intersection entre deux couches préalablement fusionnées : ## Le résutat est une seule géométrie Inter.XYUnion &lt;- st_intersection(st_union(polysX), st_union(polysY)) ## Union des deux couches Union.XY &lt;- st_union(st_union(polysX), st_union(polysY)) La fonction st_intersection peut aussi être utilisée comme la méthode clip dans un SIG (ArcGIS Pro ou QGIS). En guise d’exemple, dans le code ci-dessous, nous extrayons les points GPS localisés sur le territoire de la ville de Sherbrooke (figure 1.5). # Nous assurons que les deux couches ont la même projection PointsGPS &lt;- st_transform(PointsGPS, st_crs(Arrond.Union)) # Extraction des points PointsGPS.Sherb &lt;- st_intersection(PointsGPS, Arrond.Union) # Visualisation avant et après Map1 &lt;- ggplot()+geom_sf(data = Arrond.Union)+geom_sf(data = PointsGPS)+ labs(subtitle = &quot;Avant l&#39;intersection&quot;) Map2 &lt;- ggplot()+geom_sf(data = Arrond.Union)+geom_sf(data = PointsGPS.Sherb)+ labs(subtitle = &quot;st_intersection(points, polygone)&quot;) ggarrange(Map1, Map2, ncol = 2, nrow = 1) Figure 1.5: st_intersection() est équivalent à la méthode clip dans un SIG Quelques lignes de code suffisent pour générer les différences de superposition entre les géométries de couches géographiques. ## Différences entre deux couches Diff.XY &lt;- st_difference(st_union(polysX), st_union(polysY)) Diff.YX &lt;- st_difference(st_union(polysY), st_union(polysX)) Diff.symXY &lt;- st_sym_difference(st_union(polysY), st_union(polysX)) 1.2.4 Fonctions de mesures géométriques et de récupération des coordonnées géographiques Les principales fonctions de mesures géométriques et de coordonnées géographiques sont : st_area(x) calcule la superficie des polygones ou des multipolygones d’une couche sf . st_length(x) calcule la longueur des lignes ou des polylignes d’une couche sf . st_distance(x, y) calcule la distance 2D entre deux objets sf . st_coordinates(x) renvoie les coordonnées géographiques de géométries. Ci-dessous, nous affichons les superficies des quatre arrondissements, puis nous enregistrons les superficies en m2 et en km2 dans deux nouveaux champs dénommés SupM2 et SupKm2. ## Superficie des polygones des arrondissements st_area(Arrondissements) ## Units: [m^2] ## [1] 477791738 119343215 58289370 87034244 ## Ajout de champs de superficie dans la table attributaire Arrondissements$SupM2 &lt;- as.numeric(st_area(st_transform(Arrondissements, crs = 2949))) Arrondissements$SupKm2 &lt;- as.numeric(st_area(st_transform(Arrondissements, crs = 2949))) / 1000000 head(Arrondissements, n=2) ## Simple feature collection with 2 features and 4 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -8027109 ymin: 5668860 xmax: -8000502 ymax: 5704391 ## Projected CRS: WGS 84 / Pseudo-Mercator ## NUMERO NOM ## 1 1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville ## 2 4 Arrondissement des Nations ## geometry SupM2 SupKm2 ## 1 POLYGON ((-8005013 5702777,... 235580454 235.58045 ## 2 POLYGON ((-8005680 5690860,... 58861606 58.86161 De manière très semblable, calculons la longueur de géométries étant des lignes ou des multilignes. ## Longueurs en mètres PistesCyclables$longMetre &lt;- as.numeric(st_length(st_transform(PistesCyclables, crs = 2949))) PistesCyclables$longKm &lt;- as.numeric(st_length(st_transform(PistesCyclables, crs = 2949))) / 10000 head(PistesCyclables, n=2) ## Simple feature collection with 2 features and 5 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -8010969 ymin: 5666202 xmax: -7997972 ymax: 5697954 ## Projected CRS: WGS 84 / Pseudo-Mercator ## NOM OBJECTID SHAPE__Len geometry ## 1 Axe de la Massawippi 1 13944.09 MULTILINESTRING ((-8010969 ... ## 2 Axe de la Saint-François 2 19394.28 MULTILINESTRING ((-8001909 ... ## longMetre longKm ## 1 9807.769 0.9807769 ## 2 13602.324 1.3602324 Pour calculer la longueur d’un périmètre, il faut préalablement récupérer son contour avec la méthode st_boundary, puis calculer la longueur avec st_length. ## Conversion des polygones en lignes Arrond.Contour &lt;- st_boundary(Arrondissements) ## Calcul de la longueur et enregistrement dans deux nouveaux champs Arrondissements$PerimetreMetre &lt;- as.numeric(st_length(Arrond.Contour)) Arrondissements$PerimetreKm &lt;- as.numeric(st_length(Arrond.Contour)) / 1000 head(Arrondissements) ## Simple feature collection with 4 features and 6 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -8027109 ymin: 5668860 xmax: -7993013 ymax: 5704391 ## Projected CRS: WGS 84 / Pseudo-Mercator ## NUMERO NOM ## 1 1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville ## 2 4 Arrondissement des Nations ## 3 3 Arrondissement de Lennoxville ## 4 2 Arrondissement de Fleurimont ## geometry SupM2 SupKm2 PerimetreMetre PerimetreKm ## 1 POLYGON ((-8005013 5702777,... 235580454 235.58045 143771.63 143.77163 ## 2 POLYGON ((-8005680 5690860,... 58861606 58.86161 50476.65 50.47665 ## 3 POLYGON ((-7993443 5684778,... 28776861 28.77686 43531.03 43.53103 ## 4 POLYGON ((-7999483 5693167,... 42882506 42.88251 44172.25 44.17225 Calculons désormais la distance 2D (euclidienne) entre les centres des arrondissements. Nous utilisons donc la fonction st_distance(x), puisque nous avons une seule couche (x = Arrond.pointpoly). ## Longueurs en mètres st_distance(Arrond.pointpoly) ## Units: [m] ## 1 2 3 4 ## 1 0.00 10458.989 21787.479 18047.846 ## 2 10458.99 0.000 11555.203 8627.962 ## 3 21787.48 11555.203 0.000 9622.735 ## 4 18047.85 8627.962 9622.735 0.000 Admettons que nous souhaitons calculer la distance entre les centres des quatre arrondissements et l’hôtel de ville de Sherbrooke dont les coordonnées en degrés (WGS84, EPSG : 4326) sont les suivantes : -71.89306, 45.40417. Nous utilisons alors la fonction st_distance(x, y) dans laquelle les paramètres x et y sont les arrondissements et l’hôtel de ville. Quelques lignes de code suffisent à créer une couche pour l’hôtel de ville, à calculer les distances et à les stocker dans un nouveau champ attributaire de la couche arrondissement. ## Création d&#39;un objet sf pour l&#39;hôtel de ville HotelVille &lt;- data.frame(ID = 1, Nom = &quot;Hôtel de ville&quot;, lon = -71.89306, lat = 45.40417) HotelVille &lt;- st_as_sf(HotelVille, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) head(HotelVille) ## Simple feature collection with 1 feature and 2 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -71.89306 ymin: 45.40417 xmax: -71.89306 ymax: 45.40417 ## Geodetic CRS: WGS 84 ## ID Nom geometry ## 1 1 Hôtel de ville POINT (-71.89306 45.40417) ## Nous nous assurons que les deux couches ont la même projection HotelVille &lt;- st_transform(HotelVille, st_crs(Arrond.pointpoly)) ## Calcul des distances Arrondissements$DistHVMetre &lt;- as.numeric(st_distance(Arrond.pointpoly,HotelVille)) Arrondissements$DistHVKm &lt;- as.numeric(st_distance(Arrond.pointpoly, HotelVille)) / 1000 head(Arrondissements) ## Simple feature collection with 4 features and 8 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -8027109 ymin: 5668860 xmax: -7993013 ymax: 5704391 ## Projected CRS: WGS 84 / Pseudo-Mercator ## NUMERO NOM ## 1 1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville ## 2 4 Arrondissement des Nations ## 3 3 Arrondissement de Lennoxville ## 4 2 Arrondissement de Fleurimont ## geometry SupM2 SupKm2 PerimetreMetre PerimetreKm ## 1 POLYGON ((-8005013 5702777,... 235580454 235.58045 143771.63 143.77163 ## 2 POLYGON ((-8005680 5690860,... 58861606 58.86161 50476.65 50.47665 ## 3 POLYGON ((-7993443 5684778,... 28776861 28.77686 43531.03 43.53103 ## 4 POLYGON ((-7999483 5693167,... 42882506 42.88251 44172.25 44.17225 ## DistHVMetre DistHVKm ## 1 14661.518 14.661518 ## 2 4662.164 4.662164 ## 3 9058.677 9.058677 ## 4 4050.374 4.050374 Il est fréquent de vouloir enregistrer les coordonnées géographiques dans des champs attributaires. Dans le code ci-dessous, nous créons deux champs (x et y) dans lesquels nous enregistrons les coordonnées géographiques des points au centre de la surface de chaque arrondissement. Pour ce faire, nous utilisons la méthode st_coordinates . ## Coordonnées des centres de la surface des polygones xy &lt;- st_coordinates(st_point_on_surface(Arrondissements)) head(xy) ## X Y ## [1,] -8017707 5686628 ## [2,] -8007570 5684053 ## [3,] -7997637 5678149 ## [4,] -7999683 5687552 ## Enregistrement dans la couche Arrondissements. Notez que : ## xy[,1] signale de récupérer toutes les valeurs de la première colonne, soit X ## xy[,2] signale de récupérer toutes les valeurs de la deuxième colonne, soit Y Arrondissements$X &lt;- xy[,1] Arrondissements$Y &lt;- xy[,2] 1.2.5 Jointures spatiales En géomatique, il est fréquent de réaliser des jointures spatiales, soit une opération qui consiste à joindre les attributs d’une couche géographique à une autre à partir d’une relation spatiale. Prenons deux exemples construits avec les installations sportives et récréatives (couche InstallationSport) et les arrondissements de la ville de Sherbrooke (Arrondissements). Premièrement, pour les installations sportives et récréatives (couche InstallationSport), nous souhaitons ajouter dans la table attributaire les champs NUMERO et NOM issus de la couche des arrondissements de la ville de Sherbrooke (Arrondissements). Ces deux champs nous permettent de savoir dans quel arrondissement est localisée chaque installation sportive. ## Jointure spatiale avec le paramètre st_intersects InstallS.join &lt;- st_join(InstallationSport, Arrondissements, join = st_intersects) ## Visualisation des deux premiers enregistrements head(InstallS.join, n=2) ## Simple feature collection with 2 features and 16 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536 ## Projected CRS: WGS 84 / Pseudo-Mercator ## TYPE DETAIL NOM.x SURFACE ECLAIRAGE OBJECTID NUMERO ## 1 Aréna &lt;NA&gt; Aréna Eugène-Lalonde &lt;NA&gt; &lt;NA&gt; 1 2 ## 2 Aréna &lt;NA&gt; Aréna Philippe-Bergeron &lt;NA&gt; &lt;NA&gt; 2 1 ## NOM.y SupM2 ## 1 Arrondissement de Fleurimont 42882506 ## 2 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville 235580454 ## SupKm2 PerimetreMetre PerimetreKm DistHVMetre DistHVKm X Y ## 1 42.88251 44172.25 44.17225 4050.374 4.050374 -7999683 5687552 ## 2 235.58045 143771.63 143.77163 14661.518 14.661518 -8017707 5686628 ## geometry ## 1 POINT (-8001939 5686891) ## 2 POINT (-8009681 5696536) ## Suppression des champs utiles InstallS.join[c(&quot;SupM2&quot;, &quot;SupKm2&quot;, &quot;PerimetreMetre&quot;, &quot;PerimetreKm&quot;, &quot;DistHVMetre&quot;, &quot;DistHVKm&quot;)] &lt;- list(NULL) ## Modification des noms de champs : NOM.x et NOM.y names(InstallS.join)[names(InstallS.join) == &quot;NOM.x&quot;] &lt;- &quot;NomInstallation&quot; names(InstallS.join)[names(InstallS.join) == &quot;NOM.y&quot;] &lt;- &quot;NomArrondissement&quot; head(InstallS.join, n=2) ## Simple feature collection with 2 features and 10 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -8009681 ymin: 5686891 xmax: -8001939 ymax: 5696536 ## Projected CRS: WGS 84 / Pseudo-Mercator ## TYPE DETAIL NomInstallation SURFACE ECLAIRAGE OBJECTID NUMERO ## 1 Aréna &lt;NA&gt; Aréna Eugène-Lalonde &lt;NA&gt; &lt;NA&gt; 1 2 ## 2 Aréna &lt;NA&gt; Aréna Philippe-Bergeron &lt;NA&gt; &lt;NA&gt; 2 1 ## NomArrondissement X Y ## 1 Arrondissement de Fleurimont -7999683 5687552 ## 2 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville -8017707 5686628 ## geometry ## 1 POINT (-8001939 5686891) ## 2 POINT (-8009681 5696536) Deuxièmement, une autre jointure classique est de compter les points compris dans des polygones, soit une opération SIG communément appelée POINT-IN-POLYGON. ## Sélection des points dans les polygones des arrondissements ## Notez que la relation spatiale pour la jointure est st_contains ## Nous aurions pu aussi utiliser st_intersects Arrondissements$NbInstall = lengths(st_contains(Arrondissements, InstallationSport)) head(Arrondissements$NbInstall) ## [1] 125 166 29 116 Autres relations spatiales à appliquer lors de la jointure spatiale Avec le paramètre join de la méthode st_join, il est possible de spécifier la jointure spatiale d’une multitude de façons : st_contains_properly, st_contains, st_covered_by, st_covers, st_crosses, st_disjoint, st_equals_exact, st_equals, st_is_within_distance, st_nearest_feature,st_overlaps, st_touches et st_within. N’hésitez pas à consulter la documentation de la fonction en tapant?st_join dans la console R. 1.2.6 Requêtes spatiales Dans un logiciel SIG, la sélection d’entités spatiales par localisation est une opération courante, équivalente à Select By Location dans ArcGis Pro ou Sélection par localisation dans QGIS. Le package sf permet de réaliser des requêtes spatiales avec notamment les méthodes suivantes : st_contains(x, y) renvoie les géométries de x qui contiennent celles de y. Cette fonction est donc l’inverse de st_within. st_disjoint(x, y) renvoie les géométries de x qui ne partagent aucune portion de celles de y. Cette fonction est donc l’inverse de st_intersects(x, y). st_equals(x, y) renvoie les géométries de x qui sont identiques à celles de y. st_intersects(x, y) renvoie les géométries de x qui partagent au moins une partie de celles de y. Elle est donc l’inverse de st_disjoints(x, y). st_nearest_feature(x, y) renvoie pour chaque géométrie x, la géométrie de y qui est la plus proche. st_overlaps(x, y) cette fonction est très semblable à st_intersects(x, y). Toutefois, les types de géométries de x et de y doivent être identiques, c’est-à-dire deux couches de lignes ou de couches de polygones. Aussi, une géométrie ne peut pas contenir complètement l’autre comme avec st_within(x, y) et st_contains(x, y). st_touches(x, y) renvoie les géométries de x qui sont tangentes à celles de x sans qu’elles se chevauchent. Par exemple, deux arrondissements peuvent se toucher, c’est-à-dire qu’ils partagent une frontière commune sans que l’un chevauche l’autre. st_within(x, y) renvoie les géométries de x qui sont comprises intégralement dans celles de y. Cette fonction est donc l’inverse de st_contains(x, y). st_within_distance(x, y, dist =) renvoie les géométries de x qui sont situées à une certaine distance euclidienne de celles de y. Modification de l’affichage du résultat de la requête spatiale : le paramètre sparse Par défaut, le résultat d’une requête spatiale renvoie une liste d’indices pour les géométries x et y, mais il est aussi possible de renvoyer la matrice complète entre x et y qui prendra les valeurs TRUE quand la relation spatiale est vérifiée et FALSE pour une situation inverse. Prenons deux exemples pour illustrer le tout. La figure ci-dessous représente les quatre arrondissements de la ville de Sherbrooke. Notez que les numéros correspondent aux indices des géométries. ## NUMERO NOM ## 1 1 Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville ## 2 4 Arrondissement des Nations ## 3 3 Arrondissement de Lennoxville ## 4 2 Arrondissement de Fleurimont Appliquons une requête spatiale entre les arrondissements avec st_intersects et sparse = TRUE. Pour chaque arrondissement, nous obtenons une liste des arrondissements qui l’intersectent. st_intersects(Arrondissements, Arrondissements, sparse = TRUE) ## Sparse geometry binary predicate list of length 4, where the predicate ## was `intersects&#39; ## 1: 1, 2, 4 ## 2: 1, 2, 3, 4 ## 3: 2, 3, 4 ## 4: 1, 2, 3, 4 Avec sparse = FALSE, nous obtenons une matrice complète de dimension 4 X 4 arrondissements. Nous constatons que l’arrondissement 1 intersecte lui-même (évidemment!) et les arrondissements 2 et 4, mais il n’intersecte pas le 3. st_intersects(Arrondissements, Arrondissements, sparse = FALSE) ## [,1] [,2] [,3] [,4] ## [1,] TRUE TRUE FALSE TRUE ## [2,] TRUE TRUE TRUE TRUE ## [3,] FALSE TRUE TRUE TRUE ## [4,] TRUE TRUE TRUE TRUE Construisons des requêtes plus complexes comprenant deux couches. Premièrement, construisons une requête spatiale pour sélectionner les segments des pistes cyclables qui intersectent le parc du Mont-Bellevue. Pour ce faire, nous utilisons la fonction st_intersects avec l’argument sparse = FALSE et enregistrons le résultat dans un nouveau champ dénommé ParcMB.intersect qui prendra les valeurs TRUE ou FALSE. ## Intersection RequeteSpatiale &lt;- st_intersects(PistesCyclables, MontBellevue, sparse = FALSE) head(RequeteSpatiale) ## [,1] ## [1,] FALSE ## [2,] FALSE ## [3,] FALSE ## [4,] FALSE ## [5,] FALSE ## [6,] FALSE ## Création d&#39;un nouveau champ PistesCyclables$ParcMB.intersect &lt;- RequeteSpatiale[, 1] head(PistesCyclables) ## Simple feature collection with 6 features and 6 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -8010969 ymin: 5666202 xmax: -7997216 ymax: 5697954 ## Projected CRS: WGS 84 / Pseudo-Mercator ## NOM OBJECTID SHAPE__Len geometry ## 1 Axe de la Massawippi 1 13944.08678 MULTILINESTRING ((-8010969 ... ## 2 Axe de la Saint-François 2 19394.27693 MULTILINESTRING ((-8001909 ... ## 3 Axe du Ruisseau-Dorman 3 16337.23985 MULTILINESTRING ((-7999121 ... ## 4 Réseau utilitaire 4 467.23254 MULTILINESTRING ((-8000179 ... ## 5 Réseau utilitaire 5 15.57987 MULTILINESTRING ((-8004036 ... ## 6 Réseau utilitaire 6 823.83428 MULTILINESTRING ((-8003649 ... ## longMetre longKm ParcMB.intersect ## 1 9807.76890 0.980776890 FALSE ## 2 13602.32404 1.360232404 FALSE ## 3 11469.13476 1.146913476 FALSE ## 4 327.46928 0.032746928 FALSE ## 5 10.95083 0.001095083 FALSE ## 6 578.59143 0.057859143 FALSE ## Nous constatons qu&#39;un seul segment intersecte le parc table(PistesCyclables$ParcMB.intersect) ## ## FALSE TRUE ## 272 1 ## Création d&#39;une nouvelle couche pour la sélection PistesCyclables.Selection &lt;- PistesCyclables[PistesCyclables$ParcMB.intersect== TRUE, ] ## Visualisation tmap_mode(&quot;view&quot;) tm_shape(MontBellevue) + tm_fill(col=&quot;lightgreen&quot;)+ tm_borders(col = &quot;black&quot;, lwd=2)+ tm_shape(PistesCyclables.Selection)+tm_lines(col=&quot;red&quot;, lwd=1) Construisons une deuxième requête spatiale pour sélectionner les points GPS situés à moins de cinq kilomètres de l’hôtel de ville de Sherbrooke avec la méthode st_is_within_distance. ## Requête spatiale RequeteSpatiale &lt;- st_is_within_distance(PointsGPS, HotelVille, 5000, sparse = FALSE) ## Ajout d&#39;un champ pour la requête PointsGPS$HotelVille2km &lt;- RequeteSpatiale[, 1] ## Nous constatons que 17 points GPS sont à moins de 5 km table(PointsGPS$HotelVille2km) ## ## FALSE TRUE ## 72 17 ## Création d&#39;une nouvelle couche pour la sélection PointsGPS.selection &lt;- PointsGPS[PointsGPS$HotelVille2km== TRUE, ] ## Visualisation tm_shape(PointsGPS.selection) + tm_dots(col=&quot;red&quot;, size = .05)+ tm_shape(HotelVille)+tm_dots(col=&quot;black&quot;, size = .25) Finalement, avec la méthode st_within, nous constatons que seuls deux points GPS sont situés dans le parc du Mont-Bellevue. ## Requête spatiale RequeteSpatiale &lt;- st_within(st_transform(PointsGPS, st_crs(MontBellevue)), MontBellevue, sparse = FALSE) table(RequeteSpatiale[,1]) ## ## FALSE TRUE ## 87 2 1.2.7 Manipulation des données attributaires 1.2.7.1 Importation d’une table attributaire Joindre les attributs d’une table externe à une couche vectorielle sf En SIG, joindre une table à une couche géographique vectorielle est une opération courante. L’exemple classique est de joindre des données socioéconomiques issues d’un recensement à une couche géographique (divisions de recensement, subdivisions de recensement, secteurs de recensement, aires de diffusion, etc.). Pour ce faire, vous devez importer les données dans un DataFrame de R. Ces données peuvent être stockées dans différents formats de fichiers (texte délimité par des virgules (extension csv), dBase (dbf), Excel (xlsx)) ou dans des fichiers provenant de logiciels statistiques commerciaux comme Stata, SAS et SPSS (dta, sas7bdat, sav). Dans cette section, nous voyons uniquement comment importer des fichiers texte délimités par des virgules et des fichiers Excel et dBase. Concernant ce dernier type de fichier, notez que la table attributaire d’une couche Esri Shapefile est stockée dans un fichier dBase! Il peut être intéressant d’importer la table sans les géométries. Pour une description détaillée de l’importation d’autres fichiers (entre autres Stata, SAS et SPSS), consultez la section intitulée Manipulation d’un DataFrame (Apparicio et Gelb 2022). Dans le code ci-dessous, nous voyons comment importer trois types de fichiers : read.csv(file) pour importer un fichier délimité par des virgules. Cette fonction est de base avec R, ce qui signifie qu’elle ne nécessite pas l’installation d’un package. read.dbf(file) pour importer un fichier dBase. Cette fonction est rattachée au package foreign que vous devez installer si ce n’est pas déjà fait (commande install.packages(\"foreign\")) et le charger (commande library(\"foreign\")). read.xlsx(file) pour importer un fichier Excel. Cette fonction est rattachée au package xlsx que vous devez installer si ce n’est pas déjà fait (commande install.packages(\"xlsx\")) et le charger (commande library(\"xlsx\")). library(&quot;xlsx&quot;) # package pour importer des fichiers Excel library(&quot;foreign&quot;) # package pour importer des fichiers dBase ## Importation du fichier csv t1 &lt;- Sys.time() dfCSV &lt;- read.csv(file = &quot;data/chap01/tables/SRQC2021.csv&quot;, header = TRUE, dec = &quot;.&quot;, # séparateur de décimales qui peut être remplacé par , sep = &quot;,&quot; # séparateur des champs qui peut être remplacé par ; ) t2 &lt;- Sys.time() cat(&quot;temps de traitement (CSV) : &quot;, as.numeric(difftime(t2,t1,units=&quot;secs&quot;)), &quot; secondes&quot;) ## temps de traitement (CSV) : 0.01492 secondes ## Importation d&#39;un fichier Excel avec le nom de fichier et de la feuille Excel ## sheetIndex = 1 signale l&#39;importation de la première feuille Excel t1 &lt;- Sys.time() dfExcel &lt;- read.xlsx(file = &quot;data/chap01/tables/ADSRQC2021.xlsx&quot;, sheetIndex = 2) t2 &lt;- Sys.time() cat(&quot;temps de traitement (Excel) : &quot;, as.numeric(difftime(t2,t1,units=&quot;secs&quot;)), &quot; secondes&quot;) ## temps de traitement (Excel) : 7.688808 secondes ## Importation du fichier dBase t1 &lt;- Sys.time() dfDbf &lt;- read.dbf(file = &quot;data/chap01/tables/ADQC2021.dbf&quot;) t2 &lt;- Sys.time() cat(&quot;temps de traitement (dBase) : &quot;, as.numeric(difftime(t2,t1,units=&quot;secs&quot;)), &quot; secondes&quot;) ## temps de traitement (dBase) : 0.1332331 secondes Le temps nécessaire pour importer un fichier Excel est bien plus long que pour des fichiers texte et dBase! Par conséquent, si vous travaillez avec Excel, il est vivement conseillé de l’exporter vers un fichier texte (Fichier/Enregistrer sous/type de fichier CSV). Quelques lignes suffisent pour explorer la structure des données importées. ## Nombre de lignes et de colonnes nrow(dfCSV) ## [1] 2245 ncol(dfCSV) ## [1] 40 cat(&quot;le DataFrame dfCSV a&quot;, nrow(dfCSV), &quot;lignes (observations)&quot;, &#39;et&#39;, ncol(dfCSV), &quot;colonnes\\n&quot;) ## le DataFrame dfCSV a 2245 lignes (observations) et 40 colonnes ## Noms des champs names(dfCSV) ## [1] &quot;SRIDU&quot; &quot;PopTotAge&quot; ## [3] &quot;Pop0_14&quot; &quot;Pop15_64&quot; ## [5] &quot;Pop65plus&quot; &quot;TotalLog&quot; ## [7] &quot;MaisonIndiv&quot; &quot;MaisonJumulee&quot; ## [9] &quot;MaisonRangee&quot; &quot;AppartDuplex&quot; ## [11] &quot;AppartMoins5E&quot; &quot;Appart5EtPlus&quot; ## [13] &quot;AutreMaisonIndivAttenante&quot; &quot;LogementMobile&quot; ## [15] &quot;TotalMenag&quot; &quot;Menage1pers&quot; ## [17] &quot;Menage2pers&quot; &quot;Menage3pers&quot; ## [19] &quot;Menage4pers&quot; &quot;Menage5pPlus&quot; ## [21] &quot;RevMedMenage&quot; &quot;PopTotMFRApI&quot; ## [23] &quot;PopTotMFR&quot; &quot;PopTotMFRPct&quot; ## [25] &quot;TotalMenag2&quot; &quot;Proprietaire&quot; ## [27] &quot;Locataire&quot; &quot;TotalLog2&quot; ## [29] &quot;Log1960ouAv&quot; &quot;Log1961_80&quot; ## [31] &quot;Log1981_90&quot; &quot;Log1991_00&quot; ## [33] &quot;Log2001_05&quot; &quot;Log2006_10&quot; ## [35] &quot;Log2011_15&quot; &quot;Log2016_21&quot; ## [37] &quot;ValeurMedLog&quot; &quot;ValeurMoyLog&quot; ## [39] &quot;LoyerMedian&quot; &quot;LoyerMoyen&quot; ## Affichage des deux premières observations head(dfCSV, n=2) ## SRIDU PopTotAge Pop0_14 ## 1 4470001.01 (SR), Drummondville (RMR) (4470001.01) (00000) 5080 810 ## 2 4470001.02 (SR), Drummondville (RMR) (4470001.02) (00000) 3400 175 ## Pop15_64 Pop65plus TotalLog MaisonIndiv MaisonJumulee MaisonRangee ## 1 3285 985 2280 1290 185 210 ## 2 1305 1920 1815 155 75 85 ## AppartDuplex AppartMoins5E Appart5EtPlus AutreMaisonIndivAttenante ## 1 70 415 0 15 ## 2 15 1485 0 5 ## LogementMobile TotalMenag Menage1pers Menage2pers Menage3pers Menage4pers ## 1 100 2285 705 895 325 225 ## 2 0 1810 985 670 100 45 ## Menage5pPlus RevMedMenage PopTotMFRApI PopTotMFR PopTotMFRPct TotalMenag2 ## 1 130 69000 5085 520 10.2 2295 ## 2 15 47600 2885 545 18.9 1820 ## Proprietaire Locataire TotalLog2 Log1960ouAv Log1961_80 Log1981_90 Log1991_00 ## 1 1445 850 2295 115 590 535 485 ## 2 485 1335 1820 50 310 375 405 ## Log2001_05 Log2006_10 Log2011_15 Log2016_21 ValeurMedLog ValeurMoyLog ## 1 155 70 75 265 250000 250200 ## 2 215 200 120 140 250000 305000 ## LoyerMedian LoyerMoyen ## 1 695 742 ## 2 740 737 1.2.7.2 Jointure attributaire avec la couche géographique sf Les données importées dans la table attributive proviennent du recensement de Statistique Canada de 2021 et sont ancrées au niveau des secteurs de recensement (SR) des régions métropolitaines de recensement (RMR) et des agglomérations de recensement (AR) du Québec. Pour les SR de la RMR de Sherbrooke, les données de la couche géométrique sont importées à partir d’un fichier shapefile. Aussi, nous constatons que les deux sources de données ont un champ commun SRIDU, soit l’identifiant unique des SR, mais que l’information y est présentée différemment : Dans la couche géographique SR.RMRSherb (objet sf), nous avons par exemple la valeur 4330001.00, soit un champ avec dix caractères. Dans la table attributaire dfCSV (DataFrame), nous avons la valeur 4470001.01 (SR), Drummondville (RMR) (4470001.01) (00000). Par conséquent, avant d’appliquer une jointure, nous modifions le champ SRIDU de ce DataFrame afin qu’il comprenne aussi dix caractères avec la ligne de code suivante : dfCSV$SRIDU &lt;- substr(dfCSV$SRIDU, 1, 10). De la sorte, nous récupérons uniquement les dix premiers caractères. Finalement, la jointure est réalisée avec la fonction merge avec laquelle nous spécifions le résultat de la jointure (SR.RMRSherbDonnees), la couche géographique (SR.RMRSherb), la table attributaire (dfCSV) et le champ commun aux deux avec l’option (by =\"SRIDU\") : SR.RMRSherbDonnees &lt;- merge(SR.RMRSherb, dfCSV, by = \"SRIDU\"). ## Importation des SR de la RMR de Sherbrooke SR.RMRSherb &lt;- st_read(dsn = &quot;data/chap01/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;SherbSR&quot;, quiet=TRUE) ## Visualisation des premiers enregistrements head(as.data.frame(SR.RMRSherb), n=2) ## IDUGD SRIDU SRNOM SUPTERRE PRIDU SRpop_2021 SRtlog_2021 ## 1 2021S05074330001.00 4330001.00 0001.00 3.1882 24 5637 2918 ## 2 2021S05074330002.00 4330002.00 0002.00 0.8727 24 1868 1169 ## SRrhlog_2021 RMRcode HabKm2 geom ## 1 2756 433 1768.082 MULTIPOLYGON (((7764998 127... ## 2 1063 433 2140.484 MULTIPOLYGON (((7763361 127... head(dfCSV, n=2) ## SRIDU PopTotAge Pop0_14 ## 1 4470001.01 (SR), Drummondville (RMR) (4470001.01) (00000) 5080 810 ## 2 4470001.02 (SR), Drummondville (RMR) (4470001.02) (00000) 3400 175 ## Pop15_64 Pop65plus TotalLog MaisonIndiv MaisonJumulee MaisonRangee ## 1 3285 985 2280 1290 185 210 ## 2 1305 1920 1815 155 75 85 ## AppartDuplex AppartMoins5E Appart5EtPlus AutreMaisonIndivAttenante ## 1 70 415 0 15 ## 2 15 1485 0 5 ## LogementMobile TotalMenag Menage1pers Menage2pers Menage3pers Menage4pers ## 1 100 2285 705 895 325 225 ## 2 0 1810 985 670 100 45 ## Menage5pPlus RevMedMenage PopTotMFRApI PopTotMFR PopTotMFRPct TotalMenag2 ## 1 130 69000 5085 520 10.2 2295 ## 2 15 47600 2885 545 18.9 1820 ## Proprietaire Locataire TotalLog2 Log1960ouAv Log1961_80 Log1981_90 Log1991_00 ## 1 1445 850 2295 115 590 535 485 ## 2 485 1335 1820 50 310 375 405 ## Log2001_05 Log2006_10 Log2011_15 Log2016_21 ValeurMedLog ValeurMoyLog ## 1 155 70 75 265 250000 250200 ## 2 215 200 120 140 250000 305000 ## LoyerMedian LoyerMoyen ## 1 695 742 ## 2 740 737 ## Modification du champ SRIDU du DataFrame dfCSV dfCSV$SRIDU &lt;- substr(dfCSV$SRIDU, 1, 10) ## Jointure attributaire avec la fonction merge SR.RMRSherbDonnees &lt;- merge(SR.RMRSherb, dfCSV, by = &quot;SRIDU&quot;) Jointure avec deux champs ayant des noms différents Vous avez compris qu’une jointure attributaire s’écrit : NouvelObjetSf &lt;- merge(X, Y, by = \"Nom du champ commun pour la jointure\") avec X et Y étant respectivement l’objet sf (couche géographique) et la table attributaire à joindre. Si les champs pour la jointure ont des noms différents, il est possible d’écrire : NouvelObjetSf &lt;- merge(X, Y, by.x = \"Champ X pour la jointure\", by.y = \"Champ Y pour la jointure\") Aussi, ce type de jointure conserve uniquement les observations qui sont communes à la couche géographique et à la table attributaire. Concrètement, si une couche comprend 100 entités spatiales et la table attributaire uniquement 80 observations, la couche résultante (NouvelObjetSf) aura uniquement 80 entités spatiales (si bien entendu, les valeurs concordent…). Si vous souhaitez quand même conserver toutes les entités spatiales de la couche géographique de départ, écrivez alors : NouvelObjetSf &lt;- merge(X, Y, by = \"Nom du champ commun pour la jointure\", all.x = TRUE) Dans la nouvelle couche Sf, les entités spatiales de X qui n’ont pas été appariées avec les observations de la table attributaire Y auront des valeurs nulles (NA) pour les champs de X ajoutés à NouvelObjetSf. Pour plus d’informations sur les différentes variantes d’une jointure, tapez ?merge dans la console. 1.2.7.3 Ajout et calcul de champs Dans la section 1.2.4, nous avons vu comment ajouter des champs relatifs à la géométrie (aire, longueur, distance, coordonnées de centroïdes). Dans un SIG, il est courant de calculer de nouveaux champs à partir de champs existants dans la table attributaire (par exemple, avec les outils Calculatrice de champ dans QGIS ou Calculate Field dans ArcGIS Pro). Ce type de traitement est aussi très simple à réaliser dans R. Pour ce faire, nous utilisons des opérateurs mathématiques, relationnels et logiques comme dans n’importe quel logiciel de SIG. En guise d’exemple, nous calculons ci-dessous les pourcentages d’enfants de moins de 15 ans et de locataires. Ces pourcentages sont arrondis à deux décimales. ## Création et cacul de nouveau champs SR.RMRSherbDonnees$PctPop0_14 &lt;- round(SR.RMRSherbDonnees$Pop0_14 / SR.RMRSherbDonnees$PopTotAge * 100, 2) SR.RMRSherbDonnees$PctLocataire &lt;- round(SR.RMRSherbDonnees$Locataire / SR.RMRSherbDonnees$TotalMenag2 * 100, 2) 1.2.7.4 Requêtes attributaires Dans un SIG, il est fréquent de réaliser une requête attributaire (par exemple, avec les outils Select By Attributes dans ArcGIS Pro et Sélection avec expression dans QGIS) pour explorer les données d’une part, et exporter le résultat de la requête dans une nouvelle couche d’autre part (Export Features dans ArcGIS Pro et Sauvegarder les entités sélectionnées sous…). Dans le code ci-dessous, vous trouverez plusieurs exemples de requêtes attributaires. Remarquez que les résultats des requêtes sont enregistrés dans de nouveaux objets sf (couches géographiques) dénommés Requete1 à Requete5. ## Sélection de l&#39;axe cyclable de la Magog ############################################# # Affichage des valeurs uniques pour le champ NOM de la couche PistesCyclables unique(PistesCyclables$NOM) ## [1] &quot;Axe de la Massawippi&quot; &quot;Axe de la Saint-François&quot; ## [3] &quot;Axe du Ruisseau-Dorman&quot; &quot;Réseau utilitaire&quot; ## [5] &quot;Tronçon fermé temporairement&quot; &quot;Détour&quot; ## [7] &quot;Axe de la Magog&quot; &quot;Axe du Ruisseau-Kee&quot; ## [9] &quot;Axe de la Magog Sud&quot; NA ## [11] &quot;Axe du Sommet&quot; ## Requête attributaire et enregistrement du résultat dans un nouvel objet sf Requete1 &lt;- subset(PistesCyclables, NOM == &quot;Axe de la Magog&quot;) cat(nrow(Requete1), &quot;enregistrements sélectionnés sur&quot;, nrow(PistesCyclables)) ## 15 enregistrements sélectionnés sur 273 ## Si vous souhaitez connaître uniquement le nombre d&#39;enregistrements sélectionnés ## sans créer un nouvel objet sf, il suffit d&#39;écrire : nrow(subset(PistesCyclables, NOM == &quot;Axe de la Magog&quot;)) ## [1] 15 ## Sélection des SR dont la moitié ou plus des logements sont en location ########################################################################## ## Sommaire statistique sur le champ pourcentage de locataires summary(SR.RMRSherbDonnees$PctLocataire) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6.92 18.03 41.63 43.81 65.95 93.83 ## Requête attributaire et enregistrement du résultat dans un nouvel objet sf Requete2 &lt;- subset(SR.RMRSherbDonnees, PctLocataire &gt;= 50) cat(nrow(Requete2), &quot;enregistrements sélectionnés sur&quot;, nrow(SR.RMRSherbDonnees)) ## 20 enregistrements sélectionnés sur 50 ## Sélection des installations sportives avec un éclairage dans ## l&#39;arrondissement des Nations (deux critères dans la requête) ########################################################################## unique(InstallS.join$NomArrondissement) ## [1] &quot;Arrondissement de Fleurimont&quot; ## [2] &quot;Arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville&quot; ## [3] &quot;Arrondissement des Nations&quot; ## [4] &quot;Arrondissement de Lennoxville&quot; table(InstallS.join$ECLAIRAGE) ## ## Non Oui ## 46 85 ## Requête attributaire avec un opérateur AND (&amp;) Requete3 &lt;- subset(InstallS.join, NomArrondissement == &quot;Arrondissement des Nations&quot; &amp; ECLAIRAGE == &quot;Oui&quot;) cat(nrow(Requete3), &quot;enregistrements sélectionnés sur&quot;, nrow(InstallS.join)) ## 30 enregistrements sélectionnés sur 436 ## Sélection des SR avec deux critères et un opérateur OR (|) ########################################################################## ## Sommaires statistiques sur deux champs summary(SR.RMRSherbDonnees$LoyerMoyen) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 570.0 679.5 729.0 768.4 847.5 1200.0 summary(SR.RMRSherbDonnees$ValeurMedLog) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 200000 235500 250000 272520 300000 476000 ## Requête attributaire avec un opérateur OR Requete4 &lt;- subset(SR.RMRSherbDonnees, LoyerMoyen &lt; 700 | ValeurMedLog &lt; 250000) cat(nrow(Requete4), &quot;enregistrements sélectionnés sur&quot;, nrow(SR.RMRSherbDonnees)) ## 27 enregistrements sélectionnés sur 50 ## Sélection de différents types d&#39;installations sportives ########################################################################## unique(InstallS.join$TYPE) ## [1] &quot;Aréna&quot; &quot;Tir à l&#39;arc&quot; ## [3] &quot;Pétanque&quot; &quot;Jeu de galets&quot; ## [5] &quot;Planche à roulettes&quot; &quot;Préau et plancher de danse&quot; ## [7] &quot;Patinoire à bandes mobiles&quot; &quot;Surface, anneau ou étang glacé&quot; ## [9] &quot;Patinoire à bandes fixes&quot; &quot;Plage&quot; ## [11] &quot;Jeu d&#39;eau&quot; &quot;Piste multifonctionnelle&quot; ## [13] &quot;Glissade sur tube&quot; &quot;Jeu de fers&quot; ## [15] &quot;Piscine&quot; &quot;Tennis&quot; ## [17] &quot;Baseball&quot; &quot;Basketball&quot; ## [19] &quot;Football&quot; &quot;Volleyball&quot; ## [21] &quot;Ultimate frisbee&quot; &quot;Pickleball&quot; ## [23] &quot;Soccer&quot; &quot;Jeu modulaire&quot; ## Requête attributaire avec un opérateur %in% Requete5 &lt;- subset(InstallS.join, TYPE %in% c(&quot;Aréna&quot;, &quot;Piscine&quot;, &quot;Jeu d&#39;eau&quot;)) cat(nrow(Requete5), &quot;enregistrements sélectionnés sur&quot;, nrow(InstallS.join)) ## 26 enregistrements sélectionnés sur 436 References "],["sect013.html", "1.3 Manipulation de données raster", " 1.3 Manipulation de données raster 1.3.1 Mosaïquage et découpage d’images Dans cette section, nous abordons uniquement des fonctions simples de manipulation de données raster puisqu’un chapitre est dédié à l’analyse spatiale par l’analyse d’images. Une fois plusieurs images importées, il est fréquent de vouloir les fusionner. Pour ce faire, nous utilisons deux méthodes de terra : terra::merge fusionne plusieurs images (SpatRasters) pour former un nouvel objet SpatRasters dont l’étendue est recalculée en fonction des images fusionnées. Par contre, si les images se chevauchent, les valeurs des pixels dans les zones de chevauchement seront prises dans le même ordre que les images. terra::mosaic fusionne aussi plusieurs images. Toutefois, dans les zones de chevauchement, les moyennes des pixels sont calculées. Selon la documentation de terra, cette méthode serait plus rapide que la précédente. Dans le code ci-dessous, nous fusionnons les quatre feuillets de modèles numériques d’altitude (MNA) importés dans la section 1.1.2. ## Les GeoTIFF importés avec terra sont bien des SpatRaster class(f21e05_101) ## [1] &quot;SpatRaster&quot; ## attr(,&quot;package&quot;) ## [1] &quot;terra&quot; ## Création d&#39;une liste pour les cinq feuillets SpatRaster rlist &lt;- list(f21e05_101, f21e05_201, f31h08_102, f31h08_202, f21e12_101) rsrc &lt;- sprc(rlist) ## Création de la mosaïque MosaicSherb &lt;- mosaic(rsrc) MosaicSherb ## class : SpatRaster ## dimensions : 4187, 5575, 1 (nrow, ncol, nlyr) ## resolution : 9e-05, 9e-05 (x, y) ## extent : -72.25087, -71.74913, 45.24907, 45.6259 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## source(s) : memory ## name : f21e05_101 ## min value : 123.7184 ## max value : 845.0474 Vous constatez ci-dessous que la projection des images est lon/lat NAD83 (EPSG:4269). D’autres fonctions permettent de découper une image en fonction d’une autre image (objet SpatRaster de terra)) ou d’un objet terra vectoriel (SpatVector) : crop(x, y) découpe une image x en prenant l’étendue de y. mask(x, y) découpe une image x en prenant la zone (pixels avec des valeurs non nulles ou objets vectoriels) de y. Les pixels en dehors de cette zone auront nulle comme valeur (NA dans R). En guise d’exemple, découpons la mosaïque avec le polygone de la ville de Sherbrooke en utilisant la méthode mask. Attention, les deux sources de données doivent avoir la même projection et il faut préalablement convertir l’objet sf en objet terra. ## Changement de projection pour le polygone de la ville de Sherbrooke ## Application de la même projection que celle de la mosaïque VilleSherb.EPSG4269 &lt;- st_transform(Arrond.Union, crs(MosaicSherb)) # Convertir l&#39;objet sf en un objet SpatVector de terra VilleSherb.SpatVector = vect(VilleSherb.EPSG4269) ## Découpage de la mosaïque avec le polygone de la ville de Sherbrooke MosaicSherbCrop &lt;- terra::mask(MosaicSherb, VilleSherb.SpatVector) MosaicSherbCrop ## class : SpatRaster ## dimensions : 4187, 5575, 1 (nrow, ncol, nlyr) ## resolution : 9e-05, 9e-05 (x, y) ## extent : -72.25087, -71.74913, 45.24907, 45.6259 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## source(s) : memory ## name : f21e05_101 ## min value : 126 ## max value : 390 ## Constastez ci-dessus que le nom de l&#39;image est f21e05_101. ## Pour le changer, utilisez la fonction names() names(MosaicSherbCrop) = &quot;Elevation&quot; MosaicSherbCrop ## class : SpatRaster ## dimensions : 4187, 5575, 1 (nrow, ncol, nlyr) ## resolution : 9e-05, 9e-05 (x, y) ## extent : -72.25087, -71.74913, 45.24907, 45.6259 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat NAD83 (EPSG:4269) ## source(s) : memory ## name : Elevation ## min value : 126 ## max value : 390 ## Visualisation du résultat plot(MosaicSherbCrop) 1.3.2 Requêtes attributaires sur des images Avant d’effectuer une requête, il est judicieux d’explorer les valeurs des pixels de l’image avec un histogramme et la fonction summary(Nom de l'image) (valeurs minimales, maximales, quartiles, moyenne et valeurs nulles – NA). ## Sommaire statistique des valeurs summary(MosaicSherbCrop) ## Elevation ## Min. :126.0 ## 1st Qu.:197.7 ## Median :227.6 ## Mean :225.4 ## 3rd Qu.:251.4 ## Max. :389.9 ## NA&#39;s :78040 ## Histogramme hist(MosaicSherbCrop, main = &quot;Mosaïque du MNA pour la ville de Sherbrooke&quot;, xlab = &quot;Élévation (mètres)&quot;, ylab = &quot;Fréquence&quot;, col = &quot;lightgreen&quot;) ## Histogramme en barre de 125 à 400 avec un saut de 25 mètres hist(MosaicSherbCrop, main = &quot;Mosaïque du MNA pour la ville de Sherbrooke&quot;, xlab = &quot;Élévation (mètres)&quot;, ylab = &quot;Fréquence&quot;, breaks = seq(from = 125, to = 400, by = 25), col = &quot;lightgreen&quot;) ## Sélection des pixels avec une élévation d&#39;au moins 300 mètres MosaicSherbCrop300 = clamp(MosaicSherbCrop, lower = 300) plot(MosaicSherbCrop300, main = &quot;Pixels avec une élévation d&#39;au moins 300 mètres&quot;) ## Sélection des pixels avec une élévation de 200 à 300 mètres MosaicSherbCrop200_300 = clamp(MosaicSherbCrop, lower = 200, upper = 300) plot(MosaicSherbCrop200_300, main = &quot;Pixels avec une élévation de 200 à 300 mètres&quot;) "],["sect014.html", "1.4 Exportation de données spatiales de R vers des formats géographiques", " 1.4 Exportation de données spatiales de R vers des formats géographiques 1.4.1 Exportation de données vectorielles sf Pourquoi exporter des objets sf vers différents formats géographiques? Il manque plusieurs méthodes d’analyse de données spatiales dans les logiciels de SIG comme ArcGIS Pro ou QGIS d’où l’intérêt de recourir à R ou à Python. La démarche méthodologique classique comprend alors trois étapes : 1) importer des données géographiques, 2) réaliser des analyses avancées dans R, 3) exporter les résultats finaux vers différents formats géographiques (shapefile, GeoPackage, geodatabase d’ESRI). Trois raisons majeures motivent l’exportation des données : Cartographier les résultats finaux dans votre logiciel SIG préféré. Partager les données avec des personnes n’utilisant pas R. Réaliser éventuellement d’autres analyses dans votre logiciel de SIG préféré. Dans la section 1.1, nous avons vu que la fonction st_read() du package sf permet d’importer une multitude de formats géographiques. Comment en exporter avec sf? Rien de plus simple : avec la fonction st_write(). Le code ci-dessous illustre comment exporter des objets sf aux formats shapefile (shp), GeoPackage (GPKG), Keyhole Markup Language (kml) et GeoJSON. Par défaut, st_write() n’écrase pas un fichier existant; pour l’écraser, ajoutez le paramètre append = FALSE. ## Exportation au format shapefile st_write(PointsGPS, # couche sf &quot;data/chap01/export/PointsGPS.shp&quot;, # chemin et nom du fichier append = FALSE, # pour écraser le fichier s&#39;il existe driver = &quot;ESRI Shapefile&quot; ) ## Deleting layer `PointsGPS&#39; using driver `ESRI Shapefile&#39; ## Writing layer `PointsGPS&#39; to data source ## `data/chap01/export/PointsGPS.shp&#39; using driver `ESRI Shapefile&#39; ## Writing 89 features with 2 fields and geometry type Point. ## Exportation dans une couche dans GPKG st_write(PointsGPS, dsn = &quot;data/chap01/export/Data.gpkg&quot;, layer = &quot;PointsGPS&quot;, append = FALSE, driver = &quot;GPKG&quot;) ## Deleting layer `PointsGPS&#39; using driver `GPKG&#39; ## Writing layer `PointsGPS&#39; to data source ## `data/chap01/export/Data.gpkg&#39; using driver `GPKG&#39; ## Writing 89 features with 2 fields and geometry type Point. ## Exportation vers un fichier KML st_write(PointsGPS, dsn = &quot;data/chap01/export/PointsGPS.kml&quot;, append = FALSE, driver=&quot;KML&quot;) ## Writing layer `PointsGPS&#39; to data source ## `data/chap01/export/PointsGPS.kml&#39; using driver `KML&#39; ## Writing 89 features with 2 fields and geometry type Point. ## Exportation vers un fichier GeoJSON st_write(PointsGPS, dsn = &quot;data/chap01/export/PointsGPS.geojson&quot;, append = FALSE, driver=&quot;GeoJSON&quot;) ## Deleting layer not supported by driver `GeoJSON&#39; ## Deleting layer `PointsGPS&#39; failed ## Writing layer `PointsGPS&#39; to data source ## `data/chap01/export/PointsGPS.geojson&#39; using driver `GeoJSON&#39; ## Updating existing layer PointsGPS ## Writing 89 features with 2 fields and geometry type Point. Le paramètre driver de la fonction st_write permet de spécifier le format du fichier. Pour obtenir la liste des formats qu’il est possible d’importer et d’exporter, tapez dans la console ?st_drivers ou consultez le tableau 1.1. Tableau 1.1: Liste des formats avec le package sf (st_drivers) Nom Description Écriture Si vecteur Si raster ESRIC Esri Compact Cache FALSE TRUE TRUE netCDF Network Common Data Format TRUE TRUE TRUE PDS4 NASA Planetary Data System 4 TRUE TRUE TRUE VICAR MIPL VICAR file TRUE TRUE TRUE JP2OpenJPEG JPEG-2000 driver based on OpenJPEG library FALSE TRUE TRUE PDF Geospatial PDF TRUE TRUE TRUE MBTiles MBTiles TRUE TRUE TRUE BAG Bathymetry Attributed Grid TRUE TRUE TRUE EEDA Earth Engine Data API FALSE FALSE TRUE OGCAPI OGCAPI FALSE TRUE TRUE ESRI Shapefile ESRI Shapefile TRUE FALSE TRUE MapInfo File MapInfo File TRUE FALSE TRUE UK .NTF UK .NTF FALSE FALSE TRUE LVBAG Kadaster LV BAG Extract 2.0 FALSE FALSE TRUE OGR_SDTS SDTS FALSE FALSE TRUE S57 IHO S-57 (ENC) TRUE FALSE TRUE DGN Microstation DGN TRUE FALSE TRUE OGR_VRT VRT - Virtual Datasource FALSE FALSE TRUE Memory Memory TRUE FALSE TRUE CSV Comma Separated Value (.csv) TRUE FALSE TRUE GML Geography Markup Language (GML) TRUE FALSE TRUE GPX GPX TRUE FALSE TRUE KML Keyhole Markup Language (KML) TRUE FALSE TRUE GeoJSON GeoJSON TRUE FALSE TRUE GeoJSONSeq GeoJSON Sequence TRUE FALSE TRUE ESRIJSON ESRIJSON FALSE FALSE TRUE TopoJSON TopoJSON FALSE FALSE TRUE OGR_GMT GMT ASCII Vectors (.gmt) TRUE FALSE TRUE GPKG GeoPackage TRUE TRUE TRUE SQLite SQLite / Spatialite TRUE FALSE TRUE WAsP WAsP .map format TRUE FALSE TRUE MySQL MySQL TRUE FALSE TRUE OpenFileGDB ESRI FileGDB FALSE FALSE TRUE DXF AutoCAD DXF TRUE FALSE TRUE CAD AutoCAD Driver FALSE TRUE TRUE FlatGeobuf FlatGeobuf TRUE FALSE TRUE Geoconcept Geoconcept TRUE FALSE TRUE GeoRSS GeoRSS TRUE FALSE TRUE VFK Czech Cadastral Exchange Data Format FALSE FALSE TRUE PGDUMP PostgreSQL SQL dump TRUE FALSE TRUE OSM OpenStreetMap XML and PBF FALSE FALSE TRUE GPSBabel GPSBabel TRUE FALSE TRUE OGR_PDS Planetary Data Systems TABLE FALSE FALSE TRUE WFS OGC WFS (Web Feature Service) FALSE FALSE TRUE OAPIF OGC API - Features FALSE FALSE TRUE EDIGEO French EDIGEO exchange format FALSE FALSE TRUE SVG Scalable Vector Graphics FALSE FALSE TRUE Idrisi Idrisi Vector (.vct) FALSE FALSE TRUE XLS MS Excel format FALSE FALSE TRUE ODS Open Document/ LibreOffice / OpenOffice Spreadsheet TRUE FALSE TRUE XLSX MS Office Open XML spreadsheet TRUE FALSE TRUE Elasticsearch Elastic Search TRUE FALSE TRUE Carto Carto TRUE FALSE TRUE AmigoCloud AmigoCloud TRUE FALSE TRUE SXF Storage and eXchange Format FALSE FALSE TRUE Selafin Selafin TRUE FALSE TRUE JML OpenJUMP JML TRUE FALSE TRUE PLSCENES Planet Labs Scenes API FALSE TRUE TRUE CSW OGC CSW (Catalog Service for the Web) FALSE FALSE TRUE VDV VDV-451/VDV-452/INTREST Data Format TRUE FALSE TRUE MVT Mapbox Vector Tiles TRUE FALSE TRUE NGW NextGIS Web TRUE TRUE TRUE MapML MapML TRUE FALSE TRUE TIGER U.S. Census TIGER/Line FALSE FALSE TRUE AVCBin Arc/Info Binary Coverage FALSE FALSE TRUE AVCE00 Arc/Info E00 (ASCII) Coverage FALSE FALSE TRUE HTTP HTTP Fetching Wrapper FALSE TRUE TRUE 1.4.2 Exportation de données raster L’exportation d’objets SpatRasters de terra est très simple avec la méthode terra::writeRaster. En guise d’exemple, le code ci-dessous exporte la mosaïque de MNA dans un fichier GeoTIFF. Notez que le paramètre filetype permet de spécifier d’autres formats d’images de la liste qui est disponible au lien suivant : https://gdal.org/drivers/raster/index.html. En guise d’exemple, les paramètres EIR, ENVI, RST, ERS et GRASS permettent d’exporter vers les logiciels de télédétection ERDAS, ENVI, Idrisi, ERMapper et GRASS, tandis que le paramètre GPKG permet d’exporter vers un GeoPackage raster. terra::writeRaster(MosaicSherbCrop, &quot;data/chap01/export/MosaicSherb.tif&quot;, filetype = &quot;GTiff&quot;, overwrite = TRUE) "],["sect015.html", "1.5 Cartographie avec R", " 1.5 Cartographie avec R Pourquoi cartographier des données dans R? Vous avez certainement un logiciel de SIG préféré pour construire une carte thématique (QGIS ou ArcGIS Pro par exemple). Puisqu’en quelques clics de souris, il est facile de réaliser une carte dans un SIG, quel est donc l’intérêt d’écrire des lignes de code pour afficher une carte dans R? Autrement dit, pourquoi devriez-vous vous compliquer la vie à apprendre de la syntaxe R pour produire une simple carte? Eh bien, savoir cartographier dans R a plusieurs avantages : Cartographier rapidement les résultats d’une analyse dans R permet d’éviter des allers-retours incessants (exportation et importation de données) entre R et un logiciel de SIG. Or, la cartographie fait partie intégrante d’une démarche méthodologique d’analyse ou de modélisation spatiale. Vous restez ainsi dans le même environnement de travail (R) jusqu’à l’obtention de vos résultats finaux. Une fois ces derniers obtenus, vous pouvez les exporter et construire une carte très élaborée dans un logiciel de SIG. La syntaxe R n’est pas si compliquée. Les quelques lignes de code écrites pour une première analyse peuvent être réutilisées, modifiées et bonifiées pour une autre analyse. Au fil de vos projets, vous construirez des cartes de plus en plus élaborées. Autrement dit, après quelques heures d’investissement, vous deviendrez une personne experte en cartographie dans R! Quels packages utiliser pour la cartographie dans R? Il existe plusieurs packages R pour la cartographie, notamment : ggplot2 est certainement le meilleur package R pour réaliser des graphiques (Wickham 2016). Il permet désormais de construire des cartes. cartography permet de construire efficacement des cartes thématiques (Giraud et Lambert 2016). Pour avoir une idée de son potentiel, consultez cette très belle Cheatsheet. tmap (Tennekes 2018) est certainement à l’heure actuelle le package le plus complet et le plus utilisé. Des packages spécifiques permettent de créer des cartes interactives sur Internet, notamment mapview, mapdeck et leaflet. Ce dernier est basé sur la librairie JavaScript largement utilisée dans le domaine de la cartographie sur Internet. Dans le cadre de la carte de cette section, nous utilisons uniquement tmap dont plusieurs ressources sont disponibles sur Internet : Sur le site CRAN de tmap, une excellente vignette intitulée tmap: get started! Un article dans Journal of Statistical Software de Martijn Tennekes, créateur du package tmap. La documentation complète en PDF. 1.5.1 Manipulation des couches géométriques 1.5.1.1 Principales fonctions de représentation de couches vectorielles et matricielles Il existe trois principales fonctions pour paramétrer l’affichage de couches géographiques (tableau 1.2). Tableau 1.2: Principales fonctions pour manipuler des couches vectorielles et matricielles Fonction Description Points Lignes Polyg. Raster Fonction principale tm_shape Crée un élément tmap à partir d’une couche géographique vectorielle ou matricielle (objet sf ou terra) X X X X Fonctions de base de manipulation tm_polygons Dessine des polygones (couleur et contour) X tm_symbols Dessine des symboles X X X tm_lines Dessine des lignes X tm_text Dessine des étiquettes à partir d’un champ X X X tm_raster Affiche un raster X Autres fonctions de manipulation tm_fill Dessine l’intérieur de polygones X tm_border Dessine les contours X tm_bubbles Dessine des cercles (notamment proportionnels) X X X tm_squares Dessine des carrés (notamment proportionnels) X X X tm_dots Dessine des points X X X tm_markers Dessine des icones avec étiquettes X X X Construction d’une carte simple avec une couche vectorielle et une couche matricielle Le code ci-dessous permet d’afficher deux couches avec la fonction tm_shape : l’une vectorielle, l’autre matricielle (figure 1.6). tmap_mode(&quot;plot&quot;) # 1er objet tmap pour une couche raster tm_shape(MosaicSherbCrop)+ tm_raster(palette = terrain.colors(10))+ # 1er objet tmap pour une couche vectorielle tm_shape(Arrondissements)+ tm_borders(col = &quot;black&quot;, lwd = 3)+ # contour noir avec une épaisseur de trois points tm_text(&quot;NUMERO&quot;) # Étiquettes identifiant l&#39;arrondissement Figure 1.6: Exemple de carte tmap avec une couche polygonale et une image Ordre et hiérarchie des couches avec tmap. Vous avez compris qu’une couche est affichée avec la fonction tm_shape et que le + permet d’ajouter une ou plusieurs fonctions d’habillage à cette couche (tm_polygons, tm_lines, tm_text, tm_raster, etc.). Il est possible d’en superposer en utilisant plusieurs tm_shape comme suit : tm_shape(Nom de la première couche)+ ... paramètres de la couche + tm_shape(Nom de la seconde couche)+ ... paramètres de la couche Notez que la première couche est celle avec laquelle la projection et l’étendue de la carte sont définies. Il est toutefois possible de changer le tout en utilisant l’argument is.master = TRUE dans le tm_shape d’une couche donnée. Construction d’une carte avec plusieurs couches vectorielles Les lignes de code suivantes permettent de construire la figure 1.7 avec trois couches sf. tmap_mode(&quot;plot&quot;) ## Polygones tm_shape(Arrondissements)+ tm_text(&quot;NUMERO&quot;)+ # Étiquettes identifiant l&#39;arrondissement tm_polygons(col=&quot;wheat&quot;, border.col = &quot;black&quot;, lwd = 3)+ ## Lignes tm_shape(Rues)+ tm_lines(col= &quot;gray&quot;, lwd = 1)+ ## Points tm_shape(PointsGPS.Sherb)+ tm_dots(shape=21, col=&quot;blue&quot;, size=.3) Figure 1.7: Exemple de carte tmap avec plusieurs couches vectorielles (polygones, lignes, points) La figure 1.8 illustre la différence entre les fonctions tm_dots et tm_markers. ## Points avec tm_dots() CartePoints = tm_shape(Arrondissements) + tm_polygons(col=&quot;wheat&quot;, border.col = &quot;black&quot;) + tm_shape(PointsGPS.Sherb) + tm_dots(shape=21, col=&quot;blue&quot;, size=.3) ## Icones avec tm_markers() CarteMarkers = tm_shape(Arrondissements) + tm_polygons(col=&quot;wheat&quot;, border.col = &quot;black&quot;) + tm_shape(PointsGPS.Sherb) + tm_markers(size = 0.2) ## Combinaison des deux cartes tmap_arrange(CartePoints, CarteMarkers, ncol=2, nrow=1) Figure 1.8: Exemple de carte tmap avec tm_dots et tm_markers 1.5.1.2 Couleurs uniques et palette de couleurs dans tmap Vous avez remarqué plus haut que plusieurs fonctions comprennent l’argument col pour spécifier une couleur. Pour connaître les trois manières de spécifier une couleur dans R – nom de la couleur R (lightblue par exemple), code hexadécimal (#f03b20 par exemple) ou notation RVB (rgb(0.2, 0.4, 0.4, 0) par exemple) –, consultez la section suivante (Apparicio et Gelb 2022). Pour spécifier une palette de couleurs sur un champ dans différentes fonctions (entre autres, tm_polygons, tm_lines, tm_fill, tm_dots), il suffit d’utiliser deux arguments dans la fonction, soit col=\"Nom du champ\" et palette=\"nom de la palette de couleurs\". Le package tmap intègre les palettes de deux autres packages : viridisLite (Garnier et al. 2021) et RColorBrewer (Neuwirth 2022). Le premier propose cinq palettes de couleurs : viridis, magma, plasma, inferno, cividis. Le second intègre une série de palettes de couleurs proposées par la géographe et cartographe Cynthia Brewer et ses collègues (Harrower et Brewer 2003; Brewer, Hatchard et Harrower 2003). Vous avez probablement déjà exploré leur site Internet leur site Internet où il est possible de sélectionner une palette en fonction du nombre de classes, de la nature des données et de la codification des couleurs (HEX, RGB, CMYK). Succinctement, RColorBrewer propose plusieurs palettes regroupées selon trois catégories : Palettes qualitatives à appliquer à une variable qualitative nominale comme son nom l’indique (figure 1.9). Pour afficher les palettes et connaître leurs noms, tapez display.brewer.all(type=\"qual\") dans la console. Palettes séquentielles pour une variable continue avec des valeurs faibles à fortes (figure 1.10). Tapez display.brewer.all(type=\"seq\") dans la console. Palettes divergentes à appliquer à une variable continue dont les valeurs aux deux extrémités s’opposent (figure 1.11). Tapez display.brewer.all(type=\"div\") dans la console. Figure 1.9: Palettes de couleurs qualitatives de RColorBrewer Figure 1.10: Palettes de couleurs séquentielles de RColorBrewer Figure 1.11: Palettes de couleurs divergentes RColorBrewer Comparaison de palettes avec un nombre de classes défini Si vous connaissez le nombre de classes et que vous hésitez à choisir telle ou telle palette de couleurs, tapez dans la console : display.brewer.all(n=5, type=\"seq\", exact.n=TRUE) display.brewer.all(n=5, type=\"div\", exact.n=TRUE) display.brewer.all(n=5, type=\"qual\", exact.n=TRUE) D’autres arguments peuvent être ajoutés comme colorblindFriendly=TRUE qui renvoie uniquement des palettes de couleurs adaptées aux personnes daltoniennes. En guise d’exemple, avec cinq classes, il est possible de comparer neuf palettes divergentes et six autres adaptées aux personnes daltoniennes (figure 1.12). Figure 1.12: Palettes de couleurs divergentes de RColorBrewer avec cinq classes Vous hésitez encore à choisir une palette de couleurs? Tapez la syntaxe ci-dessous dans la console pour afficher l’ensemble des palettes des packages RColorBrewer et viridisLite. tmaptools::palette_explorer() Pour inverser les couleurs d’une palette, vous devez précéder le nom de la palette par un signe moins (exemple : -Greens). 1.5.1.3 Cartographie d’une variable qualitative : valeurs uniques Application à une couche de points Le code ci-dessous illustre comment construire une carte thématique avec des couleurs appliquées à une variable qualitative nominale (champ TYPE de la couche InstallationSport) d’une couche de points (figure 1.13). ## Carte tm_shape(Arrondissements)+ tm_borders()+ tm_shape(InstallationSport)+ tm_dots(shape = 21, size=.3, col= &quot;TYPE&quot;, palette = &quot;Set1&quot;, title =&quot;Type d&#39;installation&quot;)+ tm_layout(main.title = &quot;Installations sportives&quot;, frame=FALSE, legend.position = c(&quot;left&quot;, &quot;top&quot;), legend.outside=TRUE) Figure 1.13: Exemple de cartographie d’une variable qualitative sur des points Application à une couche de lignes Le code ci-dessous illustre comment construire une carte thématique avec des couleurs appliquées à une variable qualitative nominale (champ TYPESEGMEN) d’une couche de lignes (figure 1.14). tmap_mode(&quot;plot&quot;) ## Listes des valeurs uniques table(Rues$TYPESEGMEN) ## ## Artère Autoroute Chemin privé Collectrice Locale ## 1459 380 467 579 4846 ## Lignes tm_shape(Rues)+ tm_lines(col= &quot;TYPESEGMEN&quot;, palette = c(&quot;red&quot;, &quot;brown4&quot;, &quot;cornsilk1&quot;, &quot;lightpink&quot;, &quot;gainsboro&quot;), lwd = 2 ) Figure 1.14: Exemple de cartographie d’une variable qualitative sur des lignes Application à une couche de polygones Le code ci-dessous illustre comment construire une carte thématique avec des couleurs appliquées à une variable qualitative nominale (champ SDRNOM de la couche AD2021) d’une couche de polygones (figure 1.15). ## Importation de la couche des aires de diffusion de 2021 pour la RMR de Sherbrooke AD2021 &lt;- st_read(dsn = &quot;data/chap01/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;SherbAD&quot;, quiet = TRUE) ## Carte tmap_mode(&quot;plot&quot;) tm_shape(AD2021)+ tm_fill(col= &quot;SDRNOM&quot;, palette = &quot;Set2&quot;, lwd = 1, title =&quot;Municipalité&quot;)+ tm_borders(col=&quot;black&quot;)+ tm_layout(main.title = &quot;Aires de diffusion de 2021&quot;, frame =FALSE, legend.position = c(&quot;left&quot;, &quot;top&quot;), legend.outside=TRUE) Figure 1.15: Exemple de cartographie d’une variable qualitative sur des polygones 1.5.1.4 Cartographie d’une variable discrète : cercles proportionnels La syntaxe ci-dessous permet de créer une carte avec des cercles proportionnels pour les municipalités de la région administrative de l’Estrie (figure 1.16). ## Importation des municipalités (subdivisions de recensements - SDR) de l&#39;Estrie SDR.Estrie &lt;- st_read(dsn = &quot;data/chap01/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;sdr_Estrie&quot;, quiet = TRUE) ## Importation des MRC (divisions de recensements - DR) de l&#39;Estrie DR.Estrie &lt;- st_read(dsn = &quot;data/chap01/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DREstrie2021&quot;, quiet = TRUE) ## Importation des données sur la population PopSDR &lt;- read.csv(&quot;data/chap01/tables/SDR_Estrie.csv&quot;) PopSDR$SDRidu &lt;- as.character(PopSDR$SDRidu) ## Fusion des données SDR.Estrie &lt;- merge(SDR.Estrie, PopSDR, by.x = &quot;SDRIDU&quot;, by.y = &quot;SDRidu&quot;) ## Construction de la carte tmap_mode(&quot;plot&quot;) tm_shape(SDR.Estrie)+ tm_polygons(col=&quot;whitesmoke&quot;, border.col = &quot;grey30&quot;, lwd = 1)+ tm_bubbles(size = &quot;SDRpop_2021&quot;, border.col = &quot;black&quot;, col = &quot;tomato1&quot;, title.size = &quot;Population&quot;, scale = 3)+ # facteur multiplicateur pour la taille du cercle tm_shape(DR.Estrie)+ tm_borders(col=&quot;black&quot;, lwd = 2) Figure 1.16: Exemple de carte avec des cercles proportionnels 1.5.1.5 Cartographie d’une variable continue : cartes choroplèthes et méthodes de discrétisation L’argument style, qui est commun à plusieurs fonctions (tm_polygons, tm_fill, tm_lines, tm_dots, etc.), permet de choisir une méthode de discrétisation dont les principales sont : fixed: intervalles fixés par l’analyste. equal: intervalles égaux. pretty: intervalles arrondis aux nombres entiers. quantile: selon les quantiles (même nombre d’observations dans chaque classe). jenks: selon la méthode de Jenks. sd: selon l’écart-type. D’autres méthodes peuvent être utilisées comme kmeans, hclust, bclust, fisher, dpih, headtails et log10_pretty. En guise d’exemple, la figure 1.18 présente une discrétisation en cinq classes selon la méthode des quantiles. Notez aussi qu’il est possible de réaliser une carte avec un dégradé continu avec style = \"cont\" tel qu’illustré ci-dessous (figure 1.17). ## Sélection des aires de diffusion de Sherbrooke AD2021.sherb &lt;- subset(AD2021, SDRNOM == &quot;Sherbrooke&quot;) ## Carte tmap_mode(&quot;plot&quot;) tm_shape(AD2021.sherb)+ tm_fill(col= &quot;HabKm2&quot;, palette = &quot;Reds&quot;, style = &quot;cont&quot;, title =&quot;Hab./km2&quot;)+ tm_borders(col=&quot;black&quot;) Figure 1.17: Exemple de carte choroplèthe avec une palette continue La figure 1.18 utilise une discrétisation selon la méthode de quantiles avec cinq classes. Autrement dit, chaque classe comprend 20 % des aires de diffusion de la ville de Sherbrooke. tmap_mode(&quot;plot&quot;) tm_shape(AD2021.sherb)+ tm_fill(col= &quot;HabKm2&quot;, palette = &quot;Reds&quot;, n = 5, # nombre de classes style = &quot;quantile&quot;, title =&quot;Hab./km2&quot;)+ tm_borders(col=&quot;black&quot;, lwd = .5) Figure 1.18: Exemple de carte choroplèthe avec une discrétisation selon les quantiles La figure 1.19 présente quatre méthodes de discrétisation différentes pour le revenu médian des ménages par secteur de recensement dans la région métropolitaine de recensement de Sherbrooke en 2021. Figure 1.19: Différentes méthodes de discrétisation 1.5.2 Cartes interactives Avec la fonction tmap_mode, il possible de choisir l’un des deux modes de visualisation suivants : statique avec tmap_mode(\"plot\"). interactif avec tmap_mode(\"view\"). Vous constaterez ci-dessous que par défaut, trois fonds de carte sont disponibles dans la carte interactive, soit dans l’ordre Esri.WorldGrayCanvas, OpenStreetMap et Esri.WorldTopoMap. ## Mode active tmap tmap_mode(&quot;view&quot;) ## Importation des couches Arrond.sf = read_sf(&quot;data/chap01/shp/Arrondissements.shp&quot;) InstallSR.sf = read_sf(&quot;data/chap01/shp/Installations_sportives_et_recreatives.shp&quot;) ## Carte tm_shape(InstallSR.sf)+ tm_dots(size = 0.05, shape = 21, col = &quot;red&quot;)+ tm_shape(Arrond.sf)+ tm_borders(col=&quot;black&quot;, lwd= .5) Il est possible de changer les fonds de carte avec la fonction tm_basemap tandis que la fonction tm_tiles permet de superposer une tuile (pour la toponymie par exemple) (tableau 1.3). Tableau 1.3: Fonctions pour des cartes interactives Fonction Description tmap_mode Choisir le mode statistique ou interactive tm_basemap Spécifier une carte de fonds tm_tiles Spécifier une tuile de fonds (pour des étiquettes par exemple) Dans le code ci-dessous, nous utilisons uniquement deux fonds de carte. Remarquez les lignes avec l’argument popup.vars qui permet de définir les champs visibles dans la fenêtre surgissante (pop-up). Cliquez sur une installation sportive pour activer la fenêtre surgissante. ## Carte tm_basemap(c(&quot;OpenStreetMap&quot;, &quot;Esri.WorldTopoMap&quot;))+ tm_shape(InstallSR.sf)+ tm_dots(size = 0.05, shape = 21, col = &quot;red&quot;, # définition pour le pop-up (clic sur une installation) popup.vars=c(&quot;Nom : &quot;=&quot;NOM&quot;, &quot;Type : &quot; = &quot;TYPE&quot;, &quot;Éclairage : &quot; = &quot;ECLAIRAGE&quot;, &quot;Éclairage : &quot; = &quot;SURFACE&quot;), id = &quot;OBJECTID&quot;)+ tm_shape(Arrond.sf)+ tm_borders(col=&quot;black&quot;, lwd= .5)     Où trouver des fonds de carte? Une liste des fonds de carte Leaflet est disponible au lien suivant. 1.5.3 Mise en page d’une carte Les principales fonctions de mise en page d’une carte sont présentées au tableau 1.4. Tableau 1.4: Fonctions d’habillage d’une carte Fonction Description Principaux arguments Fonctions de composition d’une carte tm_facets Créer un élément tmap avec plusieurs vignettes by: groupé par colonne. nrow et ncol: nombres de lignes et de colonnes tmap_arrange Fusionner plusieurs cartes dans une mise en page nrow et ncol: nombre de lignes et de colonnes Fonctions d’habillage d’une carte tm_grid Ajouter une grille de lignes de coordonnées (ex. long/lat) x et y: vecteurs pour les coordonnées tm_credits Créer un texte pour spéficier l’auteur.e ou la source de la carte text: texte. size: taille du texte. fontfamily: police du texte tm_scale_bar Créer une échelle break: vecteur numérique pour l’échelle. position: position de l’échelle avec les valeurs left, center, right, bottom, top. Par exemple c(‘left’, ‘bottom’) tm_compass Créer une flèche du nord type: type de flèche du Nord (‘arrow’, ‘4star’, ‘8star’, ‘radar’, ‘rose’) tm_logo Ajouter un logo à une carte file: chemin et nom du fichier ou URL tm_xlab Ajouter un titre sur l’axe des X de la carte text: nom de l’axe tm_ylab Ajouter un titre sur l’axe des Y de la carte text: nom de l’axe tm_layout Spécifier des éléments de mise en page de la carte title: titre de la carte tm_legend Paramétrer la légende de la carte position: position de la légende avec les valeurs left, center, right, bottom, top tmap_options Paramétrer et conserver plusieurs options sur la carte unit: unités de mesures (‘imperial’, ‘km’, ‘m’, ‘mi’, and ‘ft’) 1.5.3.1 Combinaison de plusieurs cartes Tel que décrit dans le tableau 1.4, il existe deux fonctions pour combiner deux cartes : tmap_arrange et tm_facets. Pour ceux et celles réalisant régulièrement des graphiques dans R avec ggplot2, tmap_arrange est très similaire à la fonction ggarrange du package ggpubr qui permet de fusionner plusieurs graphiques. Globalement, le principe est le suivant : vous réalisez deux cartes ou plus que vous combinez dans une même sortie avec tmap_arrange. Vous trouverez ci-dessous un exemple avec deux cartes (figure 1.20). tmap_mode(&quot;plot&quot;) ## Carte 1 Carte1 = tm_shape(SDR.Estrie)+ tm_polygons(col=&quot;whitesmoke&quot;, border.col = &quot;grey30&quot;, lwd = 1)+ tm_bubbles(size = &quot;SDRpop_2021&quot;, border.col = &quot;black&quot;, col = &quot;tomato1&quot;, title.size = &quot;Population&quot;, scale = 3)+ # facteur multiplicateur pour la taille du cercle tm_shape(DR.Estrie)+ tm_borders(col=&quot;black&quot;, lwd = 2) ## Calcul de la densité de population SDR.Estrie$HabKm2 &lt;- as.numeric(SDR.Estrie$SDRpop_2021 / (st_area(SDR.Estrie) / 1000000)) ## Carte 2 Carte2 = tm_shape(SDR.Estrie)+ tm_fill(col= &quot;HabKm2&quot;, palette = &quot;Reds&quot;, style = &quot;quantile&quot;, n = 4, title =&quot;Hab./km2&quot;)+ tm_borders(col=&quot;black&quot;)+ tm_shape(DR.Estrie)+ tm_borders(col=&quot;black&quot;, lwd = 2) ## Combinaison des deux cartes tmap_arrange(Carte1, Carte2, ncol = 2, nrow = 1) Figure 1.20: Exemple de combinaison de carte avec tmap_arrange Quant à la fonction tm_facets, elle permet de créer plusieurs cartes avec l’argument by. Prenons un exemple concret : vous disposez d’une couche géographique des municipalités du Québec et vous souhaitez réaliser une carte pour chaque région administrative. L’argument by = \"Region\" vous permet alors d’avoir une vignette par région. Dans l’exemple ci-dessous, nous avons cartographié la même variable (densité de population) pour différentes zones de la région métropolitaine de Sherbrooke (figure 1.21). tmap_mode(&quot;plot&quot;) ## Création d&#39;une variable zone basée sur les noms des municipalités AD2021$Zone &lt;- ifelse(AD2021$SDRNOM == &quot;Sherbrooke&quot;, &quot;A. Sherbrooke&quot;, &quot;&quot;) AD2021$Zone &lt;- ifelse(AD2021$SDRNOM %in% c(&quot;Compton&quot;, &quot;Waterville&quot;, &quot;Hatley&quot;, &quot;North Hatley&quot;), &quot;B. Sud&quot;, AD2021$Zone) AD2021$Zone &lt;- ifelse(AD2021$SDRNOM %in% c(&quot;Orford&quot;, &quot;Magog&quot;, &quot;Saint-Denis-de-Brompton&quot;), &quot;C. Est&quot;, AD2021$Zone) AD2021$Zone &lt;- ifelse(AD2021$SDRNOM %in% c(&quot;Ascot Corner&quot;, &quot;Val-Joli&quot;, &quot;Stoke&quot;), &quot;C. Nord&quot;, AD2021$Zone) ## Création des cartes avec tm_facets tmap_mode(&quot;plot&quot;) tm_shape(AD2021)+ tm_fill(col= &quot;HabKm2&quot;, palette = &quot;Reds&quot;, n = 5, # nombre de classes style = &quot;quantile&quot;, title =&quot;Hab./km2&quot;)+ tm_borders(col=&quot;black&quot;, lwd = .5)+ tm_facets(by = &quot;Zone&quot;) Figure 1.21: Premier exemple de combinaison de carte à tm_facets L’utilisation de tm_facets peut être également très utile pour comparer les distributions spatiales de points à différentes années (figure 1.22). tmap_mode(&quot;plot&quot;) ## Importation des incidents Incidents &lt;- st_read(&quot;data/chap01/shp/IncidentsSecuritePublique.shp&quot;, quiet = TRUE) ## Création des cartes avec tm_facets tmap_mode(&quot;plot&quot;) tm_shape(Arrondissements) + tm_polygons(col=&quot;wheat&quot;, border.col = &quot;black&quot;) + tm_shape(Incidents) + tm_dots(shape=21, col=&quot;blue&quot;, size=.2) + tm_facets(by = &quot;ANNEE&quot;) Figure 1.22: Deuxième exemple de combinaison de carte à tm_facets 1.5.3.2 Mise en page d’une carte Nous reprenons la figure 1.20 et l’habillons en ajoutant une échelle (tm_scale_bar), une flèche du Nord (tm_compass), la source et l’auteur (tm_credits) et un titre (tm_layout) (figure 1.23). ## Carte 1 tmap_mode(&quot;plot&quot;) tm_shape(SDR.Estrie)+ tm_fill(col= &quot;HabKm2&quot;, palette = &quot;Greens&quot;, style = &quot;quantile&quot;, n = 4, title =&quot;Hab./km2&quot;)+ tm_bubbles(size = &quot;SDRpop_2021&quot;, border.col = &quot;black&quot;, col = &quot;tomato1&quot;, scale = 3, title.size = &quot;Population&quot;)+ tm_borders(col=&quot;black&quot;)+ ## Ajout de de la flèche du Nord tm_compass(position = c(&quot;right&quot;, &quot;bottom&quot;), size = 2)+ ## Ajout de l&#39;échelle tm_scale_bar(breaks = c(0, 25, 50), position = c(&quot;right&quot;, &quot;bottom&quot;))+ ## Ajout de la source tm_credits(&quot;Source : recensement de 2021, Statistique Canada\\nAuteur : Jéremy Lacartemplace.&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;), size = 0.7, align = &quot;right&quot;) + ## Légende tm_legend(position = c(&quot;left&quot;, &quot;top&quot;), frame = FALSE, bg.color = &quot;white&quot;)+ ## Modification de la mise en page tm_layout(main.title = &quot;Municipalités de l&#39;Estrie&quot;, legend.outside = TRUE, frame = FALSE) Figure 1.23: Habillage d’une carte Aller plus loin avec tmap? Pour être honnête, nous avons abordé uniquement les principales fonctions et arguments pour l’habillage d’une carte. Plusieurs exemples de très belles cartes créées avec tmap sont disponibles aux ressources suivantes : L’excellente vignette intitulée tmap: get started! Visualizing Spatial Data in R with tmap Making Maps with R le chapitre Making maps with R du livre Geocomputation with R 1.5.4 Exportation d’une carte Une fois la carte finalisée, il est possible de l’exporter dans différents formats avec la fonction tmap_save : En mode image (png, jpg, bmp, tiff) pour l’insérer dans un logiciel de traitement de texte (Word ou OpenOffice Writer) ou dans un éditeur LaTeX (Overleaf par exemple). En mode vectoriel (pdf ou svg) pour finaliser l’édition de la carte dans un logiciel de création graphique vectorielle (Illustrator par exemple). En HTML dans lequel la carte sera intégrée selon le mode de visualisation interactive, sous la forme d’un widget Leaflet. ## Transformation en long/lat ## Carte 1 tmap_mode(&quot;plot&quot;) Carte1 &lt;- tm_shape(SDR.Estrie)+ tm_fill(col= &quot;HabKm2&quot;, palette = &quot;Greens&quot;, style = &quot;quantile&quot;, n = 4, title =&quot;Hab./km2&quot;)+ tm_bubbles(size = &quot;SDRpop_2021&quot;, border.col = &quot;black&quot;, col = &quot;tomato1&quot;, scale = 3, title.size = &quot;Population&quot;)+ tm_borders(col=&quot;black&quot;)+ tm_compass(position = c(&quot;right&quot;, &quot;bottom&quot;), size = 2)+ tm_scale_bar(breaks = c(0, 25, 50), position = c(&quot;right&quot;, &quot;bottom&quot;))+ tm_credits(&quot;Source : recensement de 2021, Statistique Canada\\nAuteur : Jéremy Lacartemplace.&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;), size = 0.7, align = &quot;right&quot;) + tm_legend(position = c(&quot;left&quot;, &quot;top&quot;), frame = FALSE, bg.color = &quot;white&quot;)+ tm_layout(main.title = &quot;Municipalités de l&#39;Estrie&quot;, legend.outside = TRUE, frame = FALSE) ## Exportation de la Carte1 au format png tmap_save(Carte1, filename = &quot;data/chap01/export/Carte1.png&quot;, dpi = 600) ## Exportation de la Carte1 au format PDF tmap_save(Carte1, filename = &quot;data/chap01/export/Carte1.pdf&quot;) ## Exportation de la Carte1 au format HTML tmap_save(Carte1, filename = &quot;data/chap01/export/Carte1.html&quot;) References "],["sect016.html", "1.6 Quiz de révision du chapitre", " 1.6 Quiz de révision du chapitre La classe sf est composée de trois éléments : Relisez au besoin la section 1.1.1.4.2. simple feature geometry (sfg) : géométrie d’une observation simple feature column (sfc) : liste toutes les géométries d’une couche data.fame : données attributaires raster : données images Laquelle de ces fonctions permet de reprojecter une couche géographique? Relisez au besoin le début de la section 1.2.1. st_crs(x) st_transform(x, crs) st_is_longlat(x) Laquelle de ces fonctions n’est pas une fonction géométrique sur une couche? Relisez au besoin la section 1.2.2. st_bbox(x) st_union(x) st_point_on_surface(x) st_crop(x, y, xmin, ymin, xmax, ymax) st_centroid(x) Comparativement à l’algorithme de Douglas et Peucker, que permet l’algorithme de Visvalingam lors de la simplification des contours? Relisez au besoin la section 1.2.2.4. Il est plus rapide. Il permet de conserver les frontières. Tous les points sont compris dans leur enveloppe convexe. Relisez au besoin la section 1.2.2.5. Vrai Faux Quelles sont les quatre fonctions de mesures géométriques et de récupération des coordonnées géographiques? Relisez au besoin la section 1.2.4. st_area(x) st_length(x) st_distance(x,y) st_coordinates(x) st_union(x) Quelle est la différence entre les fonctions st_intersects(x, y) et st_intersection(x, y)? Relisez le deuxième encadré à la section 1.2.6. Elles génèrent le même résultat. La première est une requête spatiale, la seconde renvoie l’intersection entre deux couches. La première renvoie l’intersection entre deux couches, la seconde est une requête spatiale. Laquelle des fonctions sf permet d’exporter des données vectorielles? Relisez au besoin la section 1.4.1. st_read() st_write() writeRaster() Vérifier votre résultat "],["sect017.html", "1.7 Exercices de révision", " 1.7 Exercices de révision Exercice 1. Découpez les rues de l’arrondissement des Nations de la ville de Sherbrooke. Complétez le code ci-dessous avec les étapes suivantes : Requête attributaire pour créer un objet sf avec uniquement l’arrondissement des Nations à partir de la couche Arrondissements et le champ NOM (voir la section 1.2.7.4). Découpez les rues (Rues) sur le nouvel objet sf (voir la section 1.2.3). library(sf) ## Importation des deux couches Arrond &lt;- st_read(&quot;data/chap01/shp/Arrondissements.shp&quot;, quiet = TRUE) Rues &lt;- st_read(&quot;data/chap01/shp/Segments_de_rue.shp&quot;, quiet = TRUE) ## Requête attributaire : création d&#39;un objet sf pour l&#39;arrondissement des Nations table(Arrond$NOM) Arrond.DesNations &lt;- subset(À compléter) ## Découper les rues avec le polygone de l&#39;arrondissement des nations Rues.DesNations &lt;- À compléter Correction à la section 10.1.1. Exercice 2. Calculez un nouveau champ (DistHVKM) dans la couche des aires de diffusion (AD) (AD.RMRSherb) qui représente la distance en kilomètres entre l’hôtel de ville de Sherbrooke et les points des AD. Puis, cartographiez le champ DistHVKM en quatre classes selon la méthode de discrétisation par quantiles. Complétez le code ci-dessous avec les étapes suivantes : Ajout d’un champ pour la distance (DistHVKM) dans la couche AD.RMRSherb (voir la section 1.2.4). Cartographiez le champ DistHVKM en quatre classes selon la méthode des quantiles (voir la section 1.5.1.5). library(sf) library(tmap) ## Importation des deux couches AD.RMRSherb &lt;- st_read(dsn = &quot;data/chap01/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;SherbAD&quot;, quiet = TRUE) HotelVille &lt;- data.frame(ID = 1, Nom = &quot;Hôtel de ville&quot;, lon = -71.89306, lat = 45.40417) HotelVille &lt;- st_as_sf(HotelVille, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) ## Changement de projection avant de s&#39;assurer que les deux couches ont la même HotelVille &lt;- st_transform(HotelVille, st_crs(AD.RMRSherb)) ## Ajout d&#39;un champ pour la distance en km à l&#39;hôtel de ville pour les secteurs de recensement AD.RMRSherb$DistHVKM &lt;- À compléter ## Cartographie en quatre classes selon les quantiles tmap_mode(&quot;plot&quot;) tm_shape(À compléter)+ tm_fill(À compléter)+ tm_borders(col=&quot;black&quot;) Correction à la section 10.1.2. Exercice 3. Importez une couche shapefile pour les divisions de recensement et calculez la densité de population (nombre d’habitants au km2). Complétez le code ci-dessous avec les étapes suivantes : Jointure attributaire entre la couche DR.Qc et la table DR.Data (voir la section 1.2.7.2). Calculer du champ HabKm2, soit la division entre les champs DRpop_2021 et SUPTERRE (voir la section 1.5.1.5). library(sf) ## Importation de la couche des divisions de recensement du Québec DR.Qc &lt;- st_read(dsn = &quot;data/chap01/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DivisionsRecens2021&quot;, quiet = TRUE) ## Importation du fichier csv des divisions de recensement DR.Data &lt;- read.csv(&quot;data/chap01/tables/DRQC2021.csv&quot;) ## Jointure attributaire avec le champ IDUGD DR.Qc &lt;- A compléter ## Il y a déjà deux champs dans la table pour calculer la densité de population : ## SUPTERRE : superficie en km2 ## DRpop_2021 : population en 2021 DR.Qc$HabKm2 &lt;- A compléter head(DR.Qc, n=2) summary(DR.Qc$HabKm2) Correction à la section 10.1.3. Exercice 4. Vous recevez les coordonnées en degrés (WGS84, EPSG : 4326) : -71.91688, 45.37579. Créez un point pour cette localisation et calculez la distance la séparant du tronçon autoroutier le plus proche. Complétez le code ci-dessous avec les étapes suivantes : Requête attributaire pour créer un objet sf avec uniquement les tronçons autoroutiers à partir de la couche Rues et le champ TYPESEGMEN (voir la section 1.2.7.4). Trouvez l’identifiant du tronçon le plus proche avec la fonction st_nearest_feature (voir la section 1.2.6). library(sf) ## Importation du réseau de rues Rues &lt;- st_read(&quot;data/chap01/shp/Segments_de_rue.shp&quot;, quiet=TRUE) unique(Rues$TYPESEGMEN) ## Sélection des tronçons autoroutiers Autoroutes &lt;- À compléter ## Création d&#39;une couche sf pour le point avec les coordonnées ## en degrés (WGS84, EPSG : 4326) : -71.91688, 45.37579 Point1_sf &lt;- À compléter ## Changement de projection avant de s&#39;assurer que les deux couches ont la même Point1_sf &lt;- st_transform(Point1_sf, st_crs(Autoroutes)) ## Trouver le tronçon autoroutier le plus proche avec la fonction st_nearest_feature PlusProche &lt;- À compléter print(PlusProche) Point1_sf$AutoroutePlusProche &lt;- as.numeric(st_distance(Point1_sf, Autoroutes[PlusProche,])) cat(&quot;Distance à l&#39;autoroute la plus proche :&quot;, Point1_sf$AutoroutePlusProche, &quot;m.&quot;) ## Zone tampon ZoneTampon &lt;- st_buffer(Point1_sf, Point1_sf$AutoroutePlusProche) ## Cartographie tmap_mode(&quot;view&quot;) tm_shape(ZoneTampon)+ tm_borders(col= &quot;black&quot;)+ tm_shape(Autoroutes)+ tm_lines(col=&quot;red&quot;)+ tm_shape(Point1_sf)+ tm_dots(col= &quot;blue&quot;, shape=21, size = .2) Correction à la section 10.1.4. "],["chap02.html", "Chapitre 2 Autocorrélation spatiale", " Chapitre 2 Autocorrélation spatiale Première loi de la géographie proposée par Waldo Tobler « Tout interagit avec tout, mais les objets proches ont plus de chance de le faire que les objets éloignés [Everything is related to everything else, but near things are more related than distant things] » (Tobler 1970). Dans ce chapitre, nous mettons en œuvre dans R différentes méthodes qui permettent d’évaluer la dépendance spatiale d’une variable, soit les mesures d’autocorrélation spatiale globale et locale. Préalablement, nous voyons comment définir des matrices de pondération spatiale – selon la contiguïté et la proximité spatiale – qui sont utilisées dans les mesures d’autocorrélation spatiale, mais aussi dans les modèles spatiaux autorégressifs (chapitre 6). Dans ce chapitre, nous utilisons les packages suivants : Pour importer et manipuler des fichiers géographiques : sf pour importer et manipuler des données vectorielles. Pour calculer des mesures d’autocorrélation spatiale : spdep pour construire des matrices spatiales et calculer des mesures d’autocorrélation spatiale. Pour construire des cartes et des graphiques : tmap est certainement le meilleur package. ggplot2 est un package pour construire des graphiques. References "],["sect021.html", "2.1 Notion d’autocorrélation spatiale", " 2.1 Notion d’autocorrélation spatiale Comprendre la configuration spatiale d’un phénomène donné est une démarche fondamentale en analyse spatiale. Or, l’autocorrélation spatiale permet d’estimer la corrélation d’une variable par rapport à sa localisation dans l’espace, soit la dépendance spatiale. Autrement dit, elle permet de vérifier si les entités proches ou voisines ont tendance à être (dis)semblables en fonction d’un phénomène donné (soit une variable). Tel qu’illustré à la figure 2.1 (données fictives), on distingue trois formes d’autocorrélation spatiale : (a) Autocorrélation spatiale positive : lorsque les entités spatiales voisines ou proches se ressemblent davantage que celles non contiguës ou éloignées. Cela renvoie ainsi à la première loi de la géographie : « tout interagit avec tout, mais les objets proches ont plus de chance de le faire que les objets éloignés » (traduction libre) (Tobler 1970). (b) Autocorrélation spatiale négative : lorsque les entités spatiales voisines ou proches ont tendance à être dissemblables, comparativement à celles non contiguës ou éloignées. (c) Absence d’autocorrélation spatiale : lorsque les valeurs de la variable sont distribuées aléatoirement dans l’espace; autrement dit, lorsqu’il n’y a pas de relation entre le voisinage ou la proximité des entités spatiales et leur degré de ressemblance. Figure 2.1: Autocorrélation spatiale Analyse de la figure 2.2 Quelle est la variable pour laquelle le voisinage joue un rôle important dans sa distribution? L’autocorrélation pour cette variable est-elle positive ou négative? Pourquoi? Figure 2.2: Illustration de l’autocorrélation spatiale de deux variables pour les aires de diffusion de la ville de Sherbrooke Réponse : L’autocorrélation spatiale semble bien plus forte pour le pourcentage des logements construits avant 1960. Les aires de diffusion (AD) contiguës ou proches dans la partie centrale de la ville ont clairement des pourcentages élevés (rouge foncé) tandis que celles voisines ou proches dans les périphéries présentent des pourcentages faibles. Cela traduit donc une forte autocorrélation spatiale positive. Par contre, la distribution spatiale du pourcentage de personnes de 65 ans et plus semble plus aléatoire, traduisant ainsi une faible autocorrélation spatiale (dépendance spatiale). Vous avez compris que la simple cartographie d’une variable vous donne une indication de l’autocorrélation spatiale. Pour contre, pour « chiffrer » l’intensité de l’autocorrélation spatiale, il convient de : 1) choisir une matrice de pondération spatiale (selon le voisinage ou la distance) (section 2.2), 2) calculer une mesure d’autocorrélation spatiale à partir de cette matrice (comme l’indice de Moran) (section 2.3). References "],["sect022.html", "2.2 Matrices de pondération spatiale", " 2.2 Matrices de pondération spatiale Les mesures d’autocorrélation spatiale visent à vérifier si les entités spatiales contiguës ou proches ont tendance à être semblables (autocorrélation positive) ou dissemblables (autocorrélation négative) en fonction d’un phénomène donné (en fonction d’une variable). Il convient donc avant tout de définir la manière de mesurer la relation d’adjacence ou de proximité entre deux entités spatiales. Il existe huit principales matrices de pondération spatiale regroupées en deux grandes catégories : celles de contiguïté (basées sur l’adjacence) et celles de proximité (basées sur la distance) (tableau 2.1). Lorsque la couche géographique est composée de points, seules les matrices de proximité peuvent être utilisées. Tableau 2.1: Matrices de pondération spatiale selon la géométrie Matrice Points Lignes Polyg. Raster Matrices de contiguïté (basées sur l’adjacence) Partage d’un nœud (Queen) X X X Partage d’un segment (Rook) X X X Partage d’un nœud et ordre d’adjacence (Queen) X X X Partage d’un segment et ordre d’adjacence (Rook) X X X Matrices de proximité (basées sur la distance) Connectivité selon la distance X X X X Inverse de la distance X X X X Inverse de la distance au carré X X X X Nombre de plus proches voisins X X X X 2.2.1 Matrices de contiguïté La relation d’adjacence (de contiguïté) vise à déterminer si deux entités spatiales sont ou non voisines selon le partage soit d’un nœud, soit d’un segment (frontière commune). La contiguïté est liée à la notion de topologie qui prend en compte les relations de voisinage entre des entités spatiales, sans tenir compte de leurs tailles et de leurs formes géométriques. Elle peut être représentée à partir d’une matrice de contiguïté (avec une valeur de 1 quand deux entités sont voisines et de 0 pour une situation inverse) ou d’un graphe (formé de points représentant les entités spatiales et de lignes reliant les entités voisines) (figure 2.3). Figure 2.3: Relation topologique entre des entités spatiales polygonales Trois évaluations de la contiguïté sont représentées à la figure 2.4 : Adjacence selon le partage d’un segment, soit d’une frontière commune entre les polygones (A). Adjacence selon le partage d’un nœud (B). Ordre d’adjacence selon le partage d’un segment (C). L’ordre d’adjacence indique le nombre de frontières à traverser pour se rendre à l’entité spatiale contiguë, soit : Ordre 1 : une frontière à traverser pour se rendre dans l’entité spatiale adjacente. Ordre 2 : deux frontières à traverser pour atteindre les entités de la deuxième couronne. Ordre 3 : trois frontières à traverser pour atteindre les entités de la troisième couronne. Etc. Bien entendu, les ordres d’adjacence peuvent être également définis selon le partage d’un nœud commun. Figure 2.4: Relations de voisinage et évaluation de la contiguïté Applicabilité des ordres d’adjacence Les matrices d’adjacence sont souvent utilisées dans les analyses de diffusion spatiale. Prenons un exemple concret : imaginons que le polygone en gris à la figure 2.4 est un parc. Nous pourrions évaluer le prix moyen des maisons dans les îlots qui font face au parc (ordre 1), toutes choses étant égales par ailleurs (superficie du terrain, superficie habitable, nombre de pièces, etc.). Puis, nous pourrions comparer ce prix moyen à ceux calculés pour les ordres suivants. Il est probable que le prix au premier ordre soit significativement plus élevé qu’au deuxième ordre, voire au troisième ordre. Autre exemple, nous pourrions réaliser un exercice similaire pour des maisons dans des îlots adjacents à un tronçon autoroutier. La relation est probablement inverse : un prix moyen plus bas à l’ordre 1 comparativement aux ordres suivants. Habituellement appelée \\(W\\), la matrice de contiguïté est binaire selon le partage tant d’un nœud (Queen en anglais) (équation (2.1)) que d’un segment commun (Rook en anglais) (équation (2.2)). \\[\\begin{equation} w_{ij} = \\begin{cases} 1 &amp; \\text{si les entités spatiales }i \\text{ et }j \\text{ ont au moins un nœud commun; } i \\ne j\\\\ 0 &amp; \\text{sinon} \\end{cases} \\tag{2.1} \\end{equation}\\] \\[\\begin{equation} w_{ij} = \\begin{cases} 1 &amp; \\text{si les entités spatiales }i \\text{ et }j \\text{ partagent une frontière commune; } i \\ne j\\\\ 0 &amp; \\text{sinon} \\end{cases} \\tag{2.2} \\end{equation}\\] Exercice 1. Compléter des matrices de contiguïté. Petit conseil pour la partie A : une matrice de contiguïté est symétrique c’est-à-dire que si le polygone A est voisin du polygone B, alors B est voisin de A! Par conséquent, pour gagner du temps, complétez une ligne et transposez-la en colonne. Figure 2.5: Exercice sur la contiguïté et les ordres d’adjacence Correction à la section 10.2.1. 2.2.2 Matrices de proximité spatiale 2.2.2.1 Bref retour sur les différents types de distance Pour calculer des mesures d’autocorrélation spatiale, nous pouvons aussi utiliser des matrices de pondération spatiale basées sur la proximité spatiale. Cette fois, nous ne cherchons pas à vérifier si les entités spatiales adjacentes se ressemblent, mais plutôt à vérifier si les entités spatiales proches les unes des autres se ressemblent. Pour ce faire, nous devons calculer les distances entre les entités spatiales. Pour construire une matrice de pondération spatiale selon la proximité, nous pouvons utiliser plusieurs types de distance (Apparicio et al. 2017) : certaines sont cartésiennes, d’autres, dites réticulaires, sont calculées à partir d’un réseau de rues (figure 2.6). Les distances cartésiennes – euclidienne et de Manhattan (équations (2.3) et (2.4)) – sont facilement calculables à partir des coordonnées géographiques (x,y) dans un SIG ou dans n’importe quel logiciel tableur, de statistique ou de gestion de base de données, etc. Pour cela, la couche géographique doit être dans un système de projection plane. La distance euclidienne représente ainsi la distance à vol d’oiseau entre deux points, tandis que la distance de Manhattan est la somme des deux côtés formant l’angle droit d’un triangle rectangle (l’hypoténuse, le plus grand des côtés du triangle, étant la distance euclidienne) (figure 2.6.a). Si la projection de la couche est sphérique (longitude/latitude), il convient d’utiliser la formule de haversine (basée sur la trigonométrie sphérique) pour obtenir la distance à vol d’oiseau (équation (2.5)). Par contre, comme leurs noms l’indiquent, les distances réticulaires nécessitent un réseau de rues dans un SIG (notamment avec l’extension Network Analyst d’ArcGIS Pro) ou dans R (notamment avec le package R5R) pour calculer le chemin le plus rapide (chapitre 5). \\[\\begin{equation} d_{ij} = \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2} \\tag{2.3} \\end{equation}\\] \\[\\begin{equation} d_{ij} = \\lvert x_i-x_j \\rvert + \\lvert y_i-y_j \\rvert \\tag{2.4} \\end{equation}\\] \\[\\begin{equation} d_{ij} = 2R \\cdot \\text{ arcsin} \\left( \\sqrt{\\text{sin}^2 \\left( \\frac{\\delta _i - \\delta _j}{2} \\right) + \\text{cos }\\delta _i \\cdot \\text{cos }\\delta _j \\cdot \\text{sin}^2 \\left( \\frac{\\phi _i - \\phi _j}{2} \\right)} \\right) \\tag{2.5} \\end{equation}\\] avec \\(R\\) étant le rayon de la terre; \\(\\delta _i\\) et \\(\\delta _j\\) les coordonnées longitude pour les points \\(i\\) et \\(j\\); \\(\\phi _i\\) et \\(\\phi _j\\) les coordonnées latitude pour les points \\(i\\) et \\(j\\). Figure 2.6: Les différents types de distance 2.2.2.2 Matrice de distance binaire (de connectivité) À partir d’une matrice de distance entre les entités spatiales d’une couche géographique, il est possible de créer une matrice de pondération binaire (équation (2.6)). Ce type de matrice est habituellement appelée matrice de connectivité. Il convient alors de fixer un seuil de distance maximal. Par exemple, avec un seuil de 500 mètres, \\(w_{ij}=1\\) si la distance entre les entités spatiales \\(i\\) et \\(j\\) est inférieure ou égale à 500 mètres; sinon \\(w_{ij}=0\\). Notez que pour des lignes et des polygones, la distance est habituellement calculée à partir de leurs centroïdes. \\[\\begin{equation} w_{ij} = \\begin{cases} 1 &amp; \\text{si }d_{ij}\\leq{\\bar{d}}\\text{; } i \\ne j\\\\ 0 &amp; \\text{sinon} \\end{cases} \\tag{2.6} \\end{equation}\\] avec \\(d_{ij}\\) étant la distance entre les entités spatiales \\(i\\) et \\(j\\), et \\(\\bar{d}\\) étant un seuil de distance maximal fixé par la personne utilisatrice (par exemple, 500 mètres). En guise d’exemple, à la figure 2.7, seuls les polygones jaunes seraient considérés comme voisins du polygone bleu avec un seuil de distance maximal fixé à 2,5 kilomètres (valeur de 1); les roses se verraient affecter la valeur de 0. Figure 2.7: Illustration de la connectivité basée sur la distance 2.2.2.3 Matrices basées sur la distance À partir d’une matrice de distance entre les entités spatiales, les pondérations peuvent être calculées avec l’inverse de la distance (\\(1/d_{ij}\\)) ou l’inverse de la distance au carré (\\(1/d_{ij^2}\\)) (équation (2.7)). Analysons le graphique à la figure 2.8. Premièrement, nous constatons que plus la distance est grande, plus la valeur de la pondération est faible et inversement. De la sorte, nous accordons un rôle plus important aux entités spatiales proches les unes des autres que celles éloignées. Deuxièmement, les pondérations chutent beaucoup plus rapidement avec l’inverse de la distance au carré qu’avec l’inverse de la distance. Autrement dit, le recours à une matrice de pondération calculée avec l’inverse de la distance au carré a comme effet d’accorder un poids plus important aux entités géographiques très proches. \\[\\begin{equation} w_{ij} = \\begin{cases} \\frac{1}{d_{ij}^{\\gamma}} &amp;\\\\ 0 &amp; \\text{si } i=j \\end{cases} \\tag{2.7} \\end{equation}\\] avec \\(\\gamma = 1\\) pour une matrice de l’inverse de la distance et \\(\\gamma = 2\\) pour l’inverse de la distance au carré. Figure 2.8: Comparaison des matrices inverse de la distance et inverse de la distance au carré Pondération avec l’exponentielle inverse Dans un excellent livre intitulé Économétrie spatiale appliquée des microdonnées, Jean Dubé et Diego Legros (2014) proposent de transformer la matrice des distances avec l’inverse de l’exponentielle (ou l’exponentielle négative de la distance) (équation (2.8)). Comparativement à l’inverse de la distance au carré, cette opération fait chuter encore plus rapidement les pondérations. \\[\\begin{equation} w_{ij} = \\begin{cases} \\frac{1}{e^{d_{ij}}} = e^{-d_{ij}} &amp;\\\\ 0 &amp; \\text{si } i=j \\end{cases} \\tag{2.8} \\end{equation}\\] Notez que l’équation (2.7) peut être légèrement modifiée en introduisant un seuil maximal de la distance au-delà duquel les pondérations sont mises à 0 (équation (2.9)). Autrement dit, cela permet de ne pas tenir compte des entités spatiales distantes à plus d’un seuil fixé par l’analyste, ce qui est particulièrement intéressant lorsque vous analysez un phénomène dont la diffusion (ou propagation) cesse ou est très minime au-delà d’une certaine distance. Par exemple, pour la propagation du bruit routier, le seuil de 300 mètres est souvent utilisé. Par conséquent, une mesure d’autocorrélation spatiale sur des mesures de bruit routier devrait probablement recourir à un seuil maximal de 300 mètres. Autre exemple, la superficie du territoire vital diffère selon les espèces animales (cerf, caribou, ours et loup, par exemple). Par conséquent, une ou un biologiste calculant des mesures d’autocorrélation spatiale risque aussi de fixer un seuil maximal différent selon l’espèce étudiée. \\[\\begin{equation} w_{ij} = \\begin{cases} \\frac{1}{d_{ij}^{\\gamma}} &amp; \\text{si }d_{ij}\\leq{\\bar{d}}\\\\ 0 &amp; \\text{si }d_{ij}&gt;{\\bar{d}}\\\\ 0 &amp; \\text{si } i=j \\end{cases} \\tag{2.9} \\end{equation}\\] Calculer le rayon maximal à partir d’une aire Admettons que la superficie du territoire vital d’une espèce soit de 50 hectares, soit 0,5 km2 ou 500 000 m2. La formule bien connue pour calculer la superficie d’un cercle est \\(S = \\pi r^2\\) avec \\(S\\) et \\(r\\) étant respectivement la superficie et le rayon. Par conséquent, celle du rayon est \\(r = \\sqrt{\\frac{S}{\\pi}}\\). Pour trouver le rayon, vous devez taper sqrt(500000 / pi) dans la console de R et obtenir ainsi une distance de 398.9423 qui pourrait être arrondie à 400 mètres. 2.2.2.4 Matrices selon le critère des plus proches voisins Une autre façon très utilisée pour définir une matrice de proximité à partir d’une matrice de distance consiste à retenir uniquement les n plus proches voisins. La matrice est aussi binaire avec les valeurs de 1 si les observations sont parmi les n plus proches de l’entité spatiale \\(i\\) et de 0 pour une situation inverse. 2.2.3 Standardisation des matrices de pondération spatiale en ligne Il est recommandé de standardiser les matrices de pondération en ligne. La somme de la matrice de pondération sera alors égale au nombre d’entités spatiales de la couche géographique. Quel est l’intérêt de la standardisation? Nous verrons dans les sections suivantes que ces matrices sont utilisées pour évaluer le degré d’autocorrélation spatiale globale et locale. Or, il est fréquent de comparer les valeurs des mesures d’autocorrélation spatiale obtenues avec différentes matrices d’adjacence et de proximité (contiguïté selon le partage d’un nœud, d’une frontière commune; inverse de la distance, inverse de la distance au carré, etc.). Autrement dit, la standardisation des matrices de pondération spatiale permet de vérifier si le degré de (dis)ressemblance des entités spatiales en fonction d’une variable donnée est plus fort avec une matrice de contiguïté, d’inverse de la distance ou encore d’inverse de la distance au carré, etc. Pour illustrer comment réaliser une standardisation, nous utilisons une couche géographique comprenant peu d’entités spatiales, soit celle des quatre arrondissements de la ville de Sherbrooke (figure 2.9). Figure 2.9: Arrondissements de la ville de Sherbrooke Au tableau 2.2, différentes matrices de contiguïté et de distance ont été calculées, puis standardisées. Voici comment interpréter les différentes sections du tableau : Contiguïté selon le partage d’une frontière commune. La valeur de 1 signale que deux arrondissements sont voisins, sinon la valeur est à 0. Tel qu’indiqué aux équations (2.1) et (2.2), un arrondissement ne peut être voisin de lui-même (ex.: valeur de 0 pour la cellule Bro. et Bro.). L’arrondissement de Brompton–Rock Forest–Saint-Élie–Deauville (Bro.) a deux voisins, soit ceux des Nations et de Fleurimont (Nat. et Fle.), comme indiqué par la valeur 2 dans la colonne total. Par contre, les arrondissements des Nations et de Fleurimont sont voisins de tous les autres (valeur de 3 dans la colonne total). Standardisation de la matrice de contiguïté. Il suffit de diviser chaque valeur de la matrice de contiguïté par la somme de la ligne correspondante. De la sorte, la somme de chaque ligne est égale à 1 et la somme de l’ensemble des valeurs de la matrice est égale au nombre d’entités spatiales (ici 4). Distance (km). Nous avons calculé la distance euclidienne en kilomètres entre les centroïdes des arrondissements. Inverse de la distance. Les valeurs sont obtenues avec la formule \\(1/_{dij}\\). Par exemple, entre Bro. et Nat., nous avons \\(1/7,9930 = 0,1251\\). Inverse de la distance au carré. Les valeurs sont obtenues avec la formule \\(1/_{dij^2}\\). Par exemple, entre Bro. et Nat., nous avons \\(1/7,9930^2 = 0,0160\\). Standardisation de l’inverse de la distance. Comme précédemment, il suffit de diviser chaque valeur de la matrice par la somme de la ligne correspondante. Par exemple, pour Bro. et Nat., nous avons \\(0,1251 / 0,3241 = 0,3860\\). Remarquez que la somme des lignes est bien égale à 1. Standardisation de l’inverse de la distance au carré. Comme précédemment, il suffit de diviser chaque valeur de la matrice par la somme de la ligne correspondante. Par exemple, pour Bro. et Nat., nous avons \\(0,0160 / 0,0360 = 0,4440\\). Remarquez que la somme des lignes est bien égale à 1. Tableau 2.2: Standardisation de matrices de pondération spatiale Arrondissement Bro. Nat. Len. Fle. Somme (lignes) Matrice de contiguïté selon le partage d’une frontière commune Bro. 0,0000 1,0000 0,0000 1,0000 2,0000 Nat. 1,0000 0,0000 1,0000 1,0000 3,0000 Len. 0,0000 1,0000 0,0000 1,0000 2,0000 Fle. 1,0000 1,0000 1,0000 0,0000 3,0000 Standardisation de la matrice de contiguïté Bro. 0,0000 0,5000 0,0000 0,5000 1,0000 Nat. 0,3330 0,0000 0,3330 0,3330 1,0000 Len. 0,0000 0,5000 0,0000 0,5000 1,0000 Fle. 0,3330 0,3330 0,3330 0,0000 1,0000 Distance (km) Bro. 0,0000 7,9930 18,9940 16,1140 Nat. 7,9930 0,0000 11,1190 9,1650 Len. 18,9940 11,1190 0,0000 9,2590 Fle. 16,1140 9,1650 9,2590 0,0000 Matrice selon l’inverse de la distance Bro. 0,0000 0,1251 0,0526 0,0621 0,2398 Nat. 0,1251 0,0000 0,0899 0,1091 0,3241 Len. 0,0526 0,0899 0,0000 0,1080 0,2505 Fle. 0,0621 0,1091 0,1080 0,0000 0,2792 Matrice selon l’inverse de la distance au carré Bro. 0,0000 0,0160 0,0030 0,0040 0,0230 Nat. 0,0160 0,0000 0,0080 0,0120 0,0360 Len. 0,0030 0,0080 0,0000 0,0120 0,0230 Fle. 0,0040 0,0120 0,0120 0,0000 0,0280 Standardisation de l’inverse de la distance Bro. 0,0000 0,5220 0,2190 0,2590 1,0000 Nat. 0,3860 0,0000 0,2770 0,3370 1,0000 Len. 0,2100 0,3590 0,0000 0,4310 1,0000 Fle. 0,2220 0,3910 0,3870 0,0000 1,0000 Standardisation de l’inverse de la distance au carré Bro. 0,0000 0,6960 0,1300 0,1740 1,0000 Nat. 0,4440 0,0000 0,2220 0,3330 1,0000 Len. 0,1300 0,3480 0,0000 0,5220 1,0000 Fle. 0,1430 0,4290 0,4290 0,0000 1,0000 2.2.4 Construction de matrices de pondération spatiale dans R Construction des matrices dans R  avec le package spdep. Le package spdep dispose de différentes fonctions pour construire des matrices de contiguïté, de connectivité et de distance : poly2nb pour des matrices de contiguïté (section 2.2.4.1); nblag et nblag_cumul pour des matrices de contiguïté avec des ordres d’adjacence (section 2.2.4.2); dnearneigh pour des matrices de connectivité (section 2.2.4.3); as.matrix(dist(coords)) et mat2listw pour des matrices de distance (section 2.2.4.4); knn2nb pour des matrices selon le critère des plus proches voisins (section 2.2.4.5). 2.2.4.1 Matrices de pondération spatiale selon la contiguïté Pour créer des matrices de pondération spatiale selon la contiguïté décrites à la section 2.2.1, nous utilisons deux fonctions du package spdep : poly2nb(Nom de l'objet sf, queen=TRUE) crée une matrice de contiguïté sous la forme d’une classe nb (A neighbours list with class nb). Avec le paramètre queen=TRUE, la contiguïté est évaluée selon le partage d’un nœud; avec queen=FALSE, la contiguïté est évaluée selon le partage d’un segment (frontière). La matrice spatiale comprend une ligne par secteur de recensement avec les index des polygones adjacents. Par exemple, Queen[[1]] renvoie la liste des polygones voisins à la première entité spatiale, soit 2 14 15 16 23 32, c’est-à-dire six voisins. nb2listw(objet nb, zero.policy=TRUE, style = \"W\") crée une matrice de pondération spatiale à partir de n’importe quelle matrice spatiale (de contiguïté ou de distance). Le paramètre style = \"W\", qui est par défaut, permet de standardiser la matrice en ligne. Par exemple, W.Queen$weights[[1]] renvoie les valeurs des pondérations pour la première entité spatiale, soit 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 (0,1666667 = 1 / 6 voisins). Pour obtenir une matrice non standardisée, vous devez écrire style = \"B\", alors W.Queen$weights[[1]] renverra les valeurs de 1 1 1 1 1 1. library(sf) # pour importer des couches géographiques library(spdep) # pour construire les matrices de pondération ## Importation de la couche des secteurs de recensement de la ville de Sherbrooke SR &lt;- st_read(dsn = &quot;data/chap02/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DR_SherbSRDonnees2021&quot;, quiet=TRUE) ## Matrice selon le partage d&#39;un nœud (Queen) # Création de la matrice spatiale Queen &lt;- poly2nb(SR, queen=TRUE) # Affichage de la première ligne de la matrice Queen[[1]] ## [1] 2 14 15 16 23 32 # Création de la matrice de pondération avec une standardisation en ligne W.Queen &lt;- nb2listw(Queen, zero.policy=TRUE, style = &quot;W&quot;) # Affichage de la première ligne des pondérations W.Queen$weights[[1]] ## [1] 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 0.1666667 cat(&quot;La somme de la première ligne de la matrice de pondération est égale à&quot;, sum(W.Queen$weights[[1]])) ## La somme de la première ligne de la matrice de pondération est égale à 1 ## Matrice selon le partage d&#39;un segment (Rook) Rook &lt;- poly2nb(SR, queen=FALSE) W.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = &quot;W&quot;) ## Comparaison des deux matrices de contiguïté # Résultat de la matrice de pondération (Queen) summary(W.Queen) ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 50 ## Number of nonzero links: 272 ## Percentage nonzero weights: 10.88 ## Average number of links: 5.44 ## Link number distribution: ## ## 1 2 3 4 5 6 7 8 10 ## 1 2 2 9 13 9 8 5 1 ## 1 least connected region: ## 41 with 1 link ## 1 most connected region: ## 29 with 10 links ## ## Weights style: W ## Weights constants summary: ## n nn S0 S1 S2 ## W 50 2500 50 20.3056 205.5251 # Résultat de la matrice de pondération (Rook) summary(W.Rook) ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 50 ## Number of nonzero links: 248 ## Percentage nonzero weights: 9.92 ## Average number of links: 4.96 ## Link number distribution: ## ## 1 2 3 4 5 6 7 9 ## 1 2 4 9 17 11 5 1 ## 1 least connected region: ## 41 with 1 link ## 1 most connected region: ## 29 with 9 links ## ## Weights style: W ## Weights constants summary: ## n nn S0 S1 S2 ## W 50 2500 50 21.84674 205.0781 La syntaxe ci-dessous permet de visualiser et de comparer les graphes selon le partage d’un nœud (Queen) ou d’un segment commun (Rook). par(mfrow=c(1,2)) # permet d&#39;avoir quatre graphiques (2x2) coords &lt;- st_coordinates(st_centroid(SR)) ## Graphe selon le partage d&#39;un nœud plot(st_geometry(SR), border=&quot;gray&quot;, lwd=2, col=&quot;wheat&quot;) plot(Queen, coords, add=TRUE, col=&quot;red&quot;, lwd=2) title(main=&quot;Queen&quot;, font.main= 1) ## Graphe selon le partage d&#39;une frontière commune plot(st_geometry(SR), border=&quot;gray&quot;, lwd=2, col=&quot;wheat&quot;) plot(Rook, coords, add=TRUE, col=&quot;red&quot;, lwd=2) title(main=&quot;Rook&quot;, font.main= 1) 2.2.4.2 Matrices de pondération spatiale selon la contiguïté et un ordre d’adjacence Pour décrire la construction des matrices de contiguïté avec un ordre d’adjacence (décrites à la section 2.2.1), nous utilisons une couche géographique comprenant peu d’entités spatiales, soit celle des quatre arrondissements de la ville de Sherbrooke (figure 1). Le code ci-dessous permet d’obtenir les résultats suivants : Rook &lt;- poly2nb(Arrondissements, queen=TRUE): matrice d’ordre 1 selon le partage d’un segment. str(Rook): pour chaque arrondissement, la liste des arrondissements adjacents d’ordre 1. Rook.Ordre2 &lt;- nblag(Rook, 2): création d’une matrice d’ordre 2 avec la fonction nblag. str(Rook.Ordre2[[1]]): liste des voisins d’ordre 1. Bien entendu, le résultat est identique à str(Rook). str(Rook.Ordre2[[2]]): liste des voisins d’ordre 2. Rook.Ordre2Cumule &lt;- nblag_cumul(Rook.Ordre2): fusion des deux listes en une seule avec la fonction nblag_cumul. Arrondissements &lt;- st_read(&quot;data/chap02/Arrondissements.shp&quot;, quiet=TRUE) ## Matrice de contiguïté d&#39;ordre 1 selon le partage d&#39;un segment (Rook) Rook &lt;- poly2nb(Arrondissements, queen=TRUE) str(Rook) ## List of 4 ## $ : int [1:2] 2 4 ## $ : int [1:3] 1 3 4 ## $ : int [1:2] 2 4 ## $ : int [1:3] 1 2 3 ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## - attr(*, &quot;call&quot;)= language poly2nb(pl = Arrondissements, queen = TRUE) ## - attr(*, &quot;type&quot;)= chr &quot;queen&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE ## Matrice de contiguïté d&#39;ordre 2 selon le partage d&#39;un segment (Rook) Rook.Ordre2 &lt;- nblag(Rook, 2) ## Rook.Ordre2 comprend deux listes : l&#39;une pour l&#39;ordre 1 et l&#39;autre pour l&#39;autre 2. str(Rook.Ordre2[[1]]) ## List of 4 ## $ : int [1:2] 2 4 ## $ : int [1:3] 1 3 4 ## $ : int [1:2] 2 4 ## $ : int [1:3] 1 2 3 ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## - attr(*, &quot;call&quot;)= language poly2nb(pl = Arrondissements, queen = TRUE) ## - attr(*, &quot;type&quot;)= chr &quot;queen&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE str(Rook.Ordre2[[2]]) ## List of 4 ## $ : int 3 ## $ : int 0 ## $ : int 1 ## $ : int 0 ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE ## La fonction nblag_cumul permet de combiner les deux ordres dans une seule liste Rook.Ordre2Cumule &lt;- nblag_cumul(Rook.Ordre2) str(Rook.Ordre2Cumule) ## List of 4 ## $ : int [1:3] 2 3 4 ## $ : int [1:3] 1 3 4 ## $ : int [1:3] 1 2 4 ## $ : int [1:3] 1 2 3 ## - attr(*, &quot;region.id&quot;)= chr [1:4] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ## - attr(*, &quot;call&quot;)= language nblag_cumul(nblags = Rook.Ordre2) ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## Création de la matrice de pondération spatiale standardisée WRook.Ordre2Cumule &lt;- nb2listw(Rook.Ordre2Cumule, zero.policy=TRUE, style = &quot;W&quot;) La figure 2.10 permet de constater qu’au second ordre, chacun des arrondissements est relié aux trois autres. Figure 2.10: Adjacence de premier et de second ordre Reprenons la couche des secteurs de recensement de la ville de Sherbrooke pour construire des matrices d’adjacence d’ordre 1 à 3. # Création des matrices d&#39;ordre 1, 2 et 3 Queen1 &lt;- poly2nb(SR, queen=TRUE) Queen2 &lt;- nblag_cumul(nblag(Queen1, 2)) Queen3 &lt;- nblag_cumul(nblag(Queen1, 3)) # Création des matrices W.Queen1 &lt;- nb2listw(Queen, zero.policy=TRUE, style = &quot;W&quot;) W.Queen2 &lt;- nb2listw(Queen2, zero.policy=TRUE, style = &quot;W&quot;) W.Queen3 &lt;- nb2listw(Queen3, zero.policy=TRUE, style = &quot;W&quot;) 2.2.4.3 Matrice de connectivité (matrice distance binaire) La fonction dnearneigh(sf points, d1=, d2=) crée une matrice de connectivité (décrite à la section 2.2.2) à partir d’une couche de points. Les paramètres d1 et d2 permettent de spécifier le rayon de recherche (ex. : avec d1 = 0 et d2 = 2500, le seuil maximal de distance est de 2500 mètres). Si votre couche sf comprend des lignes ou des polygones, utilisez la fonction st_centroid ou st_point_on_surface() pour les convertir en points (section 1.2.2). ## Conversion des polygones en points avec st_centroid SR.centroides &lt;- st_centroid(SR) ## Matrice binaire avec un seuil de 2500 mètres Connect2500m &lt;- dnearneigh(SR.centroides, d1 = 0, d2 = 2500) ## Matrice de pondération spatiale standardisée en ligne W.Connect2500m &lt;- nb2listw(Connect2500m, zero.policy=TRUE, style = &quot;W&quot;) 2.2.4.4 Matrices de pondération spatiale selon l’inverse de la distance et l’inverse de la distance au carré Dans la section 2.2.3, nous avons présenté les matrices de l’inverse de la distance et de l’inverse de la distance au carré. Le code ci-dessous, qui permet de les créer, comprend les étapes suivantes : Récupération des coordonnées géographiques des entités spatiales. Création de la matrice de distance euclidienne \\(n \\times n\\) (\\(n\\) étant le nombre d’entités spatiales de la couche). Calcul des matrices d’inverse de la distance et d’inverse de la distance au carré. Standardisation de ces deux matrices et transformation dans des objets listw avec la fonction mat2listw. ## Coordonnées des centroïdes des entités spatiales coords &lt;- st_coordinates(SR.centroides) ## Création de la matrice de distance distances &lt;- as.matrix(dist(coords, method = &quot;euclidean&quot;)) # S&#39;assurer que la diagonale de la matrice est à 0 diag(distances) &lt;- 0 ## Matrices inverse de la distance et inverse de la distance au carré InvDistances &lt;- ifelse(distances!=0, 1/distances, distances) InvDistances2 &lt;- ifelse(distances!=0, 1/distances^2, distances) ## Matrices de pondération spatiale standardisées en ligne W_InvDistances &lt;- mat2listw(InvDistances, style=&quot;W&quot;) W_InvDistances2 &lt;- mat2listw(InvDistances2, style=&quot;W&quot;) ## Visualisation des valeurs des pondération pour la première entité spatiale round(W_InvDistances$weights[[1]],4) ## [1] 0.0688 0.0505 0.0377 0.0330 0.0220 0.0191 0.0152 0.0116 0.0155 0.0220 ## [11] 0.0303 0.0382 0.0582 0.0677 0.0661 0.0366 0.0373 0.0316 0.0248 0.0123 ## [21] 0.0178 0.0241 0.0055 0.0084 0.0083 0.0084 0.0106 0.0232 0.0125 0.0231 ## [31] 0.0425 0.0192 0.0164 0.0049 0.0086 0.0071 0.0062 0.0061 0.0054 0.0049 ## [41] 0.0078 0.0050 0.0032 0.0043 0.0036 0.0030 0.0035 0.0042 0.0037 # La somme de la ligne est bien égale à 1 sum(W_InvDistances$weights[[1]]) ## [1] 1 Intégration d’autres types de distance À la section 2.2.2.1, nous avons vu que plusieurs types de distances peuvent être utilisés : cartésiennes (euclidienne et de Manhattan) et réticulaires (chemin le plus rapide à pied, à vélo, en automobile et en transport en commun). Pour construire une matrice de distance de Manhattan, vous devez changer la valeur du paramètre method de la fonction dist comme suit : as.matrix(dist(coords, method = \"manhattan\")). Pour intégrer une distance réticulaire, vous devez la calculer, soit dans R (chapitre 6), soit dans un logiciel SIG (ArcGIS Pro avec l’extension Network Analyst par exemple) et l’importer dans R. Le reste du code sera alors identique. Nous avons vu qu’il est possible d’utiliser une matrice de distance en fixant une distance maximale au-delà de laquelle les pondérations sont mises à 0 (équation (2.9) à la section 2.2.2.1). Le code ci-dessous permet de créer des matrices de pondération standardisées avec l’inverse de la distance et l’inverse de la distance au carré avec des seuils de 2500 et de 5000 mètres. ## Coordonnées des centroïdes des entités spatiales coords &lt;- st_coordinates(SR.centroides) ## Création de la matrice de distance distances &lt;- as.matrix(dist(coords, method = &quot;euclidean&quot;)) ## Création de différentes matrices avec différents seuils InvDistances.2500m &lt;- ifelse(distances&lt;=2500 &amp; distances!=0, 1/distances, 0) InvDistances.5000m &lt;- ifelse(distances&lt;=5000 &amp; distances!=0, 1/distances, 0) InvDistances2.2500m &lt;- ifelse(distances&lt;=2500 &amp; distances!=0, 1/distances^2, 0) InvDistances2.5000m &lt;- ifelse(distances&lt;=5000 &amp; distances!=0, 1/distances^2, 0) ## Matrices de pondération spatiale standardisées en ligne W_InvDistances.2500 &lt;- mat2listw(InvDistances.2500m, style=&quot;W&quot;) W_InvDistances.5000 &lt;- mat2listw(InvDistances.5000m, style=&quot;W&quot;) W_InvDistances2.2500 &lt;- mat2listw(InvDistances2.2500m, style=&quot;W&quot;) W_InvDistances2.5000 &lt;- mat2listw(InvDistances2.5000m, style=&quot;W&quot;) Spécifier un seuil de distance peut toutefois être problématique. Par exemple, sur les 50 secteurs de recensement de la ville de Sherbrooke, 19 n’ont pas de voisins à 2500 mètres, indiqués par le résultat suivant : ## 19 regions with no links: ## 23 24 25 30 34 35 36 37 38 39 40 41 42 43 44 45 47 49 50 summary(W_InvDistances.2500, zero.policy=TRUE) ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 50 ## Number of nonzero links: 188 ## Percentage nonzero weights: 7.52 ## Average number of links: 3.76 ## 19 regions with no links: ## 23 24 25 30 34 35 36 37 38 39 40 41 42 43 44 45 47 49 50 ## Link number distribution: ## ## 0 1 2 3 4 6 7 8 9 10 11 12 13 ## 19 6 2 2 4 2 2 3 4 1 2 1 2 ## 6 least connected regions: ## 21 26 31 33 46 48 with 1 link ## 2 most connected regions: ## 12 13 with 13 links ## ## Weights style: W ## Weights constants summary: ## n nn S0 S1 S2 ## W 31 961 31 19.09079 128.6954 Même avec un seuil de 5000 mètres, il reste encore 11 SR sans voisins. summary(W_InvDistances.5000, zero.policy=TRUE) ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 50 ## Number of nonzero links: 532 ## Percentage nonzero weights: 21.28 ## Average number of links: 10.64 ## 11 regions with no links: ## 35 36 37 39 40 41 42 43 47 49 50 ## Link number distribution: ## ## 0 1 2 3 5 6 7 8 9 10 11 16 18 19 20 21 22 23 ## 11 2 2 4 2 1 1 2 1 1 1 2 2 4 2 4 7 1 ## 2 least connected regions: ## 24 30 with 1 link ## 1 most connected region: ## 12 with 23 links ## ## Weights style: W ## Weights constants summary: ## n nn S0 S1 S2 ## W 39 1521 39 12.11283 162.9801 Réduction de la taille des matrices de distance Plusieurs logiciels (notamment ArcGIS Pro et GeoDa) réduisent par défaut la taille des matrices de distance de la façon suivante : 1) construction d’une matrice de distance uniquement pour l’entité la plus proche (la matrice résultante est donc de dimension \\(n \\times 1\\)); 2) obtention de la distance maximale dans cette matrice, soit la distance la plus grande entre une entité spatiale et celle la plus proche; 3) construction de la matrice de distance finale avec comme seuil la distance maximale obtenue à l’étape précédente. Cette réduction procure deux avantages importants : Une diminution considérable des temps de calcul, surtout pour les couches géographiques comprenant un nombre très élevé d’entités spatiales. Par exemple, avec une couche de 50 entités spatiales, la matrice des distances comprendra 2500 valeurs (50 \\(\\times\\) 50 = 2500) tandis qu’avec 1000 entités spatiales, elle en comprendra un million (1000 \\(\\times\\) 1000 = 1000000). Comme décrit plus haut, il est préférable d’éviter d’avoir une matrice de distance avec des entités spatiales sans voisins, puisque cela a un impact négatif sur les mesures d’autocorrélation spatiale. La syntaxe ci-dessous permet ainsi de construire des matrices de pondération (inverse de la distance et inverse de la distance au carré) à partir de la distance maximale et un SR et son voisin le plus proche. ## Coordonnées des centroïdes des entités spatiales coords &lt;- st_coordinates(SR.centroides) ## Trouver le plus proche voisin k1 &lt;- knn2nb(knearneigh(coords)) ## Affichage des distances pour les 50 SR au le plus proche round(unlist(nbdists(k1,coords)),0) ## [1] 1352 563 563 659 833 1275 1275 1299 2136 1553 1171 833 ## [13] 953 953 1024 1024 936 936 1149 1242 2378 1473 3863 4963 ## [25] 2841 1755 1755 2294 1507 4002 1843 1710 2486 2977 10953 6965 ## [37] 5331 4077 6717 6892 6892 6335 5639 3530 4202 1636 6662 1636 ## [49] 10745 10034 ## Trouver la distance maximale plusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords))) cat(&quot;Distance maximale au plus proche voisin :&quot;, round(plusprochevoisin.max,0), &quot;mètres&quot;) ## Distance maximale au plus proche voisin : 10953 mètres ## Matrice de distance avec la valeur maximale # les voisins les plus proches avec le seuil de distance maximal Voisins.DistMax &lt;- dnearneigh(coords, 0, plusprochevoisin.max) # Distances avec le seuil maximum distances &lt;- nbdists(Voisins.DistMax, coords) # Inverse de la distance InvDistances &lt;- lapply(distances, function(x) (1/x)) # Inverse de la distance au carré InvDistances2 &lt;- lapply(distances, function(x) (1/x^2)) ## Matrices de pondération spatiale standardisées en ligne W_InvDistances &lt;- nb2listw(Voisins.DistMax, glist = InvDistances, style = &quot;W&quot;) W_InvDistances2 &lt;- nb2listw(Voisins.DistMax, glist = InvDistances2, style = &quot;W&quot;) 2.2.4.5 Matrices de pondération spatiale selon le critère des plus proches voisins La fonction knearneigh du package spdep crée des matrices de distance selon le critère des plus proches voisins (décrit à la section 2.2.2.4), dont le nombre est fixé avec le paramètre k. ## Coordonnées géographiques des centroïdes des polygones coords &lt;- st_coordinates(st_centroid(SR)) ## Matrices des plus proches voisins de 2 à 5 k2 &lt;- knn2nb(knearneigh(coords, k = 2)) k3 &lt;- knn2nb(knearneigh(coords, k = 3)) k4 &lt;- knn2nb(knearneigh(coords, k = 4)) k5 &lt;- knn2nb(knearneigh(coords, k = 5)) ## Matrices de pondération spatiale standardisées en ligne W.k2 &lt;- nb2listw(k2, zero.policy=FALSE, style = &quot;W&quot;) W.k3 &lt;- nb2listw(k3, zero.policy=FALSE, style = &quot;W&quot;) W.k4 &lt;- nb2listw(k4, zero.policy=FALSE, style = &quot;W&quot;) W.k5 &lt;- nb2listw(k5, zero.policy=FALSE, style = &quot;W&quot;) La syntaxe ci-dessous permet de comparer les matrices des plus proches voisins de k = 2 à 5 (figure 2.11). par(mfrow=c(2,2)) plot(st_geometry(SR), border=&quot;gray&quot;, lwd=2, col=&quot;wheat&quot;) plot(k2, coords, add=TRUE, col=&quot;red&quot;, lwd=2) title(main=&quot;k = 2&quot;) plot(st_geometry(SR), border=&quot;gray&quot;, lwd=2, col=&quot;wheat&quot;) plot(k3, coords, add=TRUE, col=&quot;red&quot;, lwd=2) title(main=&quot;k = 3&quot;) plot(st_geometry(SR), border=&quot;gray&quot;, lwd=2, col=&quot;wheat&quot;) plot(k4, coords, add=TRUE, col=&quot;red&quot;, lwd=2) title(main=&quot;k = 4&quot;) plot(st_geometry(SR), border=&quot;gray&quot;, lwd=2, col=&quot;wheat&quot;) plot(k5, coords, add=TRUE, col=&quot;red&quot;, lwd=2) title(main=&quot;k = 5&quot;) Figure 2.11: Matrices selon le critère des plus proches voisins References "],["sect023.html", "2.3 Autocorrélation spatiale globale", " 2.3 Autocorrélation spatiale globale 2.3.1 Statistique du I de Moran 2.3.1.1 Formulation du I de Moran Pour évaluer le degré d’autocorrélation spatiale d’une variable continue, les deux principales statistiques utilisées sont le I de Moran (1950) et le c de Geary (1954). Puisqu’elles renvoient une seule valeur pour la variable continue de la couche géographique étudiée, elles sont qualifiées de mesures globales de l’autocorrélation spatiale, par opposition aux mesures locales qui renvoient une valeur par entité spatiale (section 2.4). Nous présentons ici uniquement le I de Moran pour deux raisons principales. Premièrement, étant basée sur la covariance, son interprétation est bien plus facile que celle du c de Geary (basé sur la variance des écarts), c’est-à-dire qu’elle est très similaire au bien connu coefficient de corrélation (Apparicio et Gelb 2022). Deuxièmement, elle constitue la mesure la plus utilisée. Le I de Moran s’écrit : \\[\\begin{equation} I = \\frac{n}{\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}} \\frac{\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\Sigma_{i=1}^n (x_i-\\bar{x})^2} \\text{ avec :} \\tag{2.10} \\end{equation}\\] n, le nombre d’entités spatiales dans la couche géographique; \\(w_{ij}\\), la valeur de la pondération spatiale entre les entités spatiales \\(i\\) et \\(j\\); \\(x_i\\) et \\(x_j\\), les valeurs de la variable continue pour les entités spatiales \\(i\\) et \\(j\\); \\(\\bar{x}\\), la valeur moyenne de la variable \\(X\\) à l’étude. Standardisation de la matrice de pondération et I de Moran Nous avons vu que si la matrice de pondération spatiale est standardisée en ligne (section 2.2.3), alors chaque ligne de la matrice vaut 1 et la somme de l’ensemble des valeurs de la matrice est égale au nombre d’entités spatiales (\\(n\\)). Or, dans l’équation (2.10), \\(\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}\\) représente la somme des pondérations de la matrice, soit \\(n\\) si elles sont standardisées en ligne. Puisque \\(\\frac{n}{n}=1\\), alors l’équation du I de Moran est simplifié comme suit : \\[\\begin{equation} I = \\frac{\\Sigma_{i=1}^n \\Sigma_{j=1}^n w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\Sigma_{i=1}^n (x_i-\\bar{x})^2} \\tag{2.11} \\end{equation}\\] Comme évoqué dans la section 2.2.3, cela démontre l’intérêt de la standardisation : la comparaison des valeurs du I de Moran obtenues avec différentes matrices de contiguïté afin de sélectionner (éventuellement) celle avec laquelle la dépendance spatiale est la plus forte. 2.3.1.2 Interprétation du I de Moran Avec une matrice standardisée, la statistique du I de Moran varie de -1 à 1 et s’interprète de la façon suivante : quand \\(I&gt;0\\), l’autocorrélation est positive, c’est-à-dire que les entités géographiques ont tendance à se ressembler d’autant plus qu’elles sont voisines ou proches; quand \\(I=0\\), l’autocorrélation est nulle et la contiguïté ou la proximité spatiale des zones ne joue donc aucun rôle; quand \\(I&lt;0\\), l’autocorrélation est négative, c’est-à-dire que les entités géographiques ont tendance à être dissemblables d’autant plus qu’elles sont voisines ou proches. 2.3.1.3 Significativité du I de Moran Comme pour le coefficient de corrélation calculé entre deux variables, il est possible de tester la significativité de la valeur du I de Moran obtenu. Sans que nous détaillions les calculs de significativité, notez qu’il existe trois manières de tester la significativité : selon l’hypothèse de la normalité; selon l’hypothèse de la randomisation; selon des permutations Monte-Carlo (habituellement avec 999 échantillons). Comment calculer les trois tests de significativité du I de Moran? Pour une description détaillée du calcul des trois tests, consultez l’ouvrage de Jean Dubé et Diego Legros (2014). 2.3.2 Mise en œuvre dans R Calcul du I de Moran dans R Pour illustrer le calcul de I de Moran dans R, nous utilisons une couche des aires de diffusion (AD) de la ville de Sherbrooke. Les étapes suivantes sont réalisées : Construire une panoplie de matrices de pondération spatiale selon la contiguïté, la connectivité, la proximité et le critère des plus proches voisins. Comparer les valeurs de significativité (p) pour une variable continue (HabKm2). Pour cette même variable, trouver avec quelle matrice la valeur du I de Moran est la plus forte. Comparer les valeurs de I de Moran calculées sur plusieurs variables. 2.3.2.1 Étape 1. Construction des matrices de pondération spatiale library(sf) library(spdep) ## Importation de la couche des aires de diffusion de la ville de Sherbrooke AD.DR &lt;- st_read(dsn = &quot;data/chap02/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DR_SherbADDonnees2021&quot;, quiet=TRUE) ## Matrices de contiguïté ############################################## ## Partage d&#39;un nœud (Queen) Queen &lt;- poly2nb(AD.DR, queen=TRUE) W.Queen &lt;- nb2listw(Queen, zero.policy=TRUE, style = &quot;W&quot;) ## Partage d&#39;un segment (Rook) Rook &lt;- poly2nb(AD.DR, queen=FALSE) W.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = &quot;W&quot;) ## Partage d&#39;un segment (Rook) et ordres d&#39;adjacence de 2 à 5 Rook2 &lt;- nblag_cumul(nblag(Rook, 2)) Rook3 &lt;- nblag_cumul(nblag(Rook, 3)) Rook4 &lt;- nblag_cumul(nblag(Rook, 4)) Rook5 &lt;- nblag_cumul(nblag(Rook, 5)) W.Rook2 &lt;- nb2listw(Rook2, zero.policy=TRUE, style = &quot;W&quot;) W.Rook3 &lt;- nb2listw(Rook3, zero.policy=TRUE, style = &quot;W&quot;) W.Rook4 &lt;- nb2listw(Rook4, zero.policy=TRUE, style = &quot;W&quot;) W.Rook5 &lt;- nb2listw(Rook5, zero.policy=TRUE, style = &quot;W&quot;) ## Matrice de connectivité ############################################## ## Matrice binaire avec un seuil de 2500 mètres Connect2500m &lt;- dnearneigh(st_centroid(AD.DR), d1 = 0, d2 = 2500) W.Connect2500m &lt;- nb2listw(Connect2500m, zero.policy=TRUE, style = &quot;W&quot;) ## Matrices de proximité ############################################## ## Coordonnées géographiques et matrice de distance coords &lt;- st_coordinates(st_centroid(AD.DR)) distances &lt;- as.matrix(dist(coords, method = &quot;euclidean&quot;)) diag(distances) &lt;- 0 ## Matrices inverse de la distance et inverse de la distance au carré InvDistances &lt;- ifelse(distances!=0, 1/distances, distances) InvDistances2 &lt;- ifelse(distances!=0, 1/distances^2, distances) ## Matrices de pondération spatiale standardisées en ligne W.InvDistances &lt;- mat2listw(InvDistances, style=&quot;W&quot;) W.InvDistances2 &lt;- mat2listw(InvDistances2, style=&quot;W&quot;) ## Création de différentes matrices avec différents seuils InvDistances.2500m &lt;- ifelse(distances&lt;=2500 &amp; distances!=0, 1/distances, 0) InvDistances.5000m &lt;- ifelse(distances&lt;=5000 &amp; distances!=0, 1/distances, 0) InvDistances2.2500m &lt;- ifelse(distances&lt;=2500 &amp; distances!=0, 1/distances^2, 0) InvDistances2.5000m &lt;- ifelse(distances&lt;=5000 &amp; distances!=0, 1/distances^2, 0) W.InvDistances_2500 &lt;- mat2listw(InvDistances.2500m, style=&quot;W&quot;) W.InvDistances_5000 &lt;- mat2listw(InvDistances.5000m, style=&quot;W&quot;) W.InvDistances2_2500 &lt;- mat2listw(InvDistances2.2500m, style=&quot;W&quot;) W.InvDistances2_5000 &lt;- mat2listw(InvDistances2.5000m, style=&quot;W&quot;) ## Matrice de distance réduite standardisée k1 &lt;- knn2nb(knearneigh(coords)) plusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords))) Voisins.DistMax &lt;- dnearneigh(coords, 0, plusprochevoisin.max) distances &lt;- nbdists(Voisins.DistMax, coords) InvDistances &lt;- lapply(distances, function(x) (1/x)) InvDistances2 &lt;- lapply(distances, function(x) (1/x^2)) W_InvDistancesReduite &lt;- nb2listw(Voisins.DistMax, glist = InvDistances, style = &quot;W&quot;) W_InvDistances2Reduite &lt;- nb2listw(Voisins.DistMax, glist = InvDistances2, style = &quot;W&quot;) ## Matrice selon le critère des plus proches voisins ##################################################### ## Matrices des plus proches voisins de 2 à 5 k2 &lt;- knn2nb(knearneigh(coords, k = 2)) k3 &lt;- knn2nb(knearneigh(coords, k = 3)) k4 &lt;- knn2nb(knearneigh(coords, k = 4)) k5 &lt;- knn2nb(knearneigh(coords, k = 5)) ## Matrices de pondération spatiale standardisées en ligne W.k2 &lt;- nb2listw(k2, zero.policy=FALSE, style = &quot;W&quot;) W.k3 &lt;- nb2listw(k3, zero.policy=FALSE, style = &quot;W&quot;) W.k4 &lt;- nb2listw(k4, zero.policy=FALSE, style = &quot;W&quot;) W.k5 &lt;- nb2listw(k5, zero.policy=FALSE, style = &quot;W&quot;) 2.3.2.2 Étape 2. Calcul du I de Moran et des trois tests de significativité Calculons la statistique du I de Moran sur la variable continue cartographiée à la figure 2.12. Figure 2.12: Densité de population, aires de diffusion de la ville de Sherbrooke Les fonctions moran.test et moran.mc du package spdep permettent de calculer le I de Moran selon les trois façons de tester la significativité : selon l’hypothèse de la normalité avec le paramètre randomisation = FALSE moran.test(ObjetSf$Variable, listw=MatriceW, zero.policy=TRUE, randomisation = FALSE) selon l’hypothèse de la randomisation avec le paramètre randomisation = TRUE moran.test(ObjetSf$Variable, listw=MatriceW, zero.policy=TRUE, randomisation = TRUE) selon des permutations Monte-Carlo (ci-dessus avec 999 permutations) moran.mc(ObjetSf$Variable, listw=MatriceW, zero.policy=TRUE, nsim=999) Bien entendu, dans les sorties des trois méthodes, la valeur du I de Moran est la même, contrairement à la valeur de p qui peut varier. moran.test(AD.DR$HabKm2, # nom de l&#39;objet sf et de la variable continue listw=W.Queen, # nom de la matrice de pondération spatiale zero.policy=TRUE, randomisation = FALSE) # significativité selon l’hypothèse de la normalité ## ## Moran I test under normality ## ## data: AD.DR$HabKm2 ## weights: W.Queen ## ## Moran I statistic standard deviate = 11.724, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.433714579 -0.004032258 0.001394035 moran.test(AD.DR$HabKm2, # nom de l&#39;objet sf et de la variable continue listw=W.Queen, # nom de la matrice de pondération spatiale zero.policy=TRUE, randomisation = TRUE) # significativité selon l’hypothèse de la randomisation ## ## Moran I test under randomisation ## ## data: AD.DR$HabKm2 ## weights: W.Queen ## ## Moran I statistic standard deviate = 11.761, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.433714579 -0.004032258 0.001385364 moran.mc(AD.DR$HabKm2, # nom de l&#39;objet sf et de la variable continue listw=W.Queen, # nom de la matrice de pondération spatiale zero.policy=TRUE, nsim=999) # 999 permutations ## ## Monte-Carlo simulation of Moran I ## ## data: AD.DR$HabKm2 ## weights: W.Queen ## number of simulations + 1: 1000 ## ## statistic = 0.43371, observed rank = 1000, p-value = 0.001 ## alternative hypothesis: greater Nous allons calculer la mesure du I de Moran sur la variable continue cartographiée à la figure 2.13. La statistique du I de Moran (I = 0,43, p &lt; 0,001) indique que la variable densité de population a une forte autocorrélation spatiale positive (figure 2.13), avec des valeurs fortes dans les aires de diffusion contiguës dans la partie centrale de la ville et des valeurs faibles dans les aires de diffusion contiguës dans les secteurs périphériques (figure 2.12). Figure 2.13: Résultats du I de Moran selon l’hypothèse de la loi normale 2.3.2.3 Étape 3. Identification de la plus forte autocorrélation spatiale selon les différentes matrices La syntaxe ci-dessous permet de calculer la statistique du I de Moran avec plusieurs matrices de pondération spatiale. ## Création d&#39;un vecteur pour les noms des matrices VecteurMatrices &lt;- c(&quot;W.Queen&quot;, &quot;W.Rook&quot;, &quot;W.Rook2&quot;, &quot;W.Rook3&quot;, &quot;W.Rook4&quot;, &quot;W.Rook5&quot;, &quot;W.Connect2500m&quot;, &quot;W.InvDistances&quot;, &quot;W.InvDistances2&quot;, &quot;W_InvDistancesReduite&quot;, &quot;W_InvDistances2Reduite&quot;, &quot;W.InvDistances_2500&quot;, &quot;W.InvDistances_5000&quot;, &quot;W.InvDistances2_2500&quot;,&quot;W.InvDistances2_5000&quot;, &quot;W.k2&quot;, &quot;W.k3&quot;, &quot;W.k4&quot;, &quot;W.k5&quot;) ## Création d&#39;une liste pour toutes les matrices ListeMatrices &lt;- list(W.Queen, W.Rook, W.Rook2, W.Rook3, W.Rook4, W.Rook5, W.Connect2500m, W.InvDistances, W.InvDistances2, W_InvDistancesReduite, W_InvDistances2Reduite, W.InvDistances_2500, W.InvDistances2_2500, W.InvDistances2_2500, W.InvDistances2_5000, W.k2, W.k3, W.k4, W.k5) ## Vecteur pour le I de Moran et la valeur de p MoranI &lt;- c() Pvalue &lt;- c() i&lt;-0 ## Boucle pour calculer le I de Moran avec la liste des matrices for (e in ListeMatrices){ i&lt;-i+1 Test &lt;-moran.mc(AD.DR$HabKm2, listw=e, zero.policy=TRUE, nsim=999) MoranI[i]&lt;-Test$statistic Pvalue[i] &lt;- Test$p.value } # Création d&#39;un DataFrame avec les valeurs du I de Moran et de p MoranData1 &lt;- data.frame(Matrices=VecteurMatrices, MoranIs=MoranI, Pvalues=Pvalue) print(MoranData1) ## Matrices MoranIs Pvalues ## 1 W.Queen 0.43371458 0.001 ## 2 W.Rook 0.44970946 0.001 ## 3 W.Rook2 0.32509097 0.001 ## 4 W.Rook3 0.21527754 0.001 ## 5 W.Rook4 0.12614476 0.001 ## 6 W.Rook5 0.07129756 0.001 ## 7 W.Connect2500m 0.25583250 0.001 ## 8 W.InvDistances 0.10632882 0.001 ## 9 W.InvDistances2 0.27216034 0.001 ## 10 W_InvDistancesReduite 0.28566705 0.001 ## 11 W_InvDistances2Reduite 0.38836327 0.001 ## 12 W.InvDistances_2500 0.32115230 0.001 ## 13 W.InvDistances_5000 0.40350492 0.001 ## 14 W.InvDistances2_2500 0.40350492 0.001 ## 15 W.InvDistances2_5000 0.34988630 0.001 ## 16 W.k2 0.51070049 0.001 ## 17 W.k3 0.44458619 0.001 ## 18 W.k4 0.44800959 0.001 ## 19 W.k5 0.43874109 0.001 La lecture détaillée du tableau 2.3 permet d’avancer plusieurs constats intéressants : D’emblée, signalons que toutes les valeurs sont positives et significatives, témoignant d’une autocorrélation spatiale positive. Concernant les matrices de contiguïté, la dépendance spatiale est plus forte selon le partage d’un segment que d’un nœud (0,4497 contre 0,4337). Par conséquent, si nous devons choisir une matrice de contiguïté, il serait préférable d’utiliser celle définie selon le partage d’une chaîne (Rook). Sans surprise, plus nous ajoutons des ordres d’adjacence, plus la valeur de la statistique du I de Moran est faible, passant de 0,3251 à 0,0713 du deuxième au cinquième ordre. La valeur du I de Moran avec une matrice de connectivité avec 2500 mètres est de 0,2558. Elle est plus faible que celles de l’inverse de la distance et l’inverse de la distance au carré, avec le même seuil de 2500 mètres (0,3212 et 0,4035). Concernant les matrices de proximité, la méthode de l’inverse de la distance au carré, qui accorde un poids plus important aux entités spatiales très proches (comparativement à l’inverse de la distance), renvoie des valeurs toujours plus élevées, et ce, que la matrice soit complète ou réduite. Aussi, les matrices de distance réduites présentent toujours des valeurs plus fortes que celles complètes. Concernant les matrices selon le critère des plus proches voisins, l’autocorrélation spatiale diminue légèrement de k = 2 à k = 5. D’ailleurs, la valeur la plus forte est pour deux voisins (I = 0,5107). Toutefois, retenir uniquement deux voisins est discutable puisque les AD sont très majoritairement contiguës à plus de deux autres AD (sur les 249 AD, seuls 9 sont contiguës à deux autres AD selon le partage d’un segment). Pour le vérifier, tapez summary(W.Rook) et analysez le tableau sous la ligne Link number distribution. Tableau 2.3: Résultats du I de Moran selon les différentes matrices Nom Description I de Moran p (999 permutations) Matrices de contiguïté W.Queen Partage d’un nœud 0,4337 0,001 W.Rook Partage d’un segment 0,4497 0,001 Matrices de contiguïté selon le partage d’un segment et ordre d’adjacence W.Rook2 Ordre 2 0,3251 0,001 W.Rook3 Ordre 3 0,2153 0,001 W.Rook4 Ordre 4 0,1261 0,001 W.Rook5 Ordre 5 0,0713 0,001 Matrices de connectivité W.Connect2500m 2500 mètres 0,2558 0,001 Matrices de distance (complètes) W.InvDistances Inverse de la distance 0,1063 0,001 W.InvDistances2 Inverse de la distance au carré 0,2722 0,001 Matrices de distance (réduites) W_InvDistancesReduite Inverse de la distance 0,2857 0,001 W_InvDistances2Reduite Inverse de la distance au carré 0,3884 0,001 Matrices de distance avec un seuil maximal W.InvDistances_2500 Inverse de la distance (2500 mètres) 0,3212 0,001 W.InvDistances_5000 Inverse de la distance (5000 mètres) 0,4035 0,001 W.InvDistances2_2500 Inverse de la distance au carré (2500 mètres) 0,4035 0,001 W.InvDistances2_5000 Inverse de la distance au carré (5000 mètres) 0,3499 0,001 Matrices selon le critère des plus proches voisins W.k2 k = 2 0,5107 0,001 W.k3 k = 3 0,4446 0,001 W.k4 k = 4 0,4480 0,001 W.k5 k = 5 0,4387 0,001 Quelle est la matrice avec laquelle la dépendance spatiale de la variable est la plus forte? Pour la trouver, nous construisons un graphique avec les valeurs du I de Moran triées par ordre décroissant. La valeur la plus forte est obtenue avec la matrice selon les deux plus proches voisins, suivie de la matrice Rook. Quoi qu’il en soit, il serait plus judicieux de privilégier la matrice de contiguïté selon le partage d’un segment comme expliqué plus haut. library(ggplot2) ggplot(data=MoranData1, aes(x=reorder(Matrices,MoranIs), y=MoranIs)) + geom_segment( aes(x=reorder(Matrices,MoranIs), xend=reorder(Matrices,MoranIs), y=0, yend=MoranIs)) + geom_point( size=4,fill=&quot;red&quot;,shape=21)+ xlab(&quot;Matrice de pondération spatiale&quot;) + ylab(&quot;I de Moran&quot;)+ coord_flip() Figure 2.14: Graphique avec les valeurs du I de Moran selon les différentes matrices de pondération spatiale 2.3.2.4 Étape 4. Comparaison des valeurs du I de Moran pour plusieurs variables avec la même matrice La syntaxe ci-dessous permet de calculer la statistique du I de Moran pour plusieurs variables (figure 2.15) avec la même matrice de pondération spatiale (ici, matrice de contiguïté selon le partage d’un segment). Figure 2.15: Quatre variables sélectionnées pour les AD de la ville de Sherbrooke ## Vecteur pour les variables à analyser listeVars &lt;- c(&quot;PctLog1960_Av&quot;, &quot;RevMedMenage&quot; , &quot;PctProprieta&quot;, &quot;PctPop0_14&quot;) ## Vecteur pour le I de Moran et la valeur de p MoranI &lt;- c() VarName &lt;- c() Pvalue &lt;- c() i&lt;-0 ## Boucle pour calculer le I de Moran pour les différentes variables for (e in listeVars){ i&lt;-i+1 Test &lt;- moran.mc(AD.DR[[e]], listw=W.Rook, zero.policy=TRUE, nsim=999) MoranI[i]&lt;-round(Test$statistic,4) VarName[i]&lt;-e Pvalue[i] &lt;- Test$p.value } # Création d&#39;un DataFrame avec les valeurs du I de Moran et de p MoranData2 &lt;- data.frame(Variable=listeVars, MoranIs=MoranI, Pvalues=Pvalue) print(MoranData2) ## Variable MoranIs Pvalues ## 1 PctLog1960_Av 0.7071 0.001 ## 2 RevMedMenage 0.6168 0.001 ## 3 PctProprieta 0.6028 0.001 ## 4 PctPop0_14 0.4474 0.001 De nouveau, en quelques lignes de code, il est aisé de réaliser un graphique pour comparer les valeurs du I de Moran pour les différentes variables (figure 2.16). library(ggplot2) ggplot(data=MoranData2, aes(x=reorder(Variable,MoranIs), y=MoranIs)) + geom_segment( aes(x=reorder(Variable,MoranIs), xend=reorder(Variable,MoranIs), y=0, yend=MoranIs)) + geom_point( size=4,fill=&quot;red&quot;,shape=21)+ xlab(&quot;Variable continue&quot;) + ylab(&quot;I de Moran&quot;)+ coord_flip() Figure 2.16: Graphique avec les valeurs du I de Moran pour les quatre variables References "],["sect024.html", "2.4 Autocorrélation spatiale locale", " 2.4 Autocorrélation spatiale locale Nous avons vu que les statistiques d’autocorrélation globale comme le I de Moran renvoie une valeur pour l’ensemble de l’espace d’étude. Une fois démontrée la présence d’autocorrélation spatiale (positive ou négative), il est pertinent de réaliser une analyse de l’autocorrélation spatiale locale afin de vérifier si chaque entité spatiale est significativement (dis)semblable à celles voisines ou proches. Comme l’indique le terme locale, les mesures d’autocorrélation spatiale locale renvoient des valeurs pour chacune des entités spatiales. 2.4.1 Statistiques locales de Getis et Ord : repérer les points chauds et froids Les statistiques locales de Getis et Ord permettent d’évaluer la similarité d’une entité spatiale avec celles voisines ou proches (Getis et Ord 1992; Ord et Getis 1995). Autrement dit, elles nous informent si les valeurs fortes et les valeurs faibles d’une variable continue se regroupent significativement dans l’espace, et ce, avec n’importe quel type de matrice de contiguïté, de proximité, de plus proches voisins, etc. Cartographier ces statistiques permet alors de vérifier simultanément l’existence d’agrégats spatiaux de valeurs fortes (points chauds) et d’agrégats spatiaux de valeurs faibles (points froids). Il existe deux versions légèrement différentes de ces mesures locales (Getis et Ord 1992; Ord et Getis 1995) : \\(G_i\\) tient compte uniquement des entités voisines ou proches à l’entité spatiale \\(i\\). Ainsi, \\(\\Sigma_{j=1}^n w_{ij}x_j\\) représente la moyenne pondérée (par les poids de la matrice de pondération spatiale standardisée en ligne) de la variable continue \\(X\\) dans les entités voisines ou proches. \\(G_i^*\\) tient compte à la fois des valeurs des entités voisines ou proches, mais aussi de celle de \\(i\\). Toutefois, nous cartographions rarement les valeurs de \\(G_i\\) et de \\(G_i^*\\), mais plutôt celles de \\(Z(G_i)\\) et de \\(Z(G_i^*)\\) (équations (2.12) et (2.13)) qui représentent la cote Z qui, lorsque positive, indique un agrégat de valeurs plus élevées que la moyenne, et qui, lorsque négative, indique un agrégat de valeurs plus faibles que la moyenne (Bivand et Wong 2018). À première vue, ces deux formules peuvent sembler quelque peu complexes! Retenez simplement que le numérateur est la différence entre \\(G_i\\) ou \\(G_i^*\\) et la valeur attendue de \\(G_i\\) ou de \\(G_i^*\\) pour une distribution aléatoire (soit globalement la moyenne de la variable), tandis que le dénominateur représente l’écart-type de \\(G_i\\) ou de \\(G_i^*\\). \\[\\begin{equation} Z(G_i) = \\frac{\\bigr[ \\Sigma_{j=1}^n w_{ij}x_j \\bigr] - \\bigr[ (\\Sigma_{j=1}^nw_{ij}) \\bar x_i \\bigr]} {s_i \\sqrt{ \\Bigr[ \\Bigl((n-1)\\Sigma_{j=1}^nw_{ij}^2-(\\Sigma_{j=1}^nw_{ij})^2\\Bigl) \\Bigr] / (n-1) } } = \\frac{G_i - \\mathbb{E}(G_i)}{\\sqrt{Var(G_i)}} \\text{, }i \\neq j \\tag{2.12} \\end{equation}\\] \\[\\text{avec } \\bar x_i=\\frac{\\Sigma_{j=1}^nx_j}{n-1} \\text{, et } s_i = \\sqrt{ \\frac{\\Sigma_{j=1}^nx_j^2}{n-1}-\\bar x^2} \\text{, } i \\neq j\\] \\[\\begin{equation} Z(G_i^*) = \\frac{\\bigr[ \\Sigma_{j=1}^n w_{ij}x_j \\bigr] - \\bigr[ (\\Sigma_{j=1}^nw_{ij}) \\bar x^* \\bigr]} {s^* \\sqrt{ \\Bigr[ \\Bigl((n-1)\\Sigma_{j=1}^nw_{ij}^2-(\\Sigma_{j=1}^nw_{ij})^2\\Bigl) \\Bigr] / (n-1) } } = \\frac{G_i^* - \\mathbb{E}(G_i^*)}{\\sqrt{Var(G_i^*)}}\\text{, tous } \\tag{2.13} \\end{equation}\\] \\[\\text{avec } \\bar x^*=\\frac{\\Sigma_{j=1}^nx_j}{n} \\text{, et } s^* = \\sqrt{ \\frac{\\Sigma_{j=1}^nx_j^2}{n}-\\bar x^{*2}} \\text{, tous } j\\] avec : n, le nombre d’entités spatiales dans la couche géographique; \\(w_{ij}\\), la valeur de la pondération spatiale entre les entités spatiales \\(i\\) et \\(j\\); \\(x_j\\), la valeur de variable continue \\(X\\) pour l’entité spatiale \\(j\\); \\(\\bar{x^*}\\), la valeur moyenne de la variable pour toutes les observations; \\(\\bar{x_i}\\), la valeur moyenne de la variable pour toutes les observations sauf \\(i\\). Deux manières de calculer \\(Z(G_i)\\) et \\(Z(G_i^*)\\) avec le package spdep la fonction localG vous renvoie les valeurs de cote Z (Z-score en anglais) des statistiques de Getis et Ord avec les formules décrites plus haut. la fonction localG_perm permet de les obtenir avec la méthode Monte-Carlo (avec habituellement 999 permutations). Cartographie des cotes Z À partir des cotes Z, utilisez les bornes des classes suivantes et la palette de couleurs -RdBu : Minimum à -3,29 : point froid significatif avec p &lt; 0,001 (bleu foncé) -3,29 à -2,58 : point froid significatif avec p &lt; 0,01 (bleu) -2,58 à -1,96 : point froid significatif avec p &lt; 0,05 (bleu pâle) -1,96 à 1,96 : non significatif, (gris) 1,96 à 2,58 : point chaud significatif avec p &lt; 0,05 (rouge pâle) 2,58 à 3,29 : point chaud significatif avec p &lt; 0,01 (rouge) 3,29 à Maximum : point chaud significatif avec *p &lt;0 ,001 (rouge foncé) Avec la palette -RdBu, les points froids et les points chauds sont respectivement bleus et rouges comme les pastilles de votre robinet! Bref, un beau travail de plomberie! Pour un rappel sur la cote Z et les valeurs de p associées, consultez ce lien. La syntaxe ci-dessous calcule les deux statistiques locales avec localG et localG_perm pour le revenu moyen des ménages pour les aires de diffusion de 2021 de la ville de Sherbrooke. Rook1 &lt;- poly2nb(AD.DR, queen=FALSE) ## Matrice de pondération W.RookG &lt;- nb2listw(Rook1, zero.policy=TRUE) W.RookStar &lt;-nb2listw(include.self(Rook1), zero.policy=TRUE) # matrice incluant elle-même ## Calcul du Z(Gi) et du Z(Gi*) localGetis &lt;- localG(AD.DR$RevMedMenage, W.RookG) localGetisStar &lt;- localG(AD.DR$RevMedMenage, W.RookStar) ## Calcul avec la méthode Monte-Carlo (999 permutations) localGetis.MC999 &lt;- localG_perm(AD.DR$RevMedMenage, W.RookG, nsim = 999) localGetis.StarMC999 &lt;- localG_perm(AD.DR$RevMedMenage, W.RookStar, nsim = 999) ## Sommaires statistiques summary(localGetis) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.80885 -1.70432 -0.09828 -0.04120 1.41123 3.91835 summary(localGetisStar) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -4.04870 -1.88689 -0.13297 -0.03841 1.54641 4.10379 summary(localGetis.MC999) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.80885 -1.70432 -0.09828 -0.04120 1.41123 3.91835 summary(localGetis.StarMC999) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -4.04870 -1.88689 -0.13297 -0.03841 1.54641 4.10379 Cartographions ces valeurs et repérons les regroupements de valeurs significativement fortes et faibles avec les statistiques de Getis et Ord (figures 2.17 et 2.18). Les quatre cartes sont très semblables et permettent de repérer un regroupement de valeurs faibles dans le centre-ville et un autre de valeurs fortes dans l’est de la ville. ## Enregistrons les résultats dans deux nouveaux champs de la couche des AD AD.DR$RevMed_localGetis &lt;- localGetis AD.DR$RevMed_localGetisStar &lt;- localGetisStar # Définition des intervalles et des noms des classes classes.intervalles = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf) classes.noms = c(&quot;Point froid (p = 0,001)&quot;, &quot;Point froid (p = 0,01)&quot;, &quot;Point froid (p = 0,05)&quot;, &quot;Non significatif&quot;, &quot;Point chaud (p = 0,05)&quot;, &quot;Point chaud (p = 0,01)&quot;, &quot;Point chaud (p = 0,001)&quot;) # Création d&#39;un champ avec les noms des classes AD.DR$RevMed_localGetisP &lt;- cut(AD.DR$RevMed_localGetis, breaks = classes.intervalles, labels = classes.noms) AD.DR$RevMed_localGetisStarP &lt;- cut(AD.DR$RevMed_localGetisStar, breaks = classes.intervalles, labels = classes.noms) ## Cartographie pour le Z(Gi) Carte1 = tm_shape(AD.DR)+ tm_polygons(col =&quot;RevMed_localGetisP&quot;, title=&quot;Z(Gi)&quot;, palette=&quot;-RdBu&quot;, lwd = 1)+ tm_layout(frame =FALSE) ## Cartographie pour le Z(Gi)* Carte2 = tm_shape(AD.DR)+ tm_polygons(col =&quot;RevMed_localGetisStarP&quot;, title=&quot;Z(Gi*)&quot;, palette=&quot;-RdBu&quot;, lwd = 1)+ tm_layout(frame =FALSE)+ tm_credits(&quot;Auteurs : Geoffroy et Jessie Chaux.&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;), size = 0.7, align = &quot;right&quot;) ## Composition avec les deux cartes tmap_arrange(Carte1, Carte2, ncol = 2, nrow = 1) Figure 2.17: Points chauds et froids du revenu médian des ménages selon les statistiques de Getis et Ord (méthode classique) ## Enregistrons les résultats dans deux champs de la couche des AD AD.DR$RevMed_localGetis.MC999 &lt;- localGetis.MC999 AD.DR$RevMed_localGetis.StarMC999 &lt;- localGetis.StarMC999 coupures = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf) # Définition des intervalles et des noms des classes classes.intervalles = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf) classes.noms = c(&quot;Point froid (p = 0,05)&quot;, &quot;Point froid (p = 0,01)&quot;, &quot;Point froid (p = 0,001)&quot;, &quot;Non significatif&quot;, &quot;Point chaud (p = 0,05)&quot;, &quot;Point chaud (p = 0,01)&quot;, &quot;Point chaud (p = 0,001)&quot;) # Création d&#39;un champ avec les noms des classes AD.DR$RevMed_localGetis.MC999P &lt;- cut(AD.DR$RevMed_localGetis.MC999, breaks = classes.intervalles, labels = classes.noms) AD.DR$RevMed_localGetis.StarMC999P &lt;- cut(AD.DR$RevMed_localGetis.StarMC999, breaks = classes.intervalles, labels = classes.noms) ## Cartographie pour le Z(Gi) Mont-Carlo Carte3 = tm_shape(AD.DR)+ tm_polygons(col =&quot;RevMed_localGetis.MC999&quot;, title=&quot;Z(Gi)&quot;, palette=&quot;-RdBu&quot;, lwd = 1)+ tm_layout(frame =FALSE) ## Cartographie pour le Z(Gi)* Mont-Carlo Carte4 = tm_shape(AD.DR)+ tm_polygons(col =&quot;RevMed_localGetis.StarMC999&quot;, title=&quot;Z(Gi*)&quot;, palette=&quot;-RdBu&quot;, lwd = 1)+ tm_layout(frame =FALSE)+ tm_credits(&quot;Auteurs : Geoffroy et Jessie Chaux.&quot;, position = c(&quot;right&quot;, &quot;bottom&quot;), size = 0.7, align = &quot;right&quot;) ## Composition avec les deux cartes tmap_arrange(Carte3, Carte4, ncol = 2, nrow = 1) Figure 2.18: Points chauds et froids du revenu médian des ménages selon les statistiques de Getis et Ord (999 permutations Monte-Carlo) 2.4.2 Version locale du I de Moran Une version locale du I de Moran (\\(I_i\\)) a été proposée par Luc Anselin (1995). Elle permet de vérifier si une entité spatiale est voisine ou proche – dépendamment de la matrice spatiale utilisée – d’entités spatiales avec des valeurs semblables (contexte d’autocorrélation spatiale locale positive) ou dissemblables (contexte d’autocorrélation spatiale locale négative). Le I de Moran local s’écrit : \\[\\begin{equation} I_i = \\frac{(x_i-\\bar{X})\\Sigma_{j=1}^n w_{ij}(x_j-\\bar{X})} {\\frac{1}{n} \\Sigma_{i=1}^n(x_i-\\bar{X})^2} = \\frac{z_i \\Sigma_{j=1}^n z_j}{\\frac{1}{n}\\Sigma_{i=1}^n z_i^2} \\text{, } i \\ne j \\tag{2.14} \\end{equation}\\] avec : \\(z_i\\) étant la valeur de la variable continue centrée \\(X\\) pour l’entité spatiale \\(i\\), c’est-à-dire simplement l’écart de \\(i\\) à la moyenne de \\(X\\) (\\(x_i - \\bar X\\)). \\(z_j\\) étant la valeur de la variable centrée de \\(X\\) pour l’entité spatiale \\(j\\). \\(w_{ij}\\) étant la valeur de la matrice de pondération spatiale standardisée en ligne entre \\(i\\) et \\(j\\). \\(n\\) étant le nombre d’entités spatiales dans la couche géographique. Comme pour la version globale, il est possible de tester la significativité du I de Moran local de manière classique (selon la loi normale) ou avec l’approche Monte-Carlo (avec habituellement 999 permutations). Test de significativité du I de Moran local. Pour comprendre les différentes variantes pour tester la significativité (Anselin 1995; Sokal, Oden et Thomson 1998), consultez Bivand et Wong (2018) ou l’ouvrage de Dubé et Legros (2014). Le calcul du I de Moran local s’opère avec les fonctions localmoran et localMoranI.mc du package spdep (voir la syntaxe ci-dessous). Comme pour la version globale, les résultats du I de Moran local sont les mêmes pour les deux fonctions, seules les valeurs de p peuvent varier. ## Calcul du I de Moran local localMoranI &lt;- localmoran(AD.DR$RevMedMenage, # variable W.RookG) # matrice de pondération spatiale ## Calcul du I de Moran local avec la méthode Monte-Carlo localMoranI.mc &lt;- localmoran_perm(AD.DR$RevMedMenage, # variable W.RookG, # matrice de pondération spatiale nsim = 999) # nombre de permutations ## Sommaires statistiques summary(localMoranI) ## Ii E.Ii Var.Ii Z.Ii ## Min. :-0.90053 Min. :-3.054e-02 Min. :0.0000003 Min. :-1.9962 ## 1st Qu.: 0.08313 1st Qu.:-5.368e-03 1st Qu.:0.0370546 1st Qu.: 0.4539 ## Median : 0.49029 Median :-2.755e-03 Median :0.1339044 Median : 1.4404 ## Mean : 0.61678 Mean :-4.032e-03 Mean :0.2149974 Mean : 1.2503 ## 3rd Qu.: 0.97602 3rd Qu.:-8.287e-04 3rd Qu.:0.2510284 3rd Qu.: 2.1938 ## Max. : 3.37625 Max. :-8.000e-09 Max. :1.8207164 Max. : 3.9184 ## Pr(z != E(Ii)) ## Min. :0.0000892 ## 1st Qu.:0.0282519 ## Median :0.1268505 ## Mean :0.2467939 ## 3rd Qu.:0.3948293 ## Max. :0.9833048 summary(localMoranI.mc) ## Ii E.Ii Var.Ii Z.Ii ## Min. :-0.90053 Min. :-0.069635 Min. :0.0000003 Min. :-1.974 ## 1st Qu.: 0.08313 1st Qu.:-0.010034 1st Qu.:0.0365815 1st Qu.: 0.449 ## Median : 0.49029 Median :-0.001348 Median :0.1392868 Median : 1.440 ## Mean : 0.61678 Mean :-0.003444 Mean :0.2185429 Mean : 1.236 ## 3rd Qu.: 0.97602 3rd Qu.: 0.002914 3rd Qu.:0.2605064 3rd Qu.: 2.181 ## Max. : 3.37625 Max. : 0.044310 Max. :1.7678673 Max. : 3.927 ## Pr(z != E(Ii)) Pr(z != E(Ii)) Sim Pr(folded) Sim Skewness ## Min. :0.0000861 Min. :0.0020 Min. :0.0010 Min. :-0.37231 ## 1st Qu.:0.0292025 1st Qu.:0.0200 1st Qu.:0.0100 1st Qu.:-0.21930 ## Median :0.1259498 Median :0.1300 Median :0.0650 Median :-0.12122 ## Mean :0.2492671 Mean :0.2545 Mean :0.1272 Mean :-0.01551 ## 3rd Qu.:0.4062386 3rd Qu.:0.4220 3rd Qu.:0.2110 3rd Qu.: 0.21084 ## Max. :0.9892344 Max. :0.9880 Max. :0.4940 Max. : 0.45153 ## Kurtosis ## Min. :-0.52011 ## 1st Qu.:-0.28009 ## Median :-0.18964 ## Mean :-0.17193 ## 3rd Qu.:-0.07416 ## Max. : 0.43919 Avec la cartographie du I de Moran local, nous repérons localement l’autocorrélation spatiale positive (valeurs similaires, fortes ou faibles localement) et l’autocorrélation spatiale négative (valeurs dissemblables localement) (figure 2.19). ## Ajout de champs pour le I de Moran local AD.DR$RevMed_IlocalMoran.MC999 &lt;- localMoranI.mc[, 1] AD.DR$RevMed_IlocalMoran.MC999p &lt;- localMoranI.mc[, 5] ## Cartographie tmap_mode(&quot;plot&quot;) Carte1 = tm_shape(AD.DR)+ tm_polygons(col =&quot;RevMed_IlocalMoran.MC999&quot;, title=&quot;I Moran local&quot;, style=&quot;cont&quot;, palette=&quot;-RdBu&quot;, lwd = 1)+ tm_layout(frame = FALSE) Carte2 = tm_shape(AD.DR)+ tm_polygons(col= &quot;RevMed_IlocalMoran.MC999p&quot;, title=&quot;valeur de p&quot;, palette = c(&quot;darkgreen&quot;, &quot;green&quot;, &quot;lightgreen&quot;, &quot;gray&quot;), lwd = 1, breaks = c(0, 0.001, 0.01, 0.05, Inf), title =&quot;En %&quot;)+ tm_layout(frame = FALSE) ## Combinaison des deux cartes tmap_arrange(Carte1, Carte2, ncol = 2, nrow = 1) Figure 2.19: Cartographie du I de Moran local et de la valeur de p associée 2.4.3 Nuage de points du I de Moran d’Anselin Le nuage de points du I de Moran a été proposé par Luc Anselin (1996). L’idée est fort simple, mais extrêmement efficace! Avant tout, la variable continue est centrée réduite (cote \\(z\\), z-score en anglais). Pour un rappel sur la cote \\(z\\), consultez la section suivante (Apparicio et Gelb 2022). La moyenne est ainsi égale à 0 et l’écart-type à 1. Puis, il s’agit de construire un nuage de points entre : Les valeurs de la variable centrée réduite (Z) sur l’axe des X pour les entités spatiales de la couche géographique. Les valeurs de la variable centrée réduite spatialement décalée obtenues avec les pondération spatiale de la matrice \\(W\\) (voir l’encadré ci-dessous). Comment calculer une variable spatialement décalée avec une matrice de pondération spatiale? À la figure 2.20, nous détaillons le calcul de la valeur d’une variable spatialement décalée pour l’entité spatiale A à partir d’une matrice de contiguïté (selon le partage d’un segment) standardisée en ligne. Notez que A a quatre voisins (b, c, d et e). Figure 2.20: Illustration du calcul d’une variable spatialement décalée Dans R, la syntaxe est fort simple pour créer une cote \\(z\\) et une variable spatiale décalée : zx &lt;- (x - mean(x))/sd(x) # variable X centrée réduite (cote z) wzx &lt;- lag.listw(listW,zx) # variable X centrée réduite spatialement décalée Analysons les différents éléments du nuage de points du I de Moran à la figure 2.21 : La droite régression résume la relation linéaire entre la variable spatialement décalée (\\(W.ZX\\)) et l’originale (\\(ZX\\)). D’ailleurs, le coefficient de régression pour la variable \\(ZX\\), soit la pente de la droite, équivaut au I de Moran! Les traits pointillés représentent les moyennes des deux variables, toutes deux égales à 0 puisqu’elles sont centrées-réduites (cote \\(z\\)). Pour analyser ce nuage, nous le décomposons en quatre quadrants : Le quadrant HH (High-High) en haut à droite regroupe des entités spatiales avec des valeurs fortes (H) qui sont voisines ou proches d’autres entités spatiales avec aussi des valeurs fortes (H). Nous sommes donc en présence d’autocorrélation spatiale locale positive avec des valeurs fortes (HH). Le quadrant LL (Low-Low) en bas à gauche regroupe des entités spatiales avec des valeurs faibles (L) qui sont voisines ou proches d’autres entités spatiales avec aussi des valeurs faibles (L). Nous sommes donc en présence d’autocorrélation spatiale locale positive avec des valeurs faibles (LL). Le quadrant HL (High-Low) en bas à droite regroupe des entités spatiales avec des valeurs fortes (H) qui sont voisines ou proches d’autres entités spatiales avec des valeurs faibles (L). Nous sommes donc en présence d’autocorrélation spatiale locale négative (HL). Avec humour, un collègue économiste qualifie ce quadrant de village gaulois (Obélix, Astérix et leurs compagnons sont très forts et ils sont entourés de voisins romains plus faibles…). Le quadrant LH (Low-High) en haut à gauche regroupe des entités spatiales avec des valeurs faibles (L) qui sont voisines ou proches d’autres entités spatiales avec des valeurs fortes (H). Nous sommes donc en présence d’autocorrélation spatiale locale négative (LH). Figure 2.21: Nuage de points du I de Moran d’Anselin Pour créer le nuage de points du I de Moran, nous avons écrit la fonction ci-dessous. ################################################################## ## Ne pas modifier les lignes ci-dessous jusqu&#39;au prochain encadré ################################################################## #&#39; Fonction pour créer le nuage de points de Moran. #&#39; @param x un vecteur avec la variable continue X. #&#39; @param listW un objet listw de spdep (matrice de pondération spatiale). #&#39; @param titre du graphique #&#39; @param titreAxeX titre de l&#39;axe des X #&#39; @param titreAxeY titre de l&#39;axe des X #&#39; @param AfficheAide indique l&#39;affichage de l&#39;aide de lecture dans le graphique #&#39; @return un objet ggplot2. NuageMoran &lt;- function(x,listW, titre=&quot;Nuage de points du I de Moran&quot;, titreAxeX = &quot;X&quot;, titreAxeY = &quot;Variable spatialement décalée&quot;, AfficheAide=FALSE) { zx &lt;- (x - mean(x))/sd(x) # variable X centrée réduite (cote z) wzx &lt;- lag.listw(listW,zx) # variable X centrée réduite spatialement décalée typo &lt;- ifelse(zx&gt;0 &amp; wzx&gt;0, &quot;HH&quot;, &quot;ND&quot;) typo &lt;- ifelse(zx&lt;0 &amp; wzx&lt;0, &quot;LL&quot;, typo) typo &lt;- ifelse(zx&gt;0 &amp; wzx&lt;0, &quot;HL&quot;, typo) typo &lt;- ifelse(zx&lt;0 &amp; wzx&gt;0, &quot;LH&quot;, typo) couleurs &lt;- c(&quot;HH&quot; = &quot;darkred&quot;, &quot;LL&quot; = &quot;darkblue&quot;, &quot;HL&quot; = &quot;red&quot;, &quot;LH&quot; = &quot;blue&quot;) if (AfficheAide){ notetxt = &quot;Autocorrélation positive\\n&quot; notetxt = paste(notetxt, &quot;HH : valeur forte dans un contexte de valeurs fortes\\n&quot;) notetxt = paste(notetxt, &quot;LL : valeur faible dans un contexte de valeurs faibles\\n&quot;) notetxt = paste(notetxt, &quot;Autocorrélation négative\\n&quot;) notetxt = paste(notetxt, &quot;HL : valeur forte dans un contexte de valeurs faibles \\n&quot;) notetxt = paste(notetxt, &quot;LH : valeur faible dans un contexte de valeurs fortes&quot;) }else{ notetxt&lt;-&quot;&quot; } morlm &lt;- lm(wzx ~ zx) imoran &lt;- morlm$coefficients[2] par(pty=&quot;s&quot;) Graphique &lt;- ggplot(mapping = aes(x=zx, y=wzx, colour=typo)) + geom_point() + scale_colour_manual(values = couleurs)+ labs( colour = &quot;Typologie&quot;, title = titre, subtitle = paste(&quot;I de Moran = &quot;, format(round(imoran,4))), caption = notetxt) + geom_vline(xintercept = 0, colour=&quot;black&quot;, linetype=&quot;dashed&quot;, size=.5) + geom_hline(yintercept = 0, colour=&quot;black&quot;, linetype=&quot;dashed&quot;, size=.5) + stat_smooth(method=&quot;lm&quot;, se=FALSE, size=1, colour=&quot;black&quot;) + xlab(titreAxeX) + ylab(titreAxeY) return(Graphique) } ################################################################## ## Appel de la fonction, modifier les lignes ci-dessous au besoin ## pour changer la variable ou la matrice de pondération spatiale ################################################################## ## Réalisation du nuage de Moran avec la fonction NuageMoran NuageMoran(x = AD.DR$RevMedMenage, listW = W.Rook, titre = &quot;Nuage de points du I de Moran (matrice de contiguïté selon le partage d&#39;un segment)&quot;, titreAxeX = &quot;ZX : Revenu médian des ménages centré réduit&quot;, titreAxeY = &quot;WZ : Variable ZX spatialement décalée&quot;, AfficheAide=TRUE) Reste à déterminer si l’autocorrélation spatiale locale pour ces quatre quadrants est significative. Rien de plus simple, il suffit d’utiliser la valeur de p du I de Moran (section 2.4.2). Nous choisissons un seuil de signification (habituellement p = 0,05) et obtenons ainsi la typologie comprenant cinq catégories : Autocorrélation spatiale locale positive et significative (p &lt; 0,05). HH LL Autocorrélation spatiale locale négative et significative (p &lt; 0,05). HL LH Autocorrélation spatiale locale non significative (p &gt; 0,05). Le code R ci-dessous permet d’obtenir la typologie de l’autocorrélation spatiale avec le I de Moran local pour le revenu médian des ménages avec une matrice de contiguïté selon le partage d’un segment (Rook). La figure 2.22 dénote surtout une autocorrélation spatiale positive importante (respectivement 25 et 47 aires de diffusion classées HH et LL), comparativement à l’autocorrélation spatiale négative qui ne comprend qu’une aire de diffusion (LH). ## Cote Z (variable centrée réduite) zx &lt;- (AD.DR$RevMedMenage - mean(AD.DR$RevMedMenage))/sd(AD.DR$RevMedMenage) ## variable X centrée réduite spatialement décalée avec une matrice Rook wzx &lt;- lag.listw(W.Rook, zx) ## I de Moran local (notez que vous pouvez aussi utiliser la fonction localmoran_perm) localMoranI &lt;- localmoran(AD.DR$RevMedMenage, W.Rook) #localMoranI.mc &lt;- localmoran_perm(AD.DR$RevMedMenage, W.Rook, n=999) plocalMoranI &lt;- localMoranI[, 5] ## Choix d&#39;un seuil de signification signif = 0.05 ## Construction de la typologie Typologie &lt;- ifelse(zx &gt; 0 &amp; wzx &gt; 0, &quot;1. HH&quot;, NA) Typologie &lt;- ifelse(zx &lt; 0 &amp; wzx &lt; 0, &quot;2. LL&quot;, Typologie) Typologie &lt;- ifelse(zx &gt; 0 &amp; wzx &lt; 0, &quot;3. HL&quot;, Typologie) Typologie &lt;- ifelse(zx &lt; 0 &amp; wzx &gt; 0, &quot;4. LH&quot;, Typologie) Typologie &lt;- ifelse(plocalMoranI &gt; signif, &quot;Non sign&quot;, Typologie) # Non significatif ## Enregistrement de la typologie dans un champ AD.DR$TypoIMoran.RevMedian &lt;- Typologie table(AD.DR$TypoIMoran.RevMedian, useNA = &quot;always&quot;) ## ## 1. HH 2. LL 4. LH Non sign &lt;NA&gt; ## 33 48 1 167 0 ## Couleurs Couleurs &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;lightpink&quot;, &quot;skyblue2&quot;, &quot;lightgray&quot;) names(Couleurs) &lt;- c(&quot;1. HH&quot;,&quot;2. LL&quot;,&quot;3. HL&quot;,&quot;4. LH&quot;,&quot;Non sign&quot;) ## Cartographie tmap_mode(&quot;plot&quot;) tm_shape(AD.DR) + tm_polygons(col = &quot;TypoIMoran.RevMedian&quot;, palette = Couleurs, title =&quot;Autocorrélation spatiale locale&quot;)+ tm_layout(frame = FALSE) Figure 2.22: Typologie de l’autocorrélation spatiale locale avec le I de Moran local Ne pas confondre les statistiques locales de Getis et Ord et la typologie avec le I de Moran local. Succinctement, ces deux familles de mesures renvoient deux typologies différentes : Le I de Moran local comprend cinq classes : HH, LL, HL, LH et non significatif. Les statistiques locales de Getis et Ord comprennent trois classes : points chauds (sensiblement l’équivalent de HH), points froids (sensiblement l’équivalent de LL) et non significatifs. References "],["sect025.html", "2.5 Quiz de révision du chapitre", " 2.5 Quiz de révision du chapitre Parmi les matrices de pondération spatiale ci-dessous, lesquelles sont des matrice de contiguïté? Relisez au besoin la section 2.2. Partage d’un nœud Partage d’un segment Partage d’un nœud et ordre d’adjacence Partage d’un segment et ordre d’adjacence Connectivité selon la distance En anglais, comment est appelée une matrice selon le partage d’un nœud? Relisez au besoin le début de la section 2.2. Rook Queen Comparativement à une matrice de l’inverse de la distance, une matrice de l’inverse de la distance au carré accorde un poids plus important aux entités proches. Relisez au besoin la section 2.2.2.3. Vrai Faux Quels sont les avantages de la standardisation en ligne des matrices de pondération spatiale? Relisez au besoin la section 2.2.3. La somme de chaque ligne est égale à 1. La somme de l’ensemble de la matrice est égale au nombre d’entités spatiales. La standardisation permet de comparer la dépendance spatiale selon différentes matrices. La standardisation augmentation la vitesse des calculs. Quelle est la différence entre les deux mesures locales de Getis et Ord? Relisez au besoin la section 2.4.1. Gi* tient compte à la fois des valeurs des entités voisines ou proches, mais aussi de celle de i. Contrairement à Gi, Gi* a un z-score. Parmi les quatre catégories de la typologie basée sur le nuage de points du I de Moran, lesquelles renvoient à de l’autocorrélation spatiale locale positive? Relisez au besoin la section 2.4.3. HH LL HL LH Le village gaulois correspond à quelle catégorie? Relisez au besoin la section 2.4.3. HH LL HL LH Quelles sont les trois manières de tester la significativité des mesures d’autocorrélation globales? Relisez au besoin la section 2.3.1.3. Avec l’hypothèse de la normalité En relaçant plusieurs fois les calculs Avec l’hypothèse de la randomisation Avec la méthode Monte-Carlo (habituellement avec 999 échantillons) Vérifier votre résultat "],["sect026.html", "2.6 Exercices de révision", " 2.6 Exercices de révision Exercice 2. Changeons de ville et visitons la région métropolitaine de Québec! Construisez les matrices de pondération spatiale suivantes : Matrice de pondération spatiale selon le partage d’un segment commun (voir la section 2.2.4.1). Matrice de pondération spatiale selon l’inverse de la distance au carré, à partir de la distance maximale et un SR et son voisin le plus proche (voir la section 2.2.4.4). Matrice de pondération spatiale selon le critère des plus proches voisins (k = 2) (voir la section 2.2.4.5). Complétez le code ci-dessous. library(sf) library(spdep) library(tmap) ## Importation de la couche des secteurs de recensement SRQc &lt;- st_read(dsn = &quot;data/chap02/exercice/RMRQuebecSR2021.shp&quot;, quiet=TRUE) ## Matrice selon le partage d&#39;un segment (Rook) Rook &lt;- À compléter W.Rook &lt;- À compléter ## Coordonnées des centroïdes des entités spatiales coords &lt;- st_coordinates(st_centroid(SRQc)) ## Matrice de l&#39;inverse de la distance réduite # Trouver le plus proche voisin avec la fonction knn2nb k1 &lt;- À compléter plusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords))) # Voisins les plus proches avec le seuil de distance maximal Voisins.DistMax &lt;- À compléter # Distances avec le seuil maximum distances &lt;- À compléter # Inverse de la distance au carré InvDistances2 &lt;- À compléter # Matrices de pondération spatiale standardisées en ligne W_InvDistances2 &lt;- À compléter ## Matrice des plus proches voisins avec k = 2 k2 &lt;- À compléter W.k2 &lt;- À compléter Correction à la section 10.2.2. Exercice 3. Calculez le I de Moran global pour la variable D1pct (pourcentage du premier décile de revenu des familles économiques) de la couche SRQc avec les différentes matrices de pondération spatiale (voir la section 2.3.2.2). Complétez le code ci-dessous. library(sf) library(spdep) library(tmap) ## Cartographie de la variable tm_shape(SRQc)+ tm_polygons(col=&quot;D1pct&quot;, title = &quot;Premier décile de revenu (%)&quot;, style=&quot;quantile&quot;, n=5, palette=&quot;Greens&quot;)+ tm_layout(frame = F)+tm_scale_bar(c(0,5,10)) ## I de Moran avec la méthode Monte-Carlo avec 999 permutations # utilisez la fonction moran.mc # avec la matrice W.Rook À compléter # avec la matrice W_InvDistances2Reduite À compléter # avec la matrice W.k2 À compléter Quelle matrice de pondération spatiale donne la dépendance spatiale la plus forte? Correction à la section 10.2.3. Exercice 4. Calculez et cartographiez les mesures d’autocorrélation spatiale locale pour la variable D1pct de la couche SRQc avec la matrice spatiale W.Rook : Mesure \\(G_i\\) de Getis et Ord (voir la section 2.4.1) Typologie du nuage de points du I de Moran avec la fonction localmoran (voir la section 2.4.3). Complétez le code ci-dessous. #################### ## Calcul du Z(Gi) #################### SRQc$D1pct_localGetis &lt;- localG(À compléter, À compléter, zero.policy=TRUE) # Définition des intervalles et des noms des classes classes.intervalles = À compléter classes.noms = c(&quot;Point froid (p = 0,001)&quot;, &quot;Point froid (p = 0,01)&quot;, &quot;Point froid (p = 0,05)&quot;, &quot;Non significatif&quot;, &quot;Point chaud (p = 0,05)&quot;, &quot;Point chaud (p = 0,01)&quot;, &quot;Point chaud (p = 0,001)&quot;) ## Création d&#39;un champ avec les noms des classes SRQc$D1pct_localGetisP &lt;- cut(SRQc$D1pct_localGetis, breaks = classes.intervalles, labels = classes.noms) ## Cartographie À compléter #################### ## Typologie LISA #################### ## Cote Z (variable centrée réduite) zx &lt;- À compléter ## variable X centrée réduite spatialement décalée avec une matrice Rook wzx &lt;- lag.listw(À compléter) ## I de Moran local (notez que vous pouvez aussi utiliser la fonction localmoran_perm) localMoranI &lt;- localmoran(À compléter) plocalMoranI &lt;- localMoranI[, 5] ## Choisir un seuil de signification signif = 0.05 ## Construction de la typologie Typologie &lt;- ifelse(zx &gt; 0 &amp; wzx &gt; 0, &quot;1. HH&quot;, NA) Typologie &lt;- ifelse(zx &lt; 0 &amp; wzx &lt; 0, &quot;2. LL&quot;, Typologie) Typologie &lt;- ifelse(zx &gt; 0 &amp; wzx &lt; 0, &quot;3. HL&quot;, Typologie) Typologie &lt;- ifelse(zx &lt; 0 &amp; wzx &gt; 0, &quot;4. LH&quot;, Typologie) Typologie &lt;- ifelse(plocalMoranI &gt; signif, &quot;Non sign&quot;, Typologie) # Non significatif ## Enregistrement de la typologie dans un champ SRQc$TypoIMoran.D1pct &lt;- Typologie ## Couleurs Couleurs &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;lightpink&quot;, &quot;skyblue2&quot;, &quot;lightgray&quot;) names(Couleurs) &lt;- c(&quot;1. HH&quot;,&quot;2. LL&quot;,&quot;3. HL&quot;,&quot;4. LH&quot;,&quot;Non sign&quot;) ## Cartographie tmap_mode(&quot;plot&quot;) À compléter Correction à la section 10.2.4. "],["chap03.html", "Chapitre 3 Méthodes de répartition ponctuelle", " Chapitre 3 Méthodes de répartition ponctuelle Dans ce chapitre, nous abordons les méthodes de répartition ponctuelle qui permettent de décrire un semis de points dans un espace géographique donné. En géomatique appliquée, ces méthodes sont largement utilisées dans le cadre d’études rattachées à des champs disciplinaires variés comme : En études urbaines, pour décrire la répartition de services et d’équipements collectifs à travers une ville afin de vérifier s’ils sont équitablement répartis ou à l’inverse, concentrés dans certaines parties de la ville. En biologie, pour décrire la répartition d’espèces fauniques ou végétales dans un territoire. En criminologie, pour analyser la répartition spatiale d’un ou de plusieurs types de crimes. En épidémiologie spatiale, pour comprendre l’évolution de la répartition des cas d’une maladie infectieuse. En transport, pour analyser la répartition d’accidents. Et même en sciences de l’activité physique, pour analyser la distribution spatiale de personnes pratiquant un sport sur un terrain de soccer, de rugby, de tennis, de baseball, etc. etc. Dans ce chapitre, nous utilisons les packages suivants : Pour importer et manipuler des fichiers géographiques : sf pour importer et manipuler des données vectorielles. Pour construire des cartes et des graphiques : tmap est certainement le meilleur package. ggplot2 est un package pour construire des graphiques. Pour les analyses de méthodes de répartition ponctuelle : spatstat est sans aucun doute le meilleur package. Pour décrire la distribution d’un semis de points, nous voyons les méthodes suivantes : la fréquence et la densité des points dans l’espace d’étude. l’analyse centrographique : paramètres de tendance centrale (centre moyen et point central). la dispersion du semis de points (distance standard) et ces différentes représentations graphiques (cercle et ellipse de distance standard). l’arrangement spatial du semis de points : méthode du plus proche voisins. méthode des quadrats. la cartographie de la densité : dans une maille irrégulière (des polygones de forme et de taille différentes). dans une maille régulière (densité par noyau – kernel density estimation, KDE). "],["sect031.html", "3.1 Fréquence et densité des points dans l’espace d’étude", " 3.1 Fréquence et densité des points dans l’espace d’étude La fréquence est tout simplement le nombre de points présents dans une région donnée (par exemple, le nombre d’hôpitaux, de stations de métro, d’arbres, etc.). Si les régions sont différentes, il convient de calculer la densité soit le ratio entre la fréquence et la superficie totale de la région donnée ou la population. Par exemple, le tableau 3.1 renvoie le nombre et la densité des stations de métro (pour 10000 habitants) pour trois villes. Interprétez ces chiffres avec prudence, car ils varient en fonction de la taille du territoire retenu pour les trois villes. Tableau 3.1: Fréquence et densité des stations de métro dans trois villes Ville Population Stations de métro Densité (stations / 10000 hab.) New York City 8 804 000 424 0,5 Paris 2 165 000 309 1,4 Île de Montréal 2 004 000 68 0,3 "],["sect032.html", "3.2 Analyse centrographique", " 3.2 Analyse centrographique L’analyse centrographique est une approche qui a été largement utilisée durant les décennies 1990 et 2000. Son utilisation est parfois critiquée pour deux raisons principales : 1) elle ne décrit que partiellement le semis de points; 2) aucun test d’inférence n’est calculé. Quoi qu’il en soit, elle permet d’explorer les données avant de se lancer dans des analyses plus avancées. Utilisation de l’analyse centrographique au Québec et dans le monde francophone Marius Thériault (géographe et professeur émérite à l’Université Laval) a largement contribué à la popularité de l’analyse centrographique au Québec et ailleurs. Il est le créateur de MapStat, un module MapBasic intégré dans le logiciel SIG MapInfo permettant de réaliser une analyse centrographique avant même qu’elle soit implémentée dans ArcGIS. En guise d’exemple, les études suivantes utilisent l’analyse centrographique calculée avec MapStat (López Castro, Thériault et Vandersmissen 2015; Barbonne, Villeneuve et Thériault 2007). Consultez-les au besoin. 3.2.1 Paramètres de tendance centrale d’un semis de points 3.2.1.1 Centre moyen Le centre moyen (\\(cm\\)) est le centre de gravité du semis de points et correspond aux valeurs des moyennes arithmétiques des coordonnées géographiques (équation (3.1)). \\[\\begin{equation} (\\bar{x}_{cm}, \\bar{y}_{cm}) = \\Biggl( \\frac{\\Sigma_{i=1}^n x_i}{n}, \\frac{\\Sigma_{i=1}^n y_i}{n}\\Biggl)\\text{ avec :} \\tag{3.1} \\end{equation}\\] \\((\\bar{x}_{cm}, \\bar{y}_{cm})\\), les coordonnées géographiques du point moyen. \\(n\\), le nombre de points dans la couche géographique. \\(x_i\\) et \\(y_j\\), les coordonnées géographiques du point \\(i\\). Il est possible de calculer le centre moyen en pondérant chacun des points du semis avec la valeur d’une variable donnée (équation (3.2)). Ainsi, l’importance accordée à chacun des points n’est pas la même. Par exemple, nous pourrions calculer le centre moyen pondéré (\\(cmp\\)) des cliniques médicales d’une ville en pondérant chaque clinique par le nombre de médecins, ou encore le point moyen des hôpitaux pondéré par le nombre de lits. Autre exemple, avec un jeu de données sur les arbres dans une érablière, nous pourrions utiliser une pondération basée sur le DHP (diamètre à la hauteur de la poitrine) afin d’accorder un poids plus important aux arbres de plus « grand volume ». Pour un rappel sur le calcul d’une moyenne pondérée, consultez la section intitulée Statistiques descriptives pondérées (Apparicio et Gelb 2022). \\[\\begin{equation} (\\bar{x}_{cmp}, \\bar{y}_{cmp}) = \\Biggl( \\frac{\\Sigma_{i=1}^n w_ix_i}{\\Sigma_{i=1}^nw_i}, \\frac{\\Sigma_{i=1}^n w_iy_i}{\\Sigma_{i=1}^nw_i}\\Biggl) \\text{ avec :} \\tag{3.2} \\end{equation}\\] \\(n\\), \\(x_i\\) et \\(y_j\\) étant définis plus haut. \\((\\bar{x}_w, \\bar{y}_w)\\), les coordonnées géographiques du point moyen pondéré. \\(w_i\\), la valeur de pondération associée au point \\(i\\). Le centre moyen et le centre moyen pondéré sont des mesures très utiles pour comparer la distribution de plusieurs semis de points (de différents services et équipements collectifs par exemple) ou encore pour décrire l’évolution dans le temps de la répartition d’un semis de points. L’analyse du déplacement du centre moyen (pondéré) à différentes dates nous informe ainsi de l’évolution du phénomène à l’étude, et plus spécifiquement, de son orientation, de sa direction. Exemple d’utilisation temporelle du centre moyen pondéré : le centre moyen de la population américaine Un exemple classique d’utilisation du centre moyen pondéré sur plusieurs années est l’évolution du centre moyen pondéré de la population des États-Unis de 1790 à 2020. Il est calculé à partir des centroïdes des comtés américains et de la population comme variable de pondération issus des recensements de l’US Census Bureau. Sans surprise, le centre moyen pondéré se déplace vers l’ouest. Vous pouvez consulter cette carte ou visionner cette courte vidéo YouTube ludique. Notez que les centres moyen et moyen pondéré peuvent aussi être calculés sur des données géographiques comprenant l’élévation (\\(x,y,z\\)) : \\[\\begin{equation} (\\bar{x}_{cm}, \\bar{y}_{cm}, \\bar{z}_{cm}) = \\Biggl( \\frac{\\Sigma_{i=1}^n x_i}{n}, \\frac{\\Sigma_{i=1}^n y_i}{n}, \\frac{w_i\\Sigma_{i=1}^n z_i}{n}\\Biggl) \\tag{3.3} \\end{equation}\\] \\[\\begin{equation} (\\bar{x}_{cmp}, \\bar{y}_{cmp}, \\bar{z}_{cmp}) = \\Biggl( \\frac{w_i\\Sigma_{i=1}^n x_i}{\\Sigma_{i=1}^nw_i}, \\frac{w_i\\Sigma_{i=1}^n y_i}{\\Sigma_{i=1}^nw_i}, \\frac{w_i\\Sigma_{i=1}^n z_i}{\\Sigma_{i=1}^nw_i}\\Biggl) \\tag{3.4} \\end{equation}\\] 3.2.1.2 Point central Le point central d’un semis de points (\\(pc\\)) est celui qui minimise la somme des distances le séparant de tous les autres points (équation (3.5)). Bien entendu, il est aussi possible de calculer le point central pondéré (\\(pcp\\)) (équation (3.6)). \\[\\begin{equation} \\text{Min}\\Biggl(\\Sigma_{i=1}^n \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\Biggl)\\text{; avec }i \\ne j \\tag{3.5} \\end{equation}\\] \\[\\begin{equation} \\text{Min}\\Biggl(\\Sigma_{i=1}^n w_i \\sqrt{(x_i-x_j)^2+(y_i-y_j)^2}\\Biggl)\\text{; avec }i \\ne j \\tag{3.6} \\end{equation}\\] Différence entre point central et centre moyen Contrairement au centre moyen, le point central fait partie du semis de points initial. Utilité du point central pondéré Imaginez que des personnes étudiantes en géographie et en géomatique de toutes les universités du Québec souhaitent organiser une rencontre en présence. Calculer le point central, pondéré par le nombre de personnes étudiantes par établissement participant à la rencontre, permet d’identifier l’université la plus centrale. Idéalement, il faudrait calculer ce point central pondéré avec la distance réticulaire. 3.2.2 Paramètres de dispersion d’un semis de points 3.2.2.1 Distance standard et distance standard pondérée En statistique univariée, l’écart-type (équation (3.7)) est une mesure de dispersion bien connue : plus la valeur de l’écart-type est élevée, plus la dispersion des valeurs de la variable autour de la moyenne (\\(\\mu\\)) est importante. \\[\\begin{equation} \\sigma=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\mu)^2}{n}} \\tag{3.7} \\end{equation}\\] Pour un rappel sur l’écart-type, consultez la section intitulée Paramètres de dispersion (Apparicio et Gelb 2022). Distance standard (pondérée ou non) des X et des Y De manière analogue à l’écart-type, nous pouvons calculer la distance standard pour les coordonnées X et pour les coordonnées Y des points, soit l’écart moyen respectif au centre moyen (équation (3.8)) ou au centre moyen pondéré (équation (3.9)). \\[\\begin{equation} \\sigma_x=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x}_{cm})^2}{n}} \\text{ et } \\sigma_y=\\sqrt{\\frac{\\sum_{i=1}^n (y_{i}-\\bar{y}_{cm})^2}{n}} \\tag{3.8} \\end{equation}\\] \\[\\begin{equation} \\sigma_{xw}=\\sqrt{\\frac{\\sum_{i=1}^n w_i(x_{i}-\\bar{x}_{cmp})^2}{\\sum_{i=1}^n{w_i}}} \\text{ et } \\sigma_{yw}=\\sqrt{\\frac{\\sum_{i=1}^n w_i(y_{i}-\\bar{y}_{cmp})^2}{\\sum_{i=1}^n{w_i}}} \\tag{3.9} \\end{equation}\\] Distance standard (pondérée ou non) Nous pouvons aussi calculer la distance standard (\\(ds\\)) sans ou avec pondération (équations (3.10) et (3.11)). Plus elle est forte, plus les points sont dispersés autour du centre moyen ou du centre moyen pondéré. Inversement, une faible distance standard traduit une concentration de points autour du centre moyen. \\[\\begin{equation} ds=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x}_{cm})^2}{n} + \\frac{\\sum_{i=1}^n (y_{i}-\\bar{y}_{cm})^2}{n}} \\tag{3.10} \\end{equation}\\] \\[\\begin{equation} ds_w=\\sqrt{\\frac{\\sum_{i=1}^n w_i(x_{i}-\\bar{x}_{cmp})^2}{\\sum_{i=1}^n w_i} + \\frac{\\sum_{i=1}^n w_i(y_{i}-\\bar{y}_{cmp})^2}{\\sum_{i=1}^n w_i}} \\tag{3.11} \\end{equation}\\] De nouveau, ces mesures peuvent être adaptées pour des points avec une élévation (\\(x,y,z\\)) : \\[\\begin{equation} ds=\\sqrt{\\frac{\\sum_{i=1}^n (x_{i}-\\bar{x}_{cm})^2}{n} + \\frac{\\sum_{i=1}^n (y_{i}-\\bar{y}_{cm})^2}{n} + \\frac{\\sum_{i=1}^n (z_{i}-\\bar{z}_{cm})^2}{n}} \\tag{3.12} \\end{equation}\\] \\[\\begin{equation} ds_w=\\sqrt{\\frac{\\sum_{i=1}^n w_i(x_{i}-\\bar{x}_{cmp})^2}{\\sum_{i=1}^n w_i} + \\frac{\\sum_{i=1}^n w_i(y_{i}-\\bar{y}_{cmp})^2}{\\sum_{i=1}^n w_i} + \\frac{\\sum_{i=1}^n w_i(z_{i}-\\bar{z}_{cmp})^2}{\\sum_{i=1}^n w_i}} \\tag{3.13} \\end{equation}\\] 3.2.2.2 Représenter la dispersion : cercle de distance standard et ellipse La dispersion d’un semis de points peut être représentée de quatre manières différentes : Une enveloppe convexe des points décrite à la section 1.2.2.5. Un rectangle centré sur le centre moyen avec les déviations des coordonnées X et Y pondérées ou non (équations (3.8) et (3.9) décrites précédemment). Dans le cas de données comprenant l’élévation, la forme géométrique est un parallélépipède rectangle. Un cercle de rayon de distance standard pondérée ou non (équations (3.10) et (3.11) décrites précédemment). Dans le cas de données comprenant l’élévation, la forme géométrique est une sphère de rayon de distance standard. Une ellipse de distance standard pondérée ou non. Dans le cas de données comprenant l’élévation, la forme géométrique est un ellipsoïde. Prenons un jeu de données fictives pour décrire ces quatre représentations graphiques. Imaginons que nous avons observé le parc de la Laurentie à Sherbrooke pour comprendre son utilisation. Pour collecter des données sur la localisation des personnes utilisatrices, nous aurions pu utiliser un questionnaire dans QFIELD ou ArcGIS Survey 123. La figure 3.1 illustre la localisation de dix personnes (points rouges). Figure 3.1: Données fictives sur des personnes utilisatrices du parc de la Laurentie à Sherbrooke À partir de ces dix points, nous obtenons : Les coordonnées du centre moyen qui sont égales à -8 007 869 et 5 685 921, soit simplement les moyennes des coordonnées X et Y des dix observations. L’enveloppe convexe des points qui contient tous les points. Le rectangle construit avec les déviations standards des X et des Y qui est centré sur le centre moyen. À partir de ce point, nous ajoutons à l’est et à l’ouest la valeur de la distance standard des X et au nord et au sud celle des Y. Les valeurs de ces distances standards sont égales à 32,60 et 52,20 mètres (voir les calculs au tableau 3.2). La distance standard est égale à 61,59 mètres (voir les calculs au tableau 3.2). À partir de cette distance, nous traçons le cercle ayant comme rayon la distance standard. L’ellipse de déviation de distance standard (figure 3.2). Tableau 3.2: Calcul des distances standards des X et des Y et de la distance standard Pt \\(x_i\\) \\(y_i\\) \\((x_{i}-\\bar{x}_{cm})^2\\) \\((y_{i}-\\bar{y}_{cm})^2\\) \\((x_{i}-\\bar{x}_{cm})^2+\\)\\((y_{i}-\\bar{y}_{cm})^2\\) 1 -8 007 877 5 686 000 52,80 6 347,70 6 400,50 2 -8 007 843 5 685 993 716,90 5 298,00 6 014,90 3 -8 007 836 5 685 976 1 082,00 3 046,30 4 128,30 4 -8 007 859 5 685 928 98,90 60,30 159,20 5 -8 007 911 5 685 935 1 770,20 214,60 1 984,80 6 -8 007 913 5 685 871 1 934,80 2 461,00 4 395,80 7 -8 007 888 5 685 855 365,70 4 363,40 4 729,10 8 -8 007 835 5 685 857 1 185,00 4 016,80 5 201,80 9 -8 007 824 5 685 886 2 071,70 1 203,50 3 275,20 10 -8 007 906 5 685 904 1 376,50 266,70 1 643,30 \\(n =\\) 10 \\(\\text{Moyenne} =\\) -8 007 869 5 685 921 1 065,45 2 727,83 3 793,28 \\(\\text{Racine carrée} =\\) 32,60 52,20 61,59 Figure 3.2: Trois éléments composant une ellipse Calcul des ellipses : des résultats qui varient d’un logiciel à l’autre… Étonnamment, il existe plusieurs solutions pour tracer une ellipse. Pour une discussion détaillée de ces différentes solutions, lisez le court texte très intéressant de Martin Leroux. Ellipse de Yuill (Tveite 2020). Ellipse basée sur la covariance implémentée dans ArcGIS Pro. Ellipse de distance standard implémentée dans le logiciel CrimeStat. Ellipse avec la correction proposée par Wang et ses collègues (2015). Il est à noter qu’un plugin QGIS intitulé The QGIS Standard Deviational Ellipse Plugin (Tveite 2020) intègre plusieurs de ces méthodes. Les résultats varient ainsi d’un logiciel à l’autre dépendamment de la méthode implémentée. Autrement dit, pour un même jeu de données ponctuelles, l’ellipse n’est pas la même dans ArcGIS Pro, ArcMap 9.3, ArcMap 10.x et QGIS. Quoi faire alors? Quelle que soit la méthode utilisée, l’ellipse est toujours centrée sur le point moyen et a toujours le même angle de rotation. Par contre, la taille de l’ellipse (superficie) varie. Par conséquent, si vous souhaitez comparer des ellipses différentes, assurez-vous toujours qu’elles sont toutes obtenues dans le même logiciel et avec la même solution. Calcul de l’ellipse selon la méthode implémentée dans CrimeStat Ned Levine (2006; 2021) ayant développé le logiciel CrimeStat propose les formulations suivantes pour le calcul d’une ellipse : \\[\\begin{equation} \\theta = \\frac{\\text{arctan} \\Biggl\\{ \\Bigl(\\sum_{i=1}^nx_d^2-\\sum_{i=1}^ny_d^2 \\Bigl)+ \\Bigr[\\Bigl(\\sum_{i=1}^nx_d^2-\\sum_{i=1}^ny_d^2 \\Bigl)^2 + 4 \\Bigl(\\sum_{i=1}^nx_dy_d \\Bigl)^2\\Bigr]^{1/2} \\Biggl\\}} {2 \\sum_{i=1}^nx_dy_d} \\tag{3.14} \\end{equation}\\] avec \\(\\theta\\) est la rotation de l’ellipse, \\(x_d = x_i-\\bar{x}\\) et \\(y_d = y_i-\\bar{y}\\). \\[\\begin{equation} \\sigma_x =\\sqrt{2\\times \\frac{\\sum_{i=1}^n\\Bigl((x_i-\\bar{x}) \\text{cos}\\theta-(y_i-\\bar{y})\\text{sin}\\theta\\Bigl)^2}{n-2}} \\tag{3.15} \\end{equation}\\] \\[\\begin{equation} \\sigma_y =\\sqrt{2\\times \\frac{\\sum_{i=1}^n\\Bigl((x_i-\\bar{x}) \\text{sin}\\theta-(y_i-\\bar{y})\\text{cos}\\theta\\Bigl)^2}{n-2}} \\tag{3.16} \\end{equation}\\] \\[\\begin{equation} l_x=2\\sigma_x \\text{ et }l_y=2\\sigma_y \\text{ et }S_e=\\pi\\sigma_x\\sigma_y \\tag{3.17} \\end{equation}\\] avec \\(l_x\\), \\(l_y\\) et \\(S_e\\) étant les longueurs de axes X et Y et la superficie de l’ellipse. Prenons quatre situations fictives de répartition de dix personnes utilisatrices du parc de la Laurentie à Sherbrooke : Situation A. Les observations sont concentrées autour de l’aire de jeu. Situation B. Les observations sont dispersées dans la partie est du parc. Situation C. Les observations sont concentrées dans la partie nord du parc. Situation D. Les observations sont concentrées dans la partie nord du parc, excepté deux observations au sud. Figure 3.3: Données fictives sur des personnes utilisatrices du parc de la Laurentie à Sherbrooke (quatre situations) Les cercles et les ellipses de distance standard (\\(ds\\)) centrés au centre moyen (\\(cm\\)) sont représentés à la (figure 3.4). Figure 3.4: Ellipse et cercle de distance standard pour les quatre situations 3.2.2.3 Comparaison de la dispersion de deux semis de points dans deux régions différentes Pour comparer la dispersion de deux semis de points situés dans des régions de taille différente, il convient de supprimer les effets de taille des deux régions. Pour ce faire, nous divisons la distance standard ou la distance standard pondérée par la superficie de la région. Cette approche est donc très similaire au coefficient de variation en statistique univariée, soit le rapport entre l’écart-type et la moyenne. Par exemple, si nous comparons les dispersions des personnes utilisatrices du parc de la Laurentie (0,078 ha) et parc du Mont-Bellevue (409 ha). Inévitablement, la valeur de la distance standard est plus forte pour le parc du Mont-Bellevue que celle du parc de la Laurentie. Il faut donc diviser chaque distance standard par la superficie associée. 3.2.2.4 Comparaison de la dispersion de deux semis de points dans la même région Pour comparer la distribution spatiale de deux semis de points situés dans la même région, nous comparons leur cercle ou leur ellipse respective (par exemple, des points représentant des accidents l’été versus l’hiver ou encore deux espèces végétales sur le même territoire). Une démarche similaire peut être appliquée à deux groupes de population rattachés à des entités polygonales : ils ont la même distribution spatiale si les deux ellipses de distance pondérée se juxtaposent significativement. David W.S. Wong (1999) propose d’ailleurs un indice dénommé S basé sur la comparaison des deux ellipses (équation (3.18)). Le numérateur représente la surface d’intersection entre les deux ellipses tandis que le dénominateur représente leur surface d’union. L’indice varie de 0 à 1, soit respectivement d’une similitude parfaite à une dissemblance la plus grande entre les deux distributions spatiales : Si deux groupes de population ont des distributions spatiales identiques, les ellipses sont les mêmes et donc \\(E_i \\cap E_j = E_i \\cup E_j = 1\\) et \\(S_{ij} = 1 - 1 = 0\\). Si les deux groupes de population ont des distributions spatiales totalement différentes, les ellipses ne se touchent pas, alors \\(E_i \\cap E_j = 0\\) et la valeur de \\(S = 1 - 0 = 1\\). Bien évidemment, cet indice peut être étendu pour comparer les distributions de plus de deux groupes de population simultanément (équation (3.19)). \\[\\begin{equation} S_{ij}=1-\\frac{E_i \\cap E_j}{E_i \\cup E_j} \\tag{3.18} \\end{equation}\\] \\[\\begin{equation} S=1-\\frac{E_1 \\cap E_2 \\cap E_3 \\cap \\text{…} \\cap E_n}{E_1 \\cup E_2 \\cup E_3 \\cup \\text{…} \\cup E_n} \\tag{3.19} \\end{equation}\\] Voyons un exemple concret : calculons l’indice de Wong (1999) (équation (3.18)) pour les ellipses de distances standards pondérées par les effectifs de propriétaires et de locataires par secteur de recensement pour la ville de Sherbrooke en 2021 (figure 3.5). D’emblée, nous constatons que les propriétaires ont une distribution plus dispersée que celle des locataires beaucoup plus présents dans le centre de la ville. Figure 3.5: Propriétaires et locataires dans la ville de Sherbrooke (avec ellipse de distance standard), 2021 Quelques lignes de code suffisent pour obtenir l’indice de Wong (1999). ## Importation des deux ellipses E1 &lt;- st_read(dsn = &quot;data/chap03/EllipseProprio.shp&quot;, quiet=TRUE) E2 &lt;- st_read(dsn = &quot;data/chap03/EllipseLocataire.shp&quot;, quiet=TRUE) ## Intersection Inter &lt;- st_intersection(E1, E2) ## Union Union &lt;- st_union(E1, E2) ## Calcul de l&#39;indice de Wong Wong &lt;- 1 - ( as.numeric(st_area(Inter)) / as.numeric(st_area(Union))) print(paste0(&quot;Valeur de l&#39;indice de Wong : &quot;,round(Wong,3))) ## [1] &quot;Valeur de l&#39;indice de Wong : 0.515&quot; 3.2.3 Mise en œuvre de l’analyse centrographique dans R 3.2.3.1 Calcul de mesures non pondérées Pour illustrer la mise en œuvre des différentes mesures de l’analyse centrographique dans R, nous utilisons un jeu de données ouvertes sur les incidents de sécurité publique de la ville de Sherbrooke. Dans la syntaxe ci-dessous, nous importons les données et constituons une couche sf par année (2019 à 2022) et extrayons les coordonnées géographiques. library(sf) library(tmap) ## Importation des données Arrondissements &lt;- st_read(dsn = &quot;data/chap03/Arrondissements.shp&quot;, quiet=TRUE) Incidents &lt;- st_read(dsn = &quot;data/chap03/IncidentsSecuritePublique.shp&quot;, quiet=TRUE) ## Changement de projection Arrondissements &lt;- st_transform(Arrondissements, crs = 3798) Incidents &lt;- st_transform(Incidents, crs = 3798) ## Extaction des méfaits Mefaits &lt;- subset(Incidents, DESCRIPTIO == &quot;Méfait&quot;) # Méfaits par année M2019 &lt;- subset(Mefaits, ANNEE==2019) M2020 &lt;- subset(Mefaits, ANNEE==2020) M2021 &lt;- subset(Mefaits, ANNEE==2021) M2022 &lt;- subset(Mefaits, ANNEE==2022) # Coordonnées géographiques xy.2019 &lt;- st_coordinates(M2019) xy.2020 &lt;- st_coordinates(M2020) xy.2021 &lt;- st_coordinates(M2021) xy.2022 &lt;- st_coordinates(M2022) Les méfaits par année sont présentés à la figure 3.6. Figure 3.6: Localisation des méfaits par année, ville de Sherbrooke, 2021 Centre moyen Avec la fonction mean, nous pouvons calculer les valeurs moyennes sur les coordonnées X et Y, puis créer un objet sf avec les centres moyens. ## Récupération de la projection cartographique dans une variable ProjCarto = st_crs(Mefaits) ## Calcul du centre moyen pour une année (2019) print(c(mean(xy.2019[,1]), mean(xy.2019[,2]))) ## [1] 649990.3 157059.3 ## Calcul pour toutes les années # vecteur pour les moyennes des X X.moy &lt;- c(mean(xy.2019[,1]), mean(xy.2020[,1]), mean(xy.2021[,1]), mean(xy.2022[,1])) # Vecteur pour les moyennes des Y Y.moy &lt;- c(mean(xy.2019[,2]), mean(xy.2020[,2]), mean(xy.2021[,2]), mean(xy.2022[,2])) # Enregistrement dans un objet sf CentreMoyen &lt;- data.frame(Annee = c(&quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;, &quot;2022&quot;), X = X.moy, Y = Y.moy, CMx = X.moy, CMy = Y.moy) CentreMoyen &lt;- st_as_sf(CentreMoyen, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = ProjCarto) # Affichage des résultats print(CentreMoyen) ## Simple feature collection with 4 features and 3 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 649696 ymin: 157059.3 xmax: 650663.7 ymax: 157544.8 ## Projected CRS: NAD83 / MTQ Lambert ## Annee CMx CMy geometry ## 1 2019 649990.3 157059.3 POINT (649990.3 157059.3) ## 2 2020 649696.0 157544.8 POINT (649696 157544.8) ## 3 2021 650663.7 157214.8 POINT (650663.7 157214.8) ## 4 2022 650442.5 157237.0 POINT (650442.5 157237) Point central La syntaxe ci-dessous illustre comment identifier le point central, qui fait partie du jeu de données, pour l’année 2019. ## Calcul de la matrice de distances entre les points de l&#39;année 2019 DistMatrice2019 &lt;- as.matrix(dist(xy.2019, method = &quot;euclidean&quot;, diag = TRUE, upper = TRUE)) ## Somme de chaque ligne de la matrice, soit la somme des distances à tous les autres points M2019$DistATous &lt;- rowSums(DistMatrice2019) ## Sélection du point avec plus petite distance à tous les autres PointCentral2019 &lt;- subset(M2019, M2019$DistATous==min(M2019$DistATous)) Distance standard sur les coordonnées X et Y et distance standard combinée La syntaxe permet de calculer les trois distances standards. ## Calcul de la distance standard pour une année (2019) # Distance standard sur les coordonnées X et Y c(sqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))^2)), sqrt(mean((xy.2019[,2] - mean(xy.2019[,2]))^2))) ## [1] 3732.173 2593.207 # Distance standard sqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))**2 + (xy.2019[,2] - mean(xy.2019[,2]))**2)) ## [1] 4544.649 ## Calcul pour toutes les années et enregistrement des centres moyens dans de nouveaux champs CentreMoyen$DS.X &lt;- c(sqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))^2)), sqrt(mean((xy.2020[,1] - mean(xy.2020[,1]))^2)), sqrt(mean((xy.2021[,1] - mean(xy.2021[,1]))^2)), sqrt(mean((xy.2022[,1] - mean(xy.2022[,1]))^2))) CentreMoyen$DS.Y &lt;- c(sqrt(mean((xy.2019[,2] - mean(xy.2019[,2]))^2)), sqrt(mean((xy.2020[,2] - mean(xy.2020[,2]))^2)), sqrt(mean((xy.2021[,2] - mean(xy.2021[,2]))^2)), sqrt(mean((xy.2022[,2] - mean(xy.2022[,2]))^2))) CentreMoyen$DS &lt;- c(sqrt(mean((xy.2019[,1] - mean(xy.2019[,1]))**2 + (xy.2019[,2] - mean(xy.2019[,2]))**2)), sqrt(mean((xy.2020[,1] - mean(xy.2020[,1]))**2 + (xy.2020[,2] - mean(xy.2020[,2]))**2)),+ sqrt(mean((xy.2021[,1] - mean(xy.2021[,1]))**2 + (xy.2021[,2] - mean(xy.2021[,2]))**2)),+ sqrt(mean((xy.2022[,1] - mean(xy.2022[,1]))**2 + (xy.2022[,2] - mean(xy.2022[,2]))**2))) ## Visualisation des résultats head(CentreMoyen) ## Simple feature collection with 4 features and 6 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 649696 ymin: 157059.3 xmax: 650663.7 ymax: 157544.8 ## Projected CRS: NAD83 / MTQ Lambert ## Annee CMx CMy geometry DS.X DS.Y DS ## 1 2019 649990.3 157059.3 POINT (649990.3 157059.3) 3732.173 2593.207 4544.649 ## 2 2020 649696.0 157544.8 POINT (649696 157544.8) 4134.913 2393.048 4777.466 ## 3 2021 650663.7 157214.8 POINT (650663.7 157214.8) 3374.821 2294.171 4080.764 ## 4 2022 650442.5 157237.0 POINT (650442.5 157237) 3601.208 2170.215 4204.585 Représentations graphiques de la dispersion Une fois que la couche sf des centres moyens avec les trois champs pour la distance standard est créée, il suffit de tracer un rectangle et un cercle de distance standard. ## Enveloppe convexe sf.Enveloppes &lt;- st_sf(data.frame(Id=c(&quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;, &quot;2022&quot;)), geometry = c(st_convex_hull(st_union(M2019)), st_convex_hull(st_union(M2020)), st_convex_hull(st_union(M2021)), st_convex_hull(st_union(M2022)))) ## Rectangle avec les distances standards sur les coordonnées X et Y #&#39; Fonction pour tracer le rectangle #&#39; @param MoyX coordonnées X du centre moyen. #&#39; @param MoyY coordonnées Y du centre moyen. #&#39; @param SDx distance standard sur les coordonnées X. #&#39; @param SDy distance standard sur les coordonnées Y. #&#39; @param crs projection cartographique. CreationRec &lt;- function(MoyX, MoyY, SDx, SDy, ProjCarto){ pt1 = c(MoyX - SDx, MoyY - SDy) pt2 = c(MoyX - SDx, MoyY + SDy) pt3 = c(MoyX + SDx, MoyY + SDy) pt4 = c(MoyX + SDx, MoyY - SDy) Rectangle = st_polygon(list(rbind(pt1, pt2, pt3, pt4, pt1))) Rectangle = st_sfc(Rectangle) st_crs(Rectangle) = ProjCarto return(Rectangle) } MoyX &lt;- CentreMoyen$CMx MoyY &lt;- CentreMoyen$CMy SDx &lt;- CentreMoyen$DS.X SDy &lt;- CentreMoyen$DS.Y sf.Rectangles &lt;- st_sf(data.frame(Id=c(&quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;, &quot;2022&quot;)), geometry = c(CreationRec(MoyX[1], MoyY[1], SDx[1], SDy[1], ProjCarto), CreationRec(MoyX[2], MoyY[2], SDx[2], SDy[2], ProjCarto), CreationRec(MoyX[3], MoyY[3], SDx[3], SDy[3], ProjCarto), CreationRec(MoyX[4], MoyY[4], SDx[4], SDy[4], ProjCarto))) ## Cercle de distance standard avec la fonction st_buffer sf.CercleDS &lt;- st_buffer(CentreMoyen, dist = CentreMoyen$DS) Le calcul de l’ellipse est un peu plus complexe. Par conséquent, nous avons créer une fonction CreateEllipse qui renvoie un objets sf. #&#39; Calcul de l&#39;ellipse #&#39; @param id identifiant de l&#39;ellipse. #&#39; @param x coordonnées X. #&#39; @param y coordonnées Y. #&#39; @param crs projection cartographique. #&#39; CreateEllipse &lt;- function(id=1, x, y, crs){ # coordonnées du centre moyen mean.x &lt;- mean(x) mean.y &lt;- mean(y) # coordonnées centrées zx &lt;- x - mean(x) zy &lt;- y - mean(y) sumzx2 &lt;- sum(zx^2) sumzy2 &lt;- sum(zy^2) sumzxy &lt;- sum(zx * zy) # Calcul de l&#39;angle de rotation numerateur &lt;- (sumzx2-sumzy2)+ sqrt((sumzx2-sumzy2)^2+ 4*(sumzxy)^2) demoninateur &lt;- 2*sumzxy tantheta &lt;- numerateur / demoninateur # Theta theta &lt;- ifelse(tantheta &lt; 0, 180 + atan(tantheta)* 180/pi, Theta &lt;- atan(tantheta)* 180/pi) sintheta &lt;- sin(theta * pi/180) costheta &lt;- cos(theta * pi/180) sin2theta &lt;- sintheta^2 cos2theta &lt;- costheta^2 sinthetacostheta &lt;- sintheta * costheta # Calcul de Sigma.x et Sigma.y n &lt;- length(zx) sigmax &lt;- sqrt(2) * sqrt(((sumzx2) * (cos2theta) - 2 * (sumzxy) * (sinthetacostheta) + (sumzy2) * (sin2theta))/(n - 2)) sigmay &lt;- sqrt(2) * sqrt(((sumzx2) * (sin2theta) + 2 * (sumzxy) * (sinthetacostheta) + (sumzy2) * (cos2theta))/(n - 2)) # Theta corrigé if (sigmax &gt; sigmay) { Theta.Corr &lt;- theta + 90 Major &lt;- &quot;SigmaX&quot; Minor &lt;- &quot;SigmaY&quot; } else { Theta.Corr &lt;- theta Major &lt;- &quot;SigmaY&quot; Minor &lt;- &quot;SigmaX&quot; } # Coordonnées B &lt;- min(sigmax, sigmay) A &lt;- max(sigmax, sigmay) d2 &lt;- (A - B) * (A + B) phi &lt;- 2 * pi * seq(0, 1, len = 360) sp &lt;- sin(phi) cp &lt;- cos(phi) r &lt;- sigmax * sigmay/sqrt(B^2 + d2 * sp^2) xy &lt;- r * cbind(cp, sp) al &lt;- (90 - theta) * pi/180 ca &lt;- cos(al) sa &lt;- sin(al) coordsSDE &lt;- xy %*% rbind(c(ca, sa), c(-sa, ca)) + cbind(rep(mean.x, 360), rep(mean.y, 360)) # Polygone EllPoly &lt;- st_polygon(list(as.matrix(coordsSDE))) ellipse &lt;- st_sfc(EllPoly, crs=crs) ellipse &lt;- st_sf(data.frame(Id=id, CMx = mean.x, CMy = mean.y, Sigmax = sigmax, Sigmay = sigmay, Lx = 2*sigmax, Ly = 2*sigmay, Aire = pi*sigmax*sigmay, Theta = theta, ThetaCorr =Theta.Corr, Major = Major, Minor = Minor), geometry = ellipse) return(ellipse) } El2019 &lt;- CreateEllipse(id=&quot;2019&quot;, x = xy.2019[,1], y = xy.2019[,2], crs = ProjCarto) El2020 &lt;- CreateEllipse(id=&quot;2020&quot;, x = xy.2020[,1], y = xy.2020[,2], crs = ProjCarto) El2021 &lt;- CreateEllipse(id=&quot;2021&quot;, x = xy.2021[,1], y = xy.2021[,2], crs = ProjCarto) El2022 &lt;- CreateEllipse(id=&quot;2022&quot;, x = xy.2022[,1], y = xy.2022[,2], crs = ProjCarto) sf.Ellipse &lt;- rbind(El2019, El2020, El2021, El2022) head(sf.Ellipse) ## Simple feature collection with 4 features and 12 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 643833.8 ymin: 153377.3 xmax: 655559 ymax: 160937.9 ## Projected CRS: NAD83 / MTQ Lambert ## Id CMx CMy Sigmax Sigmay Lx Ly Aire Theta ## 1 2019 649990.3 157059.3 3055.157 5683.790 6110.315 11367.580 54553357 64.60944 ## 2 2020 649696.0 157544.8 3154.712 5994.647 6309.423 11989.294 59411858 75.81068 ## 3 2021 650663.7 157214.8 3128.891 4869.299 6257.783 9738.598 47863757 76.14675 ## 4 2022 650442.5 157237.0 2540.594 5406.184 5081.187 10812.368 43149511 68.52718 ## ThetaCorr Major Minor geometry ## 1 64.60944 SigmaY SigmaX POLYGON ((655125.1 159496.4... ## 2 75.81068 SigmaY SigmaX POLYGON ((655507.7 159014.3... ## 3 76.14675 SigmaY SigmaX POLYGON ((655391.4 158380.7... ## 4 68.52718 SigmaY SigmaX POLYGON ((655473.5 159216, ... Les quatre représentations de la dispersion sont présentées à la figure 3.7 : 1) enveloppe convexe (gris), 2) cercle de distance standard (bleu), 3) rectangle de distance standard sur les X et Y (noir) et 4) ellipse de distance standard (rouge). Figure 3.7: Représentations de la dispersion des méfaits pour les quatre années 3.2.3.2 Calcul de mesures pondérées Pour illustrer le calcul des mesures pondérées, nous utilisons des données sur les effectifs du premier et du dernier déciles de revenu après impôt des familles économiques pour les secteurs de recensement de la ville de Sherbrooke en 2021 (figure 3.8). Figure 3.8: Déciles extrêmes de revenu après impôt des familles économiques La syntaxe suivante permet d’importer et de structurer les données. puis de calculer les différentes mesures (centre moyen pondéré et distance standard pondérée). ## Importation et structuration des données dfdecile &lt;- read.csv(&quot;data/chap03/DataDecilesSR.csv&quot;, header = TRUE, sep = &quot;,&quot;) dfdecile$SRIDU &lt;- substr(dfdecile$SRIDU, 1, 10) SR &lt;- st_read(dsn = &quot;data/chap03/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DR_SherbSRDonnees2021&quot;, quiet=TRUE) SR &lt;- merge(SR[,c(&quot;SRIDU&quot;)], dfdecile, by = &quot;SRIDU&quot;) ProjCarto &lt;- st_crs(SR) ## Coordonnées géographiques des secteurs de recensement xy &lt;- st_coordinates(st_point_on_surface(SR)) ## Pondérations pour les deux déciles wd1 &lt;- SR$D1 wd10 &lt;- SR$D10 ## Sommes des pondérations swd1 &lt;- sum(wd1) swd10 &lt;- sum(wd10) ## Calcul du centre pondéré XmoyD1 &lt;- sum(xy[,1]*wd1) / swd1 XmoyD10 &lt;- sum(xy[,1]*wd10) / swd10 YmoyD1 &lt;- sum(xy[,2]*wd1) / swd1 YmoyD10 &lt;- sum(xy[,2]*wd10) / swd10 CentreMoyenPond &lt;- data.frame(Decile = c(&quot;Premier&quot;, &quot;Dernier&quot;), X = c(XmoyD1, XmoyD10), Y = c(YmoyD1, YmoyD10), CMwx = c(XmoyD1, XmoyD10), CMwy = c(YmoyD1, YmoyD10)) CentreMoyenPond &lt;- st_as_sf(CentreMoyenPond, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = ProjCarto) ## Calcul de la distance standard pondérée sdD1 &lt;- sqrt((sum(wd1*(xy[,1]- XmoyD1)^2) / swd1) + (sum(wd1*(xy[,2]- YmoyD1)^2) / swd1)) sdD10 &lt;- sqrt((sum(wd10*(xy[,1]- XmoyD10)^2) / swd1) + (sum(wd10*(xy[,2]- YmoyD10)^2) / swd10)) ## Zones tampons CentreMoyenPond$SDw &lt;- c(sdD1, sdD10) sf.CercleDSW &lt;- st_buffer(CentreMoyenPond, dist = CentreMoyenPond$SDw) ## Cartographie tmap_mode(&quot;plot&quot;) tm_shape(Arrondissements)+tm_borders(lwd = 1, col=&quot;black&quot;)+tm_fill(col=&quot;wheat&quot;)+ tm_shape(CentreMoyenPond) + tm_dots(size = 1, col=&quot;gray&quot;)+ tm_shape(sf.CercleDSW) + tm_polygons(lwd = 1, col=&quot;Decile&quot;, border.col=&quot;black&quot;, alpha=.3, title=&quot;Décile&quot;)+ tm_layout(frame=FALSE, title = &quot;2019&quot;) Figure 3.9: Déciles extrêmes de revenu après impôt des familles économiques References "],["sect033.html", "3.3 Forme d’un semis de points", " 3.3 Forme d’un semis de points Étudier la forme d’un semis de points, c’est vouloir décrire l’arrangement spatial et l’espacement des points dans une région donnée. Autrement dit, l’objectif est de répondre à la question suivante : comment se répartissent les points dans une région donnée? Nous distinguons habituellement trois types de distribution spatiale d’un semis de points figure 3.10 : Distribution dispersée quand les points du semis sont régulièrement espacés. Distribution aléatoire quand la distribution des points n’est nullement guidée par des considérations géographiques. Autrement dit, chaque point du semis a la même probabilité d’être situé dans n’importe quelle partie de la zone d’étude. Distribution concentrée quand il existe des regroupements des points dans une ou plusieurs parties de la région d’étude. Par exemple, les musées et les théâtres sont habituellement concentrés dans les parties centrales des métropoles. Figure 3.10: Trois types de distribution spatiale d’un semis de points Il existe deux grandes familles pour décrire la forme d’un semis de points : Celles basées sur la distance (l’indice du plus proche voisin, fonctions K et L de Ripley) (section 3.3.1). Celles basées sur la densité (méthode des quadrats avec différents tests statistiques) (section 3.3.2). 3.3.1 Méthode du plus proche voisin Le principe de base de cette méthode est fort simple et se décompose en quatre étapes : Mesurer, pour chaque point du semis, la distance le séparant du point le plus proche, puis calculer la distance moyenne du point le plus proche (équation (3.20)). Calculer la moyenne attendue du point le plus proche pour une dispersion aléatoire (équation (3.22)). Calculer l’indice du plus proche voisin, soit le ratio entre la distance observée et la distance aléatoire (équation (3.21)). L’indice R s’interprète alors comme suit : Si R est égal à 1, la dispersion du semis de points est aléatoire. Si R est inférieur à 1, la distribution du semis de points tend vers la concentration (avec une concentration absolue quand R = 0; tous les points ont les mêmes coordonnées géographiques). Si R est supérieur à 1, la distribution du semis de points tend vers la dispersion. Calculer les valeurs de Z et de p pour déterminer si la valeur de R obtenue est significative (équation (3.23)). \\[\\begin{equation} R_{o}= \\frac{\\sum_{i=1}^n d_i}{n} \\tag{3.20} \\end{equation}\\] \\[\\begin{equation} R_{a}= \\frac{1}{2 \\sqrt{(n/S)}} \\tag{3.21} \\end{equation}\\] \\[\\begin{equation} R = \\frac{R_{o}}{R_{a}} \\tag{3.22} \\end{equation}\\] \\[\\begin{equation} Z = \\frac{R_o-R_a}{SE} \\text{, } SE = \\frac{0.26136}{\\sqrt{(n^2/S)}} \\text{ avec :} \\tag{3.23} \\end{equation}\\] \\(n\\), nombre de points. \\(d_i\\), distance séparant le point i de son voisin le plus proche. \\(S\\), Superficie de l’espace d’étude. Le code ci-dessous permet de mettre en œuvre la méthode du plus proche voisin pour les méfaits pour les quatre années. library(spatstat) library(ggplot2) ## Indice du plus proche voisin : R observé (équation 3.20) # le paramètre k indique le nombre de plus proches voisins Robs2019 &lt;- mean(nndist(st_coordinates(M2019),k=1)) Robs2020 &lt;- mean(nndist(st_coordinates(M2020),k=1)) Robs2021 &lt;- mean(nndist(st_coordinates(M2021),k=1)) Robs2022 &lt;- mean(nndist(st_coordinates(M2022),k=1)) ## Indice du plus proche voisin : R attendu (distribution aléatoire) (équation 3.21) # Attention, il faut spéficier S, la superficie de l&#39;espace d&#39;étude Arrondissements &lt;- st_read(dsn = &quot;data/chap03/Arrondissements.shp&quot;, quiet=TRUE) Arrondissements &lt;- st_transform(Arrondissements, crs = 3798) S &lt;- as.numeric(st_area(st_union(Arrondissements))) # Nombre de points par année N2019 &lt;- nrow(M2019) N2020 &lt;- nrow(M2020) N2021 &lt;- nrow(M2021) N2022 &lt;- nrow(M2022) # Calcul de Ra Ra2019 &lt;- 1 / (2 * sqrt(N2019 / S)) Ra2020 &lt;- 1 / (2 * sqrt(N2020 / S)) Ra2021 &lt;- 1 / (2 * sqrt(N2021 / S)) Ra2022 &lt;- 1 / (2 * sqrt(N2022 / S)) ## Calculons le R # Création d&#39;un DataFrame IndicePPV &lt;- data.frame(id = c(&quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;, &quot;2022&quot;), points = c(N2019, N2020, N2021, N2022), Superficie = c(S, S, S, S), Robs = c(Robs2019, Robs2020, Robs2021, Robs2022), Rattendu = c(Ra2019, Ra2020, Ra2021, Ra2022)) # Calcul du R (équation 3.22) IndicePPV$R &lt;- IndicePPV$Robs / IndicePPV$Rattendu # Calcul du Z (équation 3.23) IndicePPV$SE &lt;- 0.26136 / sqrt(IndicePPV$points^2 / IndicePPV$Superficie) IndicePPV$Z &lt;- (IndicePPV$Robs - IndicePPV$Rattendu) / IndicePPV$SE IndicePPV$P &lt;- round(2*pnorm(q=abs(IndicePPV$Z), lower.tail=FALSE),3) print(IndicePPV) ## id points Superficie Robs Rattendu R SE Z P ## 1 2019 251 366358191 251.5498 604.0684 0.4164260 19.93051 -17.68739 0 ## 2 2020 383 366358191 183.9866 489.0166 0.3762380 13.06151 -23.35335 0 ## 3 2021 344 366358191 227.1881 515.9929 0.4402931 14.54232 -19.85961 0 ## 4 2022 220 366358191 251.3347 645.2256 0.3895299 22.73890 -17.32234 0 Interprétation des résultats Analysons les différentes colonnes du tableau 3.3 : points (n) : il y a respectivement 251, 383, 344 et 220 méfaits pour les années 2019 à 2022. R observé : en moyenne, un méfait est distant de 252, 184, 227 et 251 mètres du méfait le plus proche pour les quatre années. R attendu : pour une distribution aléatoire, un méfait devrait être distant du méfait le plus proche de 604, 489, 516, et 645 mètres. Indice du plus proche voisin : toutes les valeurs sont inférieures à 1, indiquant des distributions spatiales concentrées. La concentration est la plus forte pour l’année 2020 (R = 0,376). valeur de p : toutes les valeurs sont égales à 0, signalant que les résultats sont significatifs. Tableau 3.3: Résultats de la méthode du plus proche voisin pour les méfait par année Année points (n) R observé R attendu Indice plus proche voisin Erreur standard Z p 2019 251 252 604 0,416 19,931 -17,687 0 2020 383 184 489 0,376 13,062 -23,353 0 2021 344 227 516 0,440 14,542 -19,860 0 2022 220 251 645 0,390 22,739 -17,322 0 Notez qu’il est possible aussi de construire un graphique pour le R observé avec plusieurs voisins, tel que réalisé avec la syntaxe ci-dessous avec k = 1 à 50 (figure 3.11). # k = 1 à 50 Robs2019N1_50 &lt;- apply(nndist(st_coordinates(M2019), k=1:50), 2, FUN=mean) Robs2020N1_50 &lt;- apply(nndist(st_coordinates(M2020), k=1:50), 2, FUN=mean) Robs2021N1_50 &lt;- apply(nndist(st_coordinates(M2021), k=1:50), 2, FUN=mean) Robs2022N1_50 &lt;- apply(nndist(st_coordinates(M2022), k=1:50), 2, FUN=mean) # Enregistrement dans des dataFrames Robs2019N1_50 &lt;- data.frame(An=&quot;2019&quot;, Voisins=1:length(Robs2019N1_50), Robs=Robs2019N1_50) Robs2020N1_50 &lt;- data.frame(An=&quot;2020&quot;, Voisins=1:length(Robs2020N1_50), Robs=Robs2020N1_50) Robs2021N1_50 &lt;- data.frame(An=&quot;2021&quot;, Voisins=1:length(Robs2021N1_50), Robs=Robs2021N1_50) Robs2022N1_50 &lt;- data.frame(An=&quot;2022&quot;, Voisins=1:length(Robs2022N1_50), Robs=Robs2022N1_50) # Combinaison des dataFrames en un seul RobsN1_50 &lt;- rbind(Robs2019N1_50, Robs2020N1_50, Robs2021N1_50, Robs2022N1_50) # Création du graphique ggplot(RobsN1_50)+ geom_point(aes(x = Voisins, y = Robs, color = An))+ geom_line(aes(x = Voisins, y = Robs, color = An))+ labs(x = &quot;Nombre de voisins&quot;, y = &quot;Robs - Distance en mètres&quot;, color = &quot;Année&quot;) Figure 3.11: Distance au plus proche voisin de 1 à 50 3.3.2 Méthode des quadrats 3.3.2.1 Principe de base Le principe de base de la méthode des quadrats peut être décomposé en trois étapes : Superposer à la région d’étude comprenant le semis de points un ensemble de quadrats (habituellement une grille régulière formée d’un ensemble de carrés). Compter le nombre de points compris dans chacun des quadrats. De la sorte, certains quadrats ne comprennent aucun point tandis que d’autres en contiennent un, deux, trois, etc. Nous obtenons ainsi une table des fréquences. Réaliser des tests statistiques à partir des fréquences observées et théoriques pour qualifier la distribution du semis de points (test de Kolmogorov-Smirnov, test du khi-deux ou méthode Monte-Carlo). 3.3.2.2 Forme, distribution et taille des quadrats Il est possible de paramétrer les quadrats selon leur forme, leur distribution et leur taille. Habituellement, la forme retenue pour les quadrats est le carré, mais d’autres formes géométriques peuvent être utilisées comme l’hexagone et plus rarement, le cercle. La distribution des quadrats peut aussi être soit régulière, soit aléatoire (figure 3.12). Notez que dans le cas d’un cercle, le maillage ne peut être qu’irrégulier puisque certains points risqueraient de ne pas être contenus dans un cercle pour un maillage régulier. Figure 3.12: Formes et distributions de quadrats Bien entendu, les résultats varient selon la taille des quadrats. Par exemple, dans le cas d’une distribution spatiale concentrée d’un semis de points, diminuer la taille des quadrats risque d’augmenter la perception de la dispersion. Certains auteurs proposent alors une formule pour déterminer la superficie optimale du quadrat (Wong et Lee 2005; Mitchel 2005) : \\[\\begin{equation} S_q = \\frac{2S}{n} \\tag{3.24} \\end{equation}\\] \\[\\begin{equation} l_q = \\sqrt{S_q} \\text{ et } r_q = \\sqrt{\\frac{S_q}{\\pi}} \\text{ et } l_a = \\sqrt{ \\frac{2S_q}{3 \\sqrt{3}}} \\text{ avec :} \\tag{3.25} \\end{equation}\\] \\(S_q\\), superficie du quadrat. \\(n\\), nombre de points dans le semis. \\(l_q\\), longueur du côté si la forme du quadrat est un carré. \\(r_q\\), longueur du rayon si la forme du quadrat est une cercle. \\(l_a\\), longueur du côté d’un hexagone régulier. 3.3.2.3 Tests statistiques Construction du tableau de fréquences observées et théoriques Une fois les quadrats créés, nous devons compter le nombre de points compris dans chacun d’eux. Une distribution spatiale concentrée à l’extrême se traduit par la localisation de tous les points du semis d’un seul quadrat, tandis que pour une distribution dispersée maximale se traduit par le fait que tous les quadrats contiennent le même nombre de points. Par la suite, nous construisons un tableau de fréquence. Prenons deux situations à la figure 3.13 : A. Une distribution dispersée, puisque les points sont présents dans la plupart des quadrats. B. Une distribution concentrée, puisque les points sont localisés dans quelques quadrats. Notez que pour les deux situations, nous avons 42 points (\\(n\\)) et 36 quadrats (\\(k\\)), soit une moyenne de 1,167 point par quadrat (\\(\\lambda = n / k = 42 / 36 = 1,167\\)). Détaillons les différentes colonnes du tableau de fréquences observées et théoriques : Fréquences observées (\\(f_o\\)) : pour la situation A, nous avons 16 quadrats qui ne comprennent aucun point, 4 quadrats avec 1 point, 10 quadrats avec 2 points et finalement 6 quadrats avec 3 points. À l’inverse, pour la situation B, 27 quadrats sur les 36 ne comprennent aucun point, suggérant ainsi une concentration plus forte! Proportions observées : simplement les fréquences observées divisées par le nombre total de quadrats (par exemple, \\(16 / 36 = 0,444\\) pour A). Proportions théoriques : à partir de la loi de probabilité de Poisson (équation (3.26)), il est possible de calculer les proportions théoriques que nous devrions avoir si les points étaient distribués aléatoirement. Pas de panique avec la lecture de la formule, nous verrons qu’il existe une fonction pour la calculer facilement dans R. Nous calculons aussi les proportions théoriques cumulées. Fréquences théoriques : les fréquences théoriques sont simplement les proportions théoriques multipliées par le nombre de quadrats (par exemple, \\(\\text{0,311} \\times 36 = \\text{11,196}\\)). \\[\\begin{equation} p(x = k )= \\frac{\\lambda^k e^{-\\lambda}}{x!}\\text{ avec :} \\tag{3.26} \\end{equation}\\] \\(\\lambda \\text{ (lambda)} = n / k\\), soit le nombre moyen de points (\\(n\\)) par quadrat (\\(k\\)); \\(x\\), le nombre de points dans le quadrat (0, 1, 2, etc.); \\(!x\\), la factorielle d’un nombre (par exemple, \\(!3 = 1 \\times 2 \\times 3 = 6\\)); \\(e\\), la constante de l’Euler, soit \\(exp(1) = \\text{2,718282}\\). À partir de ce tableau des fréquences observées et théoriques, nous pouvons calculer les tests de Kolmogorov-Smirnov et du khi-deux. Figure 3.13: Illustrations des tests statistiques sur les quadrats Test statistique de Kolmogorov-Smirnov Ce test se décompose en six étapes : Formuler l’hypothèse nulle stipulant que les fréquences observées et théoriques ne sont pas statistiquement différentes (\\(H_0\\)). Choisir un seuil de signification (par exemple, \\(\\alpha = \\text{0,05}\\)). Calculer la différence absolue entre les proportions cumulées observées et théoriques. Calculer la statistique \\(D\\), soit la plus forte valeur des différences absolues entre les fréquences cumulées observées et théoriques (équation (3.27)). Calculer la valeur critique pour une distribution aléatoire avec un seuil de signification \\(\\alpha\\) (équation (3.28)). Comparer les valeurs de \\(D\\) et de \\(D_{\\alpha = 0.05}\\) : Si \\(D = D_{\\alpha = \\text{0,05}}\\), la distribution est aléatoire. Si \\(D &lt; D_{\\alpha = \\text{0,05}}\\), la distribution est dispersée. Si \\(D &gt; D_{\\alpha = \\text{0,05}}\\), la distribution est concentrée. Plus la valeur de \\(D\\) est élevée, plus la distribution spatiale du semis de points est concentrée. \\[\\begin{equation} D = \\text{max}\\lvert poi_{cumulé} - pti_{cumulé} \\rvert \\tag{3.27} \\end{equation}\\] \\[\\begin{equation} D_{\\alpha = \\text{0,05}}= \\frac{\\text{1,36}}{\\sqrt{m}}\\text{ avec } \\tag{3.28} \\end{equation}\\] \\(m\\) étant le nombre total de quadrats; \\(poi_{cumulé}\\) et \\(pti_{cumulé}\\), les proportions cumulées observées et théoriques. Appliquons cette démarche du test de Kolmogorov-Smirnov aux deux distributions de la figure 3.13 : \\(D_{\\alpha = \\text{0,05}}= \\frac{\\text{1,36}}{\\sqrt{36}}=\\text{0,210}\\) pour la situation A, \\(D = \\text{0,133}\\), donc \\(D &lt; D_{\\alpha = \\text{0,05}}\\), alors la distribution est significativement dispersée. pour la situation B, \\(D = \\text{0,439}\\), donc \\(D &gt; D_{\\alpha = \\text{0,05}}\\), alors la distribution est significativement concentrée. Test statistique du khi-deux Ce test se décompose en quatre étapes : Formuler l’hypothèse nulle stipulant que la distribution des fréquences observées dans les quadrats suit une distribution de Poisson (\\(H_0\\)). Calculer le khi-deux (équation (3.29)). Comparer la valeur du khi-deux obtenue avec celle du khi-deux théorique (\\(\\chi^2_{\\alpha,dl}\\)) avec \\(k-1\\) degrés de liberté (\\(dl\\)) et un seuil de signification \\(\\alpha\\) (0,05 par exemple). Si \\(\\chi^2 &gt; \\chi^2_{\\alpha,dl}\\), alors l’hypothèse nulle est rejetée. \\[\\begin{equation} \\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\\text{ avec } \\tag{3.29} \\end{equation}\\] \\(O_i\\) et \\(E_i\\) étant respectivement les fréquences observée et attendue pour \\(i\\) (quadrat avec 0 point, 1, 2, etc.). Pour les deux situations, le khi-deux calculé est supérieur au khi-deux théorique avec un seuil \\(\\alpha\\) de 0,001 et 35 degrés de liberté. Par conséquent, les deux distributions ne sont pas aléatoires. round(qchisq(p=0.95, df=35, lower.tail = TRUE),3) ## [1] 49.802 round(qchisq(p=0.99, df=35, lower.tail = TRUE),3) ## [1] 57.342 round(qchisq(p=0.999, df=35, lower.tail = TRUE),3) ## [1] 66.619 Pas de panique! Vous n’êtes pas familier avec la loi de probabilité de Poisson et le test du khi-deux, retenez simplement la démarche générale, nous allons utiliser des fonctions qui vont vous faciliter la vie dans R! 3.3.2.4 Mise en œuvre dans R La syntaxe dessous permet de déterminer la superficie optimale des quadrats en fonction du nombre de points (méfaits pour l’année 2020) et de la superficie de l’espace d’étude. library(spatstat) ## Taille des quadrats # Nombre de points npoints &lt;- nrow(M2020) # Superficie de l&#39;espace d&#39;étude S &lt;- as.numeric(st_area(st_union(Arrondissements))) # Superficie du quadrat (équation 3.24) Sq &lt;- (2*S) / npoints # Longueur du carré et du côté de l&#39;hexagone régulier (équation 3.25) lq &lt;- sqrt(Sq) la &lt;- sqrt( (2*Sq) / (3*sqrt(3))) # Trouver la longueur du côté du carré dans lequel est compris l&#39;hexagone cellsizeHex &lt;- 2 * sqrt(Sq/((3*sqrt(3)/2))) * sqrt(3)/2 cat(&quot;Nombre de points =&quot;, npoints, &quot;\\nSuperficie (éq. 3.24) =&quot;, Sq, &quot;\\nLongueur du côté du carré (éq. 3.25) =&quot;, lq, &quot;\\nLongueur du côté du l&#39;hexagone (éq. 3.25) =&quot;, la, &quot;\\nLongueur du côté du carré dans lequel est compris l&#39;hexagone =&quot;, cellsizeHex, &quot;\\n&quot;) ## Nombre de points = 383 ## Superficie (éq. 3.24) = 1913098 ## Longueur du côté du carré (éq. 3.25) = 1383.148 ## Longueur du côté du l&#39;hexagone (éq. 3.25) = 858.1093 ## Longueur du côté du carré dans lequel est compris l&#39;hexagone = 1486.289 Nous pouvons ensuite créer deux couches avec des quadrats carrés et hexagonaux avec la fonction st_make_grid du package sf. Repérez le paramètre square dans la fonction st_make_grid : écrivez square = TRUE pour obtenir des carrés et square = FALSE pour des hexagones réguliers. ## Création des quadrats #Géométrie pour l&#39;espace d&#39;étude EspaceEtude &lt;- st_geometry(st_union(Arrondissements)) # Création des carrés Carres.sf &lt;- st_make_grid(EspaceEtude, lq, crs = st_crs(Arrondissements), what = &quot;polygons&quot;, square = TRUE) # Création des hexagones Hexagones.sf &lt;- st_make_grid(EspaceEtude, cellsizeHex, crs = st_crs(Arrondissements), what = &quot;polygons&quot;, square = FALSE) Carres.sf &lt;- st_sf(idCarre = 1:length(lengths(Carres.sf)), Carres.sf) Hexagones.sf &lt;- st_sf(idHex = 1:length(lengths(Hexagones.sf)), Hexagones.sf) cat(&quot;Superficie (éq. 3.24) =&quot;, Sq, &quot;\\nVérifier la superficie des carrés et des hexagones&quot;, &quot;\\nSuperficie des carrés =&quot;, as.numeric(st_area(Carres.sf[1,])), &quot;\\nSuperficie des hexagones =&quot;, as.numeric(st_area(Hexagones.sf[1,])), &quot;Les superficies sont bien égales!\\n&quot;) ## Superficie (éq. 3.24) = 1913098 ## Vérifier la superficie des carrés et des hexagones ## Superficie des carrés = 1913098 ## Superficie des hexagones = 1913098 Les superficies sont bien égales! ## Visualisation tmap_mode(&quot;plot&quot;) tm_shape(Hexagones.sf)+tm_borders(col=&quot;black&quot;)+ tm_shape(Carres.sf)+tm_borders(col=&quot;red&quot;)+ tm_shape(Arrondissements)+tm_borders(col=&quot;blue&quot;, lwd=3)+ tm_shape(M2020)+tm_dots(col=&quot;green&quot;, size=0.15)+ tm_layout(frame = FALSE) La figure ci-dessus permet de constater que certains carrés et hexagones n’intersectent pas l’espace d’étude. Par conséquent, nous allons les supprimer puis calculer le nombre de points par carré et par hexagone. ## Suppression des carrés qui n&#39;intersectent pas les quatre arrondissements RequeteSpatiale &lt;- st_intersects(Carres.sf, st_union(Arrondissements), sparse = FALSE) Carres.sf$Intersect &lt;- RequeteSpatiale[, 1] Carres.sf &lt;- Carres.sf[Carres.sf$Intersect== TRUE, ] ## Suppression des hexagones qui n&#39;intersectent pas les quatre arrondissements RequeteSpatiale &lt;- st_intersects(Hexagones.sf, st_union(Arrondissements), sparse = FALSE) Hexagones.sf$Intersect &lt;- RequeteSpatiale[, 1] Hexagones.sf &lt;- Hexagones.sf[Hexagones.sf$Intersect== TRUE, ] ## Jointure spatiale : compter le nombre de méfaits de 2020 dans les carrés et les hexagones Carres.sf$Mefaits2020 = lengths(st_intersects(Carres.sf, M2020)) Hexagones.sf$Mefaits2020 = lengths(st_intersects(Hexagones.sf, M2020)) ## Tableau de fréquences table(Carres.sf$Mefaits2020) ## ## 0 1 2 3 4 5 6 7 8 10 11 13 21 38 41 64 ## 172 25 6 3 8 1 3 4 3 2 3 1 1 1 1 1 table(Hexagones.sf$Mefaits2020) ## ## 0 1 2 3 4 5 6 7 8 9 15 17 19 20 40 48 ## 173 23 5 5 3 5 6 1 3 1 2 1 1 1 1 2 Visualisons les résultats à la figure 3.14. Il y a clairement une tendance à la concentration puisque de nombreux quadrats ne contiennent aucun point. ## Visualisation tmap_mode(&quot;plot&quot;) Carte1 = tm_shape(subset(Carres.sf, Mefaits2020 == 0))+ tm_polygons(col=&quot;gray90&quot;, border.col = &quot;white&quot;, lwd = 1)+ tm_shape(subset(Carres.sf, Mefaits2020!= 0))+ tm_polygons(col=&quot;Mefaits2020&quot;, style=&quot;cont&quot;, title=&quot;Nombre&quot;, border.col = &quot;white&quot;, lwd = 1)+ tm_shape(Arrondissements)+tm_borders(col=&quot;black&quot;, lwd=2)+ tm_layout(frame = FALSE) Carte2 = tm_shape(subset(Hexagones.sf, Mefaits2020 == 0))+ tm_polygons(col=&quot;gray90&quot;, border.col = &quot;white&quot;, lwd = 1)+ tm_shape(subset(Hexagones.sf, Mefaits2020!= 0))+ tm_polygons(col=&quot;Mefaits2020&quot;, style=&quot;cont&quot;, title=&quot;Nombre&quot;, border.col = &quot;white&quot;, lwd = 1)+ tm_shape(Arrondissements)+tm_borders(col=&quot;black&quot;, lwd=2)+ tm_layout(frame = FALSE) tmap_arrange(Carte1, Carte2) Figure 3.14: Nombre de méfaits dans les deux géométries de quadrats Nous pouvons maintenant construire le tableau de fréquences et calculer les différents tests statistiques. La syntaxe ci-dessous génère le tableau de fréquences et applique le test de Kolmogorov-Smirnov. ## Construction du tableau de fréquences TabFreq &lt;- as.data.frame(table(Carres.sf$Mefaits2020)) names(TabFreq) &lt;- c(&quot;Npoints&quot;,&quot;Fo&quot;) TabFreq$Npoints &lt;- as.numeric(as.character(TabFreq$Npoints)) # Calcul pour les fréquences observées (fo) TabFreq$Fo.pro &lt;- TabFreq$Fo / sum(TabFreq$Fo) TabFreq$Fo.proCum &lt;- cumsum(TabFreq$Fo.pro) # Calcul pour les fréquences théoriques npoints &lt;- sum(TabFreq$Npoints*TabFreq$Fo) nquadrats &lt;- sum(TabFreq$Fo) Lambda &lt;- npoints / nquadrats TabFreq$Ft.pro &lt;- dpois(TabFreq$Npoints, lambda= Lambda) TabFreq$Ft.proCum &lt;- ppois(TabFreq$Npoints, lambda= Lambda, lower.tail = TRUE) TabFreq$Ft &lt;- TabFreq$Ft.pro * TabFreq$Npoints # Différences absolues entre les fréquences observées et théoriques cumulées TabFreq$Difffoft &lt;- abs(TabFreq$Fo.proCum - TabFreq$Ft.proCum) #calcul de D et Da D &lt;- max(TabFreq$Difffoft) Da &lt;- 1.36 / sqrt(nquadrats) # avec p à 0,05 ## Diagnostic if (D&gt;Da){ cat(&quot;D =&quot;,round(D,3),&quot; et Da =&quot;, round(Da,3), &quot;\\nD &gt; Da avec p =0,05, alors la distribution tend vers la concentration.&quot;) }else{ cat(&quot;D =&quot;,round(D,3),&quot; et Da =&quot;, round(Da,3), &quot;\\nD &lt; Da avec p =0,05, alors la distribution tend vers la dispersion.&quot;) } ## D = 0.536 et Da = 0.089 ## D &gt; Da avec p =0,05, alors la distribution tend vers la concentration. Le package spatstat permet de réaliser différents tests avec la fonction quadrat.test. Pour ce faire, il faut préalablement convertir les données dans les formats utilisés par ce package (ppp, owin et tess). ## Conversion des points au format ppp M2020.ppp &lt;- ppp(x = st_coordinates(M2020)[,1], y = st_coordinates(M2020)[,2], window = as.owin(Arrondissements), check = T) ## Conversion des quadrats carrés ou hexagonaux en objet owin quadrats &lt;- as(st_geometry(Carres.sf), &quot;Spatial&quot;) FenetresC &lt;- lapply(quadrats@polygons,function(x){ coords &lt;- x@Polygons[[1]]@coords coords&lt;-coords[nrow(coords):1,] owin(poly = coords)}) quadrats &lt;- as(st_geometry(Hexagones.sf), &quot;Spatial&quot;) FenetresH &lt;- lapply(quadrats@polygons,function(x){ coords &lt;- x@Polygons[[1]]@coords coords&lt;-coords[nrow(coords):1,] owin(poly = coords)}) ## Ces fenêtres sont ensuite converties en un objet tess (tesselation) TessalationC &lt;- as.tess(FenetresC) TessalationH &lt;- as.tess(FenetresH) Nous pouvons calculer différents tests avec la fonction quadrat.test. Dans un premier temps, nous vérifions si la distribution est dispersée avec la méthode Monte-Carlo et 999 permutations (alternative =\"regular\"). Que ce soit avec les quadrats carrés ou hexagonaux, la valeur de p est égale à 0. Cela signifie que nous sommes 100 % certains que la distribution n’est pas dispersée. ## Réalisation des différents tests du khi-deux # test pour une distribution dispersée (Carrés) cat(&quot;Test pour une distribution dispersée avec les quadrats carrés&quot;) ## Test pour une distribution dispersée avec les quadrats carrés quadrat.test(M2020.ppp, tess = TessalationC, method = &quot;MonteCarlo&quot;, nsim = 999, alternative =&quot;regular&quot;) # dispersée ## ## Conditional Monte Carlo test of CSR using quadrat counts ## Test statistic: Pearson X2 statistic ## ## data: M2020.ppp ## X2 = 5212.2, p-value = 1 ## alternative hypothesis: regular ## ## Quadrats: 235 tiles (irregular windows) cat(&quot;Test pour une distribution dispersée avec les quadrats hexagonaux&quot;) ## Test pour une distribution dispersée avec les quadrats hexagonaux quadrat.test(M2020.ppp, tess = TessalationH, method = &quot;MonteCarlo&quot;, nsim = 999, alternative =&quot;regular&quot;) # dispersée ## ## Conditional Monte Carlo test of CSR using quadrat counts ## Test statistic: Pearson X2 statistic ## ## data: M2020.ppp ## X2 = 4792.3, p-value = 1 ## alternative hypothesis: regular ## ## Quadrats: 233 tiles (irregular windows) Dans un second temps, nous vérifions si la distribution est concentrée, toujours avec la méthode Monte-Carlo et 999 permutations (alternative =\"clustered\"). Que ce soit avec les quadrats carrés ou hexagonaux, la valeur de p est égale à 0,001. Cela signifie qu’il y a moins de 1% de chances que la distribution concentrée soit due au hasard! # test pour une distribution concentrée cat(&quot;Test pour une distribution concentrée avec les quadrats carrés&quot;) ## Test pour une distribution concentrée avec les quadrats carrés quadrat.test(M2020.ppp, tess = TessalationC, method = &quot;MonteCarlo&quot;, nsim = 999, alternative =&quot;clustered&quot;) # concentrée ## ## Conditional Monte Carlo test of CSR using quadrat counts ## Test statistic: Pearson X2 statistic ## ## data: M2020.ppp ## X2 = 5212.2, p-value = 0.001 ## alternative hypothesis: clustered ## ## Quadrats: 235 tiles (irregular windows) cat(&quot;Test pour une distribution concentrée avec les quadrats hexagonaux&quot;) ## Test pour une distribution concentrée avec les quadrats hexagonaux quadrat.test(M2020.ppp, tess = TessalationH, method = &quot;MonteCarlo&quot;, nsim = 999, alternative =&quot;clustered&quot;) # concentrée ## ## Conditional Monte Carlo test of CSR using quadrat counts ## Test statistic: Pearson X2 statistic ## ## data: M2020.ppp ## X2 = 4792.3, p-value = 0.001 ## alternative hypothesis: clustered ## ## Quadrats: 233 tiles (irregular windows) References "],["sect034.html", "3.4 Identification des agrégats spatiaux", " 3.4 Identification des agrégats spatiaux Pour repérer les concentrations d’un semis de points dans une région, nous cartographions la densité des points dans une maille soit irrégulière, soit régulière. 3.4.1 Cartographie de la densité dans une maille irrégulière Une maille irrégulière est habituellement un découpage administratif comme les MRC du Québec ou les arrondissements d’une ville. À la figure 3.15, nous avons compté le nombre de méfaits par secteurs de recensement pour l’année 2021, puis calculer la densité, soit le nombre de méfaits pour 1000 habitants. Les cercles proportionnels permettent ainsi de repérer les secteurs comprenant le plus de méfaits. Toutefois, la population totale variant d’un SR à l’autre, il est préférable de cartographier la densité, soit le nombre de méfaits pour 1000 habitants. Notez que pour d’autres jeux de données, il serait plus judicieux d’utiliser la superficie comme dénominateur pour calculer la densité (par exemple, le nombre d’arbres au kilomètre carré ou à l’hectare). En quelques lignes de code, il est très facile de calculer et de cartographier la densité dans un maillage irrégulier. ## Secteurs de recensement (SR) de la ville de Sherbrooke en 2021 SR &lt;- st_read(dsn = &quot;data/chap03/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DR_SherbSRDonnees2021&quot;, quiet=TRUE) ## Sélection des méfaits pour l&#39;année 2021 Incidents &lt;- st_read(dsn = &quot;data/chap03/IncidentsSecuritePublique.shp&quot;, quiet=TRUE) M2021 &lt;- subset(Incidents, DESCRIPTIO == &quot;Méfait&quot; &amp; ANNEE==2021) ## Nous nous assurons que les deux couches ont la même projection cartographique SR &lt;- st_transform(SR, st_crs(M2021)) ## Calcul du nombre d&#39;incidents par SR SR$Mefaits2021 &lt;- lengths(st_intersects(SR, M2021)) ## Calcul du nombre de méfaits pour 1000 habitants SR$DensiteM21Hab &lt;- SR$Mefaits2021 / (SR$SRpop_2021 / 1000) ## Cartographie tm_shape(SR)+ tm_polygons(col=&quot;Mefaits2021&quot;, style=&quot;pretty&quot;, title=&quot;Nombre pour 1000 habitants&quot;, border.col = &quot;black&quot;, lwd = 1)+ tm_bubbles(size = &quot;DensiteM21Hab&quot;, border.col = &quot;black&quot;, alpha = .5, col = &quot;aquamarine3&quot;, title.size = &quot;Nombre&quot;, scale = 1.5)+ tm_layout(frame = FALSE)+tm_scale_bar(text.size = .5, c(0, 5, 10)) Figure 3.15: Densité des méfaits par secteurs de recensement, ville de Sherbrooke, 2021 3.4.2 Cartographie de la densité dans une maille régulière 3.4.2.1 Description de la méthode KDE Le principe de base d’une cartographie de la densité dans une maille régulière – carte de chaleur ou estimation de la densité par noyau (kernel density estimation – KDE, en anglais) – est fort simple et se décompose en trois étapes : Juxtaposer une grille de cellules sur l’espace d’étude, soit les pixels d’une image de n mètres de côté (par exemple, 50 mètres). Pour chaque pixel, juxtaposer une zone de recherche de n mètres de rayon (1000 mètres par exemple) et calculer le nombre de points présents dans cette zone de recherche. Calculer la densité à partir d’une fonction simple (appelée habituellement fonction uniforme) ou d’une fonction de densité (kernel). Pour une fonction simple, nous divisons simplement le nombre de points présents dans la zone de recherche par la superficie de la zone, soit \\(πr^2\\); cette approche est toutefois peu recommandée puisqu’elle accorde la même pondération à chaque point situé de la zone d’influence! En utilisant une fonction de densité (kernel) (équation (3.30)), nous accordons une pondération à chacun des points compris dans la zone de recherche : plus le point est proche du centre de la cellule, plus son poids est important dans l’estimation de la densité. \\[\\begin{equation} \\lambda(s) = \\sum_{i=1}^n \\frac{1}{\\pi r^2} k\\biggl(\\frac{d_{si}}{r}\\biggl)\\text{ avec } \\tag{3.30} \\end{equation}\\] \\(\\lambda(s)\\), estimation de la densité à la localisation \\(s\\). \\(r\\), rayon de la zone de recherche. \\(d_{is}\\), distance entre la localisation \\(s\\) et le point \\(i\\). \\(k\\), fonction kernel définissant la pondération à accorder au ratio \\(\\frac{d_{si}}{r}\\) quand \\(0 &lt; d_{si} \\leq r\\). Si \\(d_{si} &gt; r\\), alors \\(k(d_{si}/r) = 0\\). Il existe différentes fonctions kernel pour obtenir les pondérations des points. Telle qu’illustrée à la figure 3.16, l’idée générale est que plus la distance entre la localisation et le point augmente (\\(d_{si}\\)), plus la pondération \\(k(d_{si}/r)\\) de l’équation (3.30) est faible. Figure 3.16: Principe de la fonction kernel pour définir les pondérations des points voisins dans la zone d’influence Les différentes fonctions kernel (kernel) d’estimation de la densité Pour une description des différentes fonctions kernel, consultez le lien suivant. Notez que l’outil Carte de chaleur (estimation par noyau) de QGIS inclut plusieurs fonctions kernel alors que l’outil Kernel Density d’ArcGIS Pro utilise uniquement la fonction quadratique. Finalement, la fonction density.ppp du package spatstat intègre quatre fonctions. Influence de la forme du noyau (de la fonction kernel) Excepté la fonction uniforme, la plupart des auteurs s’entendent sur le fait que le choix de fonctions kernel a relativement peu d’influence sur le résultat final comparativement au choix du rayon d’influence. Tableau 3.4: Fonctions kernel disponibles selon différents logiciels Fonction QGis ArcGIS Pro CrimeStat R (density.ppp) Gaussienne X X Triangulaire X X Quadratique X X X X Uniforme X X X Cubique X Epanechnikov X X Exponentielle X 3.4.2.2 Mise en œuvre du KDE dans R Pour calculer la densité dans une maille régulière (KDE), vous pouvez utiliser deux packages : SpatialKDE, un package proposé très récemment qui reprend le code de l’outil Carte de chaleur (estimation par noyau) de QGIS (Caha 2023). Vous obtiendrez donc les mêmes fonctionnalités et résultats qu’avec QGIS. Pour un exemple d’utilisation, consultez cette vignette. spatstat, soit le package le plus complet pour réaliser des analyses de répartition ponctuelle (Baddeley et Turner 2005; Baddeley, Rubak et Turner 2015). Par conséquent, nous privilégions son utilisation dans les exemples ci-dessous. Afin d’expliquer comment réaliser un KDE dans R, nous utilisons les accidents survenus dans l’arrondissement des Nations. Notez que nous retenons uniquement cet arrondissement et non l’ensemble de la ville afin d’accélérer les temps de calculs. library(sf) library(spatstat) library(tmap) library(terra) ## Importation des données Arrondissements &lt;- st_read(dsn = &quot;data/chap03/Arrondissements.shp&quot;, quiet=TRUE) Incidents &lt;- st_read(dsn = &quot;data/chap03/IncidentsSecuritePublique.shp&quot;, quiet=TRUE) Arrondissements &lt;- st_transform(Arrondissements, crs = 3798) Incidents &lt;- st_transform(Incidents, crs = 3798) ## Couche pour les accidents Accidents &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% c(&quot;Accident avec blessés&quot;, &quot;Accident mortel&quot;)) ## Pour accélérer les calculs, nous retenons uniquement l&#39;arrondissement des Nations # Couche pour l&#39;arrondissement des Nations ArrDesNations &lt;- subset(Arrondissements, NOM == &quot;Arrondissement des Nations&quot;) # Sélection des accidents localisés dans l&#39;arrondissement des Nations RequeteSpatiale &lt;- st_intersects(Accidents, ArrDesNations, sparse = FALSE) Accidents$Nations &lt;- RequeteSpatiale[, 1] AccNations &lt;- subset(Accidents, Accidents$Nations == TRUE) # Une couche par année AccNations2019 &lt;- subset(AccNations, AccNations$AN == 2019) AccNations2020 &lt;- subset(AccNations, AccNations$AN == 2020) AccNations2021 &lt;- subset(AccNations, AccNations$AN == 2021) AccNations2022 &lt;- subset(AccNations, AccNations$AN == 2022) Nous convertissons les points dans le format ppp utilisé par le package spatstat. ## Conversion des données sf dans le format de spatstat # la fonction as.owin est utilisée pour définir la fenêtre de travail fenetre &lt;- as.owin(ArrDesNations) ## Conversion des points au format ppp pour les différentes années # 2019 AccN2019.ppp &lt;- ppp(x = st_coordinates(AccNations2019)[,1], y = st_coordinates(AccNations2019)[,2], window = fenetre, check = T) # 2020 AccN2020.ppp &lt;- ppp(x = st_coordinates(AccNations2020)[,1], y = st_coordinates(AccNations2020)[,2], window = fenetre, check = T) # 2021 AccN2021.ppp &lt;- ppp(x = st_coordinates(AccNations2021)[,1], y = st_coordinates(AccNations2021)[,2], window = fenetre, check = T) # 2022 AccN2022.ppp &lt;- ppp(x = st_coordinates(AccNations2022)[,1], y = st_coordinates(AccNations2022)[,2], window = fenetre, check = T) Puis, il faut définir la taille des pixels et le rayon d’influence (bandwidth en anglais) : Plus la taille des pixels est réduite, plus la taille de l’image résultante est grande et les calculs longs à réaliser. Le choix du rayon est aussi délicat. Avec un rayon trop petit, les résultats sont trop détaillés avec des valeurs très faibles. À l’inverse, un rayon trop grand a comme effet de lisser les résultats et de masquer des disparités locales. Ratio entre la taille du pixel et la taille du rayon d’influence Assurez-vous que la taille du pixel soit bien inférieure (au moins dix fois plus petite) à celle du rayon d’influence afin d’obtenir des résultats précis. À notre connaissance, il n’existe pas de règle pour optimiser la valeur ratio entre la taille du pixel et la taille du rayon d’influence. Plusieurs algorithmes permettant de sélectionner la valeur optimale du rayon d’influence sont implémentés dans le package spatstat avec les fonctions bw.diggle, bw.ppl, bw.scott et bw.CvL. À la lecture des résultats ci-dessous, la valeur du rayon proposée par l’algorithme de Diggle semble bien trop petite et celle du critère de Cronie et van Lieshout bien trop grande. Le rayon optimal est certainement compris entre 700 et 750 mètres. Par conséquent, nous retenons une taille de pixel de 50 mètres et la valeur du rayon optimisée par la fonction bw.ppl. cat(&quot;\\nDiggle :&quot;, bw.diggle(AccN2020.ppp), &quot;\\nMaximum de vraissemblance :&quot;, bw.ppl(AccN2020.ppp), &quot;\\nCritère de Scott 1 :&quot;, bw.scott(AccN2020.ppp)[1], &quot;\\nCritère de Scott 2 :&quot;, bw.scott(AccN2020.ppp)[2], &quot;\\nCritère de Cronie et van Lieshout :&quot;, bw.CvL(AccN2020.ppp)) ## ## Diggle : 124.196 ## Maximum de vraissemblance : 494.5408 ## Critère de Scott 1 : 971.3489 ## Critère de Scott 2 : 514.2688 ## Critère de Cronie et van Lieshout : 1644.489 ## Taille des pixels et du rayon en mètres pixel_m &lt;- 50 RayonOpt &lt;- bw.ppl(AccN2020.ppp) 3.4.2.2.1 Comparaison des fonctions kernel Le paramètre kernel de la fonction density.ppp permet de sélectionner l’une des quatre fonctions kernel (gaussian, quartic, epanechnikov et disc). Pour la dernière, le même poids est attribué aux points compris dans le rayon d’influence; elle correspond ainsi à un kernel uniforme ou simple. Excepté ce dernier kernel, la figure 3.17 démontre que les résultats sont très semblables d’une kernel à l’autre. ## Calcul du KDE avec différentes fonctions kernel # kernel gaussien kdeG &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=&quot;gaussian&quot;) # kernel quadratique kdeQ &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=&quot;quartic&quot;) # kernel Epanechnikov kdeE &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=&quot;epanechnikov&quot;) # kernel disc (uniforme ou simple) # le même poids est accordé à chaque point dans le rayon kdeD &lt;- density.ppp(AccN2020.ppp, sigma=RayonOpt, eps=pixel_m, kernel=&quot;disc&quot;) ## Conversion en raster # étant donné que les valeurs sont exprimées en nombre de points par m2, # nous les multiplions par 1 000 000 pour obtenir une densité au km2. RkdeG &lt;- terra::rast(kdeG)*1000000 RkdeQ &lt;- terra::rast(kdeQ)*1000000 RkdeE &lt;- terra::rast(kdeE)*1000000 RkdeD &lt;- terra::rast(kdeD)*1000000 ## Projection cartographique crs(RkdeG) &lt;- &quot;epsg:3798&quot; crs(RkdeQ) &lt;- &quot;epsg:3798&quot; crs(RkdeE) &lt;- &quot;epsg:3798&quot; crs(RkdeD) &lt;- &quot;epsg:3798&quot; ## Visualisation des résultats tmap_mode(&quot;plot&quot;) Carte1 &lt;- tm_shape(RkdeG) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;Gaussien&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte2 &lt;- tm_shape(RkdeQ) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;Quadratique&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte3 &lt;- tm_shape(RkdeE) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;Epanechnikov&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte4 &lt;- tm_shape(RkdeD) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;Uniforme&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_scale_bar(breaks = c(0, 1, 2))+tm_layout(frame = F) tmap_arrange(Carte1, Carte2, Carte3, Carte4, ncol = 2, nrow = 2) Figure 3.17: Comparaison des résultats du KDE pour différents kernels 3.4.2.2.2 Évaluation de l’impact du rayon d’influence Le code R permet de réaliser des KDE avec des rayons de 250, 500, 750 et 1000 mètres. La figure 3.17 démontre bien que plus le rayon est grand, plus la carte est lissée. kdeQ250 &lt;- density.ppp(AccN2020.ppp, sigma=250, eps=50, kernel=&quot;quartic&quot;) kdeQ500 &lt;- density.ppp(AccN2020.ppp, sigma=500, eps=50, kernel=&quot;quartic&quot;) kdeQ750 &lt;- density.ppp(AccN2020.ppp, sigma=750, eps=50, kernel=&quot;quartic&quot;) kdeQ1000 &lt;- density.ppp(AccN2020.ppp, sigma=1000, eps=50, kernel=&quot;quartic&quot;) RkdeQ250 &lt;- terra::rast(kdeQ250)*1000000 RkdeQ500 &lt;- terra::rast(kdeQ500)*1000000 RkdeQ750 &lt;- terra::rast(kdeQ750)*1000000 RkdeQ1000 &lt;- terra::rast(kdeQ1000)*1000000 crs(RkdeQ250) &lt;- &quot;epsg:3857&quot; crs(RkdeQ500) &lt;- &quot;epsg:3857&quot; crs(RkdeQ750) &lt;- &quot;epsg:3857&quot; crs(RkdeQ1000) &lt;- &quot;epsg:3857&quot; ## Visualisation des résultats tmap_mode(&quot;plot&quot;) Carte1 &lt;- tm_shape(RkdeQ250) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;r = 250 m&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte2 &lt;- tm_shape(RkdeQ500) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;r = 500 m&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte3 &lt;- tm_shape(RkdeQ750) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;r = 750 m&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte4 &lt;- tm_shape(RkdeQ1000) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;r = 1000 m&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_scale_bar(breaks = c(0, 1, 2))+tm_layout(frame = F) tmap_arrange(Carte1, Carte2, Carte3, Carte4, ncol = 2, nrow = 2) Figure 3.18: Comparaison des résultats du KDE selon le rayon d’influence 3.4.2.2.3 Comparaison entre différentes années Bien entendu, pour un même jeu de données, il est possible de représenter la densité pour différentes années (figure 3.19). kde2019 &lt;- density.ppp(AccN2019.ppp, sigma=500, eps=50, kernel=&quot;quartic&quot;) kde2020 &lt;- density.ppp(AccN2020.ppp, sigma=500, eps=50, kernel=&quot;quartic&quot;) kde2021 &lt;- density.ppp(AccN2021.ppp, sigma=500, eps=50, kernel=&quot;quartic&quot;) kde2022 &lt;- density.ppp(AccN2022.ppp, sigma=500, eps=50, kernel=&quot;quartic&quot;) Rkde2019 &lt;- terra::rast(kde2019)*1000000 Rkde2020 &lt;- terra::rast(kde2020)*1000000 Rkde2021 &lt;- terra::rast(kde2020)*1000000 Rkde2022 &lt;- terra::rast(kde2021)*1000000 crs(Rkde2019) &lt;- &quot;epsg:3798&quot; crs(Rkde2020) &lt;- &quot;epsg:3798&quot; crs(Rkde2021) &lt;- &quot;epsg:3798&quot; crs(Rkde2022) &lt;- &quot;epsg:3798&quot; Carte1 &lt;- tm_shape(Rkde2019) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;2019&quot;)+ tm_shape(AccNations2019) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte2 &lt;- tm_shape(Rkde2020) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;2021&quot;)+ tm_shape(AccNations2020) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte3 &lt;- tm_shape(Rkde2021) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;2022&quot;)+ tm_shape(AccNations2021) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) Carte4 &lt;- tm_shape(Rkde2022) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;2023&quot;)+ tm_shape(AccNations2022) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_scale_bar(breaks = c(0, 1, 2))+tm_layout(frame = F) tmap_arrange(Carte1, Carte2, Carte3, Carte4, ncol = 2, nrow = 2) Figure 3.19: Comparaison des résultats du KDE pour différentes années References "],["sect035.html", "3.5 Quiz de révision du chapitre", " 3.5 Quiz de révision du chapitre Quelle est la différence entre le centre point et le point central? Relisez au besoin la section 3.2.1.2. Le centre moyen peut être pondéré contrairement au point central. Le point central est un point qui fait partie du semis de points initial. Associer la distance standard à chacun des trois semis de points suivants : Relisez au besoin la section 3.2.2. A = 219, B = 347, C = 685 A = 685, B = 219, C = 347 A = 685, B = 347, C = 219 Quelles sont les quatre principales manières de représenter graphiquement la dispersion d’un semis de points? Relisez au besoin la section 3.2.3.1. Enveloppe convexe Centre moyen et point central Rectangle construit avec les déviations standards des X et des Y Distance standard Ellipse de déviation de distance standard Quels sont les trois principales distributions d’un semis de points? Relisez au besoin la section 3.3. Distribution dispersée Distribution aléatoire Distribution concentrée Distribution normale À la lecture des résultats de la méthode du plus proche voisin, quelle année a la distribution spatiale la plus concentrée? Relisez au besoin la section 3.3.1. 2019 2020 2021 2022 En analyse des quadrats, quelles sont les trois principales formes utilisées? Relisez au besoin la section 3.3.2.1. Triangle Carré Hexagone Cercle Quelle est le paramètre influençant le plus les résultats de la méthode KDE? Relisez au besoin la section 3.4.2.1. La fonction kernel utilisée La taille du rayon d’influence Quelles sont les principales fonctions kernel? Relisez au besoin la section 3.4.2.1. Gaussienne Quadratique Uniforme Manhattan Epanechnikov Vérifier votre résultat "],["sect036.html", "3.6 Exercices de révision", " 3.6 Exercices de révision Exercice 1. Calculez le centre moyen et la distance standard pour les accidents. Complétez le code ci-dessous. library(sf) library(tmap) ## Importation des données Arrondissements &lt;- st_read(dsn = &quot;data/chap03/Arrondissements.shp&quot;, quiet=TRUE) Incidents &lt;- st_read(dsn = &quot;data/chap03/IncidentsSecuritePublique.shp&quot;, quiet=TRUE) ## Changement de projection Arrondissements &lt;- st_transform(Arrondissements, crs = 3798) Incidents &lt;- st_transform(Incidents, crs = 3798) ## Couche pour les accidents Accidents &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% c(&quot;Accident avec blessés&quot;, &quot;Accident mortel&quot;)) ## Coordonnées et projection cartographique xy &lt;- À compléter ProjCarto &lt;- À compléter ## Centre moyen CentreMoyen &lt;- data.frame(À compléter) CentreMoyen &lt;- st_as_sf(CentreMoyen, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = À compléter) # Distance standard combinée CentreMoyen$DS &lt;- À compléter CercleDS &lt;- À compléter head(CercleDS) Correction à la section 10.3.1. Exercice 2. Calculez et cartographiez la densité des accidents dans un maillage irrégulier pour l’année 2021. Complétez le code ci-dessous. library(sf) library(tmap) ## Importation des données SR &lt;- st_read(dsn = &quot;data/chap03/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DR_SherbSRDonnees2021&quot;, quiet=TRUE) ## Couche pour les accidents pour l&#39;année 2021 Acc2021 &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% c(&quot;Accident avec blessés&quot;, &quot;Accident mortel&quot;) &amp; ANNEE==2021) ## Nous nous assurons que les deux couches ont la même projection cartographique SR &lt;- st_transform(SR, st_crs(Acc2021)) ## Calcul du nombre d&#39;incidents par SR SR$Acc2021 &lt;- À compléter ## Calcul du nombre de méfaits pour 1000 habitants SR$DensiteMAcc2021Hab &lt;- À compléter ## Cartographie tm_shape(SR)+ tm_polygons(col= À compléter, style=&quot;pretty&quot;, title=&quot;Nombre pour 1000 habitants&quot;, border.col = &quot;black&quot;, lwd = 1)+ tm_bubbles(size = À compléter, border.col = &quot;black&quot;, alpha = .5, col = &quot;aquamarine3&quot;, title.size = &quot;Nombre&quot;, scale = 1.5)+ tm_layout(frame = FALSE)+tm_scale_bar(text.size = .5, c(0, 5, 10)) Correction à la section 10.3.2. Exercice 3. Calculez et cartographiez la densité des accidents dans un maillage régulier pour l’année 2021. Complétez le code ci-dessous. library(sf) library(spatstat) library(tmap) library(terra) ## Importation des données Arrondissements &lt;- st_read(dsn = &quot;data/chap03/Arrondissements.shp&quot;, quiet=TRUE) Incidents &lt;- st_read(dsn = &quot;data/chap03/IncidentsSecuritePublique.shp&quot;, quiet=TRUE) ## Changement de projection Arrondissements &lt;- st_transform(Arrondissements, crs = 3798) Incidents &lt;- st_transform(Incidents, crs = 3798) ## Couche pour les méfaits pour l&#39;année 2021 M2021 &lt;- subset(Incidents, DESCRIPTIO == &quot;Méfait&quot; &amp; ANNEE==2021) ## Pour accélérer les calculs, nous retenons uniquement l&#39;arrondissement des Nations # Couche pour l&#39;arrondissement des Nations ArrDesNations &lt;- subset(Arrondissements, NOM == &quot;Arrondissement des Nations&quot;) # Sélection des accidents localisés dans l&#39;arrondissement Des Nations RequeteSpatiale &lt;- st_intersects(M2021, ArrDesNations, sparse = FALSE) M2021$Nations &lt;- RequeteSpatiale[, 1] M2021Nations &lt;- subset(M2021, M2021$Nations == TRUE) ## Conversion des données sf dans le format de spatstat # la fonction as.owin est utilisée pour définir la fenêtre de travail fenetre &lt;- à complétér ## Conversion des points au format ppp pour les différentes années M2021.ppp &lt;- à complétér ## Kernel quadratique avec un rayon de 500 mètres et une taille de pixel de 50 mètres kdeQ &lt;- density.ppp(M2021.ppp, sigma=500, eps=50, kernel=&quot;quartic&quot;) ## Conversion en raster RkdeQ &lt;- terra::rast(kdeQ)*1000000 ## Projection cartographique crs(RkdeQ) &lt;- &quot;epsg:3857&quot; ## Visualisation des résultats tmap_mode(&quot;plot&quot;) À compléter Correction à la section 10.3.3. "],["chap04.html", "Chapitre 4 Méthodes de détection d’agrégats spatiaux et spatio-temporels", " Chapitre 4 Méthodes de détection d’agrégats spatiaux et spatio-temporels Dans ce chapitre, nous abordons deux familles de méthodes de détection d’agrégats spatiaux et spatio-temporels qui s’appliquent à des géométries différentes : les méthodes de classification basées sur la densité des points (couche de points), principalement les algorithmes DBSCAN (Ester et al. 1996) et ST-DBSCAN (Birant et Kut 2007), et les méthodes de balayage de Kulldorff (1997) (couche polygonale). Dans ce chapitre, nous utilisons les packages suivants : Pour importer et manipuler des fichiers géographiques : sf pour importer et manipuler des données vectorielles. dplyr pour manipuler les données. Pour construire des cartes et des graphiques : tmap pour les cartes. ggplot2 pour construire des graphiques. dbscan pour l’algorithme DBSCAN. SpatialEpi pour les méthodes de balayage de Kurlldoff. References "],["sect041.html", "4.1 Agrégats d’entités spatiales ponctuelles", " 4.1 Agrégats d’entités spatiales ponctuelles 4.1.1 DBSCAN : agrégats spatiaux Dans le chapitre précédent, portant sur les méthodes de répartition ponctuelles, nous avons abordé la méthode KDE permettant de cartographier la densité de points dans une maille régulière (section 3.4.2). La carte de chaleur obtenue avec le KDE représente les valeurs de densité (variable continue) pour les pixels couvrant le territoire à l’étude. Avec l’algorithme DBSCAN (Ester et al. 1996), l’objectif est différent : il s’agit d’identifier des agrégats spatiaux d’évènements ponctuels dans un territoire donné (par exemple, des cas de maladies, d’accidents, d’espèces fauniques ou végétales, de crimes, etc.). Autrement dit, il s’agit d’identifier plusieurs régions du territoire à l’étude dans lesquelles la densité de points est forte. Concrètement, si la méthode KDE renvoie une variable continue pour l’ensemble du territoire, l’algorithme DBSCAN renvoie une variable qualitative uniquement pour les points du jeu de données. 4.1.1.1 Fonctionnement de DBSCAN DBSCAN (Density-Based Spatial Clustering of Applications with Noise) est un algorithme de classification non supervisée qui regroupe des observations en fonction de leur densité dans un espace à deux, trois ou n dimensions (Ester et al. 1996). Comme pour toute autre méthode de classification non supervisée, ces dimensions sont des variables. Par conséquent, en appliquant DBSCAN sur les coordonnées géographiques d’entités ponctuelles 2D (x, y) ou 3D (x, y, z), nous classifions les points du jeu de données. Prenons un jeu de données fictives (figure 4.1.a). À l’œil nu, nous identifions clairement cinq régions distinctes avec une forte densité de points et des zones de faible densité; ces dernières étant représentées par les points noirs avec DBSCAN (figure 4.1.b). Figure 4.1: Jeu de données fictives et classification DBSCAN avec cinq classes L’intérêt majeur de l’algorithme DBSCAN est qu’il est basé sur la densité des points et non sur la distance entre les points comme les algorithmes classiques de classification non supervisée que sont les k-moyennes, k-médianes, k-médoïdes ou la classification ascendante hiérarchique. Tel qu’illustré à la figure 4.2, utiliser la distance pour identifier cinq groupes de points renvoient des résultats peu convaincants. D’une part, tous les points appartiennent à une classe, sans séparer les régions de fortes et de faibles densité. D’autre part, les algorithmes classiques basés sur la distance ne parviennent pas à bien identifier les deux agrégats circulaires (bleu et rouge à la figure 4.1.b) et parfois linéaires (vert et mauve à la figure 4.1.b). Figure 4.2: Classification avec d’autres algorithmes basés sur la distance L’algorithme DBSCAN comprend deux paramètres qui doivent être définis par la personne utilisatrice : Le rayon de recherche, dénommé \\(\\epsilon\\) (epsilon), habituellement basé sur la distance euclidienne. Les distances de Manhattan ou réticulaires peuvent aussi être utilisées. Le nombre minimum de points, dénommé \\(MinPts\\), requis pour qu’un point, incluant lui-même, soit considéré comme un point central et appartienne à un agrégat, un regroupement (cluster en anglais). Avantage de DBSCAN : nul besoin de spécifier le nombre d’agrégats (clusters)! Comparativement à d’autres méthodes de classification non supervisées comme les k-moyennes, k-médianes et k-médoïdes, DBSCAN ne requiert pas de spécifier le nombre de classes à identifier dans le jeu de données. Autrement dit, appliqué à des géométries ponctuelles, l’algorithme DBSCAN détecte autant d’agrégats spatiaux que nécessaires en fonction des valeurs des deux paramètres (\\(\\epsilon\\) et \\(MinPts\\)). À la figure 4.3, nous appliquons l’algorithme DBSCAN à un semis de points avec un rayon de recherche de 500 mètres (\\(\\epsilon=500\\)) et un nombre minimum de cinq points (\\(MinPts = 5\\)). Dans un premier temps, l’algorithme distingue trois types de points : Des points centraux (core points en anglais) qui ont au moins cinq points (incluant eux-mêmes) dans un rayon de 500 mètres (points rouges). Des points frontières (border points) qui ont moins de cinq points (incluant eux-mêmes) dans un rayon de 500 mètres, mais qui sont inclus dans la zone tampon de 500 mètres d’un point central (points bleus). Des points aberrants (noise points) qui ont moins de cinq points (incluant eux-mêmes) dans un rayon de 500 mètres et qui ne sont pas inclus dans la zone tampon d’un point central (points noirs). Figure 4.3: Trois types de points identifiés par l’algorithme DBSCAN Par la suite, les étapes de l’algorithme sont les suivantes : Étape 1. Formation du premier agrégat Nous tirons au hasard un point central et l’assignons au premier agrégat (groupe ou cluster). Puis, les points compris dans la zone tampon du premier point central sont ajoutés à ce premier agrégat. De façon itérative, nous étendons l’agrégat avec les points centraux ou frontières qui sont compris dans les zones tampons des points ajoutés précédemment. Étape 2. Formation d’autres agrégats Lorsque le premier agrégat est complété, nous tirons au hasard un autre point central n’appartenant pas au premier agrégat. Nous appliquons la même démarche qu’à l’étape 1 pour étendre et compléter cet autre agrégat. Les deux sous-étapes ci-dessus sont répétées jusqu’à que tous les points centraux et frontières soient assignés à un agrégat. Nous obtenons ainsi k agrégats (valeurs de 1 à k) tandis que les points aberrants sont affectés à la même classe (valeur de 0 habituellement). Appliqué au semis de points, DBSCAN a détecté deux agrégats et quatre points aberrants (figure 4.4) Figure 4.4: Résultats de l’algorithme DBSCAN 4.1.1.2 Sensibilité et optimisation des paramètres de DBSCAN Les résultats de l’algorithme de DBSCAN varient en fonction de ses deux paramètres, soit le rayon de recherche (\\(\\epsilon\\)) et le nombre minimum de points (\\(MinPts\\)). Concernant le paramètre \\(\\epsilon\\), plus sa valeur est réduite, plus le nombre de points identifiés comme aberrants est important. Inversement, plus elle est grande, plus le nombre d’agrégats diminue. En guise d’illustration, faisons varier la valeur du rayon en maintenant à cinq le nombre minimum de points : Avec un rayon de 250 mètres, cinq agrégats sont identifiés tandis que 29 points sont considérés comme du bruit (figure 4.5.a). Avec un rayon de 500 mètres, la solution semble plus optimale avec deux agrégats et cinq points aberrants (figure 4.5.b). Avec un rayon de 1000 mètres, deux agrégats sont aussi identifiés. Par contre, il ne reste plus qu’un point aberrant. Par conséquent, quatre points qui, à l’œil nu, sont très éloignés d’un agrégat y sont pourtant affectés (figure 4.5.c). Avec un rayon de 1500 mètres, tous les points sont affectés à un et un seul agrégat (figure 4.5.d). Figure 4.5: Variations de résultats de l’algorithme DBSCAN selon la taille du rayon Concernant le paramètre \\(MinPts\\), plusieurs règles de pouce (à la louche) ont été proposées pour fixer sa valeur : \\(MinPts \\geq dim(D) + 1\\), c’est-à-dire que sa valeur doit être minimalement égale au nombre de dimensions plus un (variables) du jeu de données. \\(MinPts = dim(D) \\times 2\\), c’est-à dire que le nombre de points devrait être égal à deux fois le nombre de dimensions du tableau (Sander et al. 1998). \\(MinPts = 4\\) quand le jeu de données ne comprend que deux dimensions (Ester et al. 1996), soit un critère qui s’applique à des géométries ponctuelles 2D. Une fois fixé le nombre minimal de points, nous pouvons optimiser la valeur du rayon de recherche de la façon suivante : Pour chacun des points, nous calculons la distance au kième point le plus proche. Nous trions les valeurs obtenues pour construire un graphique en courbe. Dans ce graphique, le critère du coude est utilisé pour repérer la ou les valeurs signalant un décrochement dans la courbe. À la lecture de la figure 4.6, les valeurs d’epsilon (\\(\\epsilon\\)) à retenir pourraient être 300, 350, 425 et 450 mètres. Figure 4.6: Optimisation de la valeur d’epsilon Si vous repérez plusieurs seuils de distance dans le graphique des distances au k plus proche voisin, réalisez et comparez les résultats des DBSCAN avec ces valeurs d’epsilon. À la figure 4.7, nous constatons que les résultats avec des seuils de 425 et 450 sont identiques et semblent optimaux. Par contre, la solution avec un rayon de 350 mètres identifie deux points aberrants qui pourraient être intégrés au deuxième agrégat tandis que celle avec un rayon de 300 mètres identifie un agrégat supplémentaire, mais classifie de nombreux points comme aberrants. Quel résultat choisir parmi les quatre solutions? Comme pour toute analyse de classification, votre choix peut être objectif et reposer uniquement sur des indicateurs statistiques (ici, le graphique des distances au k plus proche voisin). Il devrait aussi s’appuyer sur vos connaissances du terrain. Selon vous, l’identification d’un troisième agrégat avec une valeur d’epsilon fixée à 300 mètres pourrait refléter une réalité terrain particulièrement intéressante qui motiverait fortement le choix de cette solution. Figure 4.7: Comparaison de solutions DBSCAN avec différentes valeurs d’epsilon Autres algorithmes de classification non supervisée basée sur la densité Bien que DBSCAN soit certainement l’algorithme le plus utilisé, d’autres algorithmes basés sur la densité peuvent être utilisés pour détecter des agrégats spatiaux de points, notamment : HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) (Campello, Moulavi et Sander 2013). Brièvement, cette version modifiée de DBSCAN permet d’obtenir une hiérarchie de partitions, comme dans une classification ascendante hiérarchique. OPTICS (Ordering Points To Identify the Clustering Structure) (Campello, Moulavi et Sander 2013). À titre de rappel, l’algorithme DBSCAN a deux paramètres, soit le nombre minimal de points (\\(MinPts\\)) et la distance de voisinage (\\(\\epsilon\\)). Avec OPTICS, le second paramètre n’a pas besoin d’être spécifié. Succinctement, pour chaque point du jeu de données, il utilise la distance au \\(k\\) (\\(MinPts\\)) plus proche voisin. Application à des évènements localisés sur un réseau de rues Lorsque les évènements sont localisés sur un réseau de rues (des accidents par exemple), il convient d’utiliser une autre métrique que la distance euclidienne pour le rayon de recherche (\\(\\epsilon\\)), soit la distance du chemin le plus court à travers le réseau de rues. Plusieurs solutions ont été récemment proposées : un code Python basé sur la librairie OSMnx proposé par Geoff Boeing. un code R utilisant les packages spNetwork et dbscan proposé par Jérémy Gelb. Le premier est utilisé pour construire la matrice de distance entre les points à partir d’un réseau de rues tandis que le second est utilisé pour l’algorithme DBSCAN. 4.1.2 ST-DBSCAN : agrégats spatio-temporels Derya Birant et Alp Kut (2007) ont proposé une modification de l’algorithme de DBSCAN afin qu’il puisse s’appliquer à des données spatio-temporelles (\\(x\\), \\(y\\), \\(d\\)) avec \\(d\\) étant la date de l’évènement. Dénommé ST-DBSCAN, l’algorithme comprend toujours les deux paramètres de DBSCAN (\\(MinPts\\) et \\(\\epsilon\\)), auxquels s’ajoute un autre paramètre \\(\\epsilon\\) pour le temps (défini en heure, jour, semaine, mois ou année). Autrement dit, deux paramètres de distance sont utilisés : \\(\\epsilon_1\\) pour la proximité spatiale (comme avec DBSCAN) et \\(\\epsilon_2\\) pour la proximité temporelle (Birant et Kut 2007). De la sorte, deux points sont considérés comme voisins si la distance spatiale et la distance temporelle sont toutes deux inférieures aux seuils fixés. Fenêtre temporelle des points formant un agrégat Attention, les points formant un agrégat peuvent avoir une fenêtre temporelle bien plus grande que le seuil \\(\\epsilon_2\\) fixé. Par exemple, fixons les valeurs de \\(\\epsilon_1\\) à 500 mètres et de \\(\\epsilon_2\\) à 7 jours. Si le point A (\\(d\\) = 2023-01-15) est distant de 400 mètres du point B (\\(d\\) = 2023-01-20), les deux points sont considérés comme voisins. Par contre, si le point B est distant du point C (\\(d\\) = 2023-01-25) de moins de 500 mètres, il peut être aussi agrégé à l’agrégat puisque l’écart temporel entre B et C est de 5 jours. Habituellement, plus la valeur de \\(\\epsilon_2\\) est faible, plus le nombre de points considérés comme aberrants est important. 4.1.3 Mise en œuvre dans R 4.1.3.1 DBSCAN Nous utilisons le package dbscan (Hahsler et Piekenbrock 2022; Hahsler, Piekenbrock et Doran 2019) dans lequel sont implémentés plusieurs algorithmes dont DBSCAN, mais aussi OPTICS et HDBSCAN. La fonction dbscan(x, eps, minPts, weights = NULL) comprend plusieurs paramètres : x: une matrice, un DataFrame, un objet dist ou un objet frNN. eps: le rayon de recherche epsilon (\\(\\epsilon\\)). minPts: le nombre de points minimum requis pour que chaque point soit considéré comme un point central. weights: un vecteur numérique optionnel pour pondérer les points. Pour illustrer le fonctionnement de la méthode DBSCAN, nous avons extrait les accidents d’un jeu de données sur les incidents de sécurité publique survenus sur le territoire de la Ville de Sherbrooke de juillet 2019 à juin 2022 (figure 4.8). Figure 4.8: Accidents survenus entre juillet 2019 et juin 2022, Ville de Sherbrooke Dans le code ci-dessous, nous réalisons trois étapes préalables au calcul de DBSCAN : Importation des accidents. Récupération des coordonnées (\\(x\\), \\(y\\)) des accidents dans une matrice. Construction du graphique à partir de la distance au quatrième point le plus proche. Nous n’observons pas de décrochement particulier dans la courbe de la figure 4.9. Par conséquent, nous pourrions tout aussi bien retenir comme rayon pour epsilon une distance euclidienne de 250, 500, 1000 ou 1500 mètres. library(sf) library(tmap) library(dbscan) library(ggplot2) ## Importation des accidents Accidents.sf &lt;- st_read(dsn = &quot;data/chap04/DataAccidentsSherb.shp&quot;, quiet=TRUE) ## Coordonnées géographiques xy &lt;- st_coordinates(Accidents.sf) ## Graphique pour la distance au quatrième voisin le plus proche DistKplusproche &lt;- kNNdist(xy, k = 4) DistKplusproche &lt;- as.data.frame(sort(DistKplusproche, decreasing = FALSE)) names(DistKplusproche) &lt;- &quot;distance&quot; ggplot(data = DistKplusproche)+ geom_path(aes(x = 1:nrow(DistKplusproche), y = distance), size=1)+ labs(x = &quot;Points triés par ordre croissant selon la distance&quot;, y = &quot;Distance au quatrième point le plus proche&quot;)+ geom_hline(yintercept=250, color = &quot;#08306b&quot;, linetype=&quot;dashed&quot;, size=1)+ geom_hline(yintercept=500, color = &quot;#00441b&quot;, linetype=&quot;dashed&quot;, size=1)+ geom_hline(yintercept=1000, color = &quot;#67000d&quot;, linetype=&quot;dashed&quot;, size=1)+ geom_hline(yintercept=1500, color = &quot;#3f007d&quot;, linetype=&quot;dashed&quot;, size=1) Figure 4.9: Optimisation de la valeur d’epsilon pour les accidents Appliquons la méthode DBSCAN avec un minimum de quatre points et les quatre valeurs de distance euclidienne. set.seed(123456789) ## DBSCAN avec les quatre distances dbscan250 &lt;- dbscan(xy, eps = 250, minPts = 4) dbscan500 &lt;- dbscan(xy, eps = 500, minPts = 4) dbscan1000 &lt;- dbscan(xy, eps = 1000, minPts = 4) dbscan1500 &lt;- dbscan(xy, eps = 1500, minPts = 4) ## Affichage des résultats dbscan250 ## DBSCAN clustering for 1106 objects. ## Parameters: eps = 250, minPts = 4 ## Using euclidean distances and borderpoints = TRUE ## The clustering contains 45 cluster(s) and 353 noise points. ## ## 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## 353 4 6 7 5 15 5 4 22 7 5 5 19 295 4 18 5 4 7 8 ## 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 ## 4 6 49 4 11 4 4 41 5 4 8 31 25 10 6 23 4 5 18 15 ## 40 41 42 43 44 45 ## 6 4 6 4 7 4 ## ## Available fields: cluster, eps, minPts, dist, borderPoints dbscan500 ## DBSCAN clustering for 1106 objects. ## Parameters: eps = 500, minPts = 4 ## Using euclidean distances and borderpoints = TRUE ## The clustering contains 33 cluster(s) and 143 noise points. ## ## 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## 143 7 6 14 4 5 5 4 6 734 18 8 6 6 5 9 16 9 5 4 ## 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ## 23 5 3 4 6 5 4 9 6 9 4 4 4 6 ## ## Available fields: cluster, eps, minPts, dist, borderPoints dbscan1000 ## DBSCAN clustering for 1106 objects. ## Parameters: eps = 1000, minPts = 4 ## Using euclidean distances and borderpoints = TRUE ## The clustering contains 10 cluster(s) and 42 noise points. ## ## 0 1 2 3 4 5 6 7 8 9 10 ## 42 8 6 37 962 8 4 6 5 12 16 ## ## Available fields: cluster, eps, minPts, dist, borderPoints dbscan1500 ## DBSCAN clustering for 1106 objects. ## Parameters: eps = 1500, minPts = 4 ## Using euclidean distances and borderpoints = TRUE ## The clustering contains 3 cluster(s) and 7 noise points. ## ## 0 1 2 3 ## 7 1047 12 40 ## ## Available fields: cluster, eps, minPts, dist, borderPoints Pour les 1106 accidents du jeu de données, les résultats des quatre DBSCAN ci-dessus sont les suivants : Avec \\(\\epsilon=250\\), 45 agrégats et 353 points aberrants (bruit). Avec \\(\\epsilon=500\\), 33 agrégats et 143 points aberrants. Avec \\(\\epsilon=1000\\), 10 agrégats et 42 points aberrants. Avec \\(\\epsilon=1500\\), 3 agrégats et 7 points aberrants. Pour les n points du jeu de données, l’appartenance à un agrégat est enregistrée dans un vecteur numérique avec des valeurs de 0 à k agrégats (ResultatDbscan$cluster). Notez que la valeur de 0 est attribuée aux points aberrants. Avec ce vecteur, nous enregistrons les résultats dans un nouveau champ de la couche de points sf. ## Enregistrement des résultats de DBSCAN dans la couche de points sf Accidents.sf$Dbscan250 &lt;- as.character(dbscan250$cluster) Accidents.sf$Dbscan500 &lt;- as.character(dbscan500$cluster) Accidents.sf$Dbscan1000 &lt;- as.character(dbscan1000$cluster) Accidents.sf$Dbscan1500 &lt;- as.character(dbscan1500$cluster) Accidents.sf$Dbscan250 &lt;- ifelse(nchar(Accidents.sf$Dbscan250) == 1, paste0(&quot;0&quot;, Accidents.sf$Dbscan250), Accidents.sf$Dbscan250) Accidents.sf$Dbscan500 &lt;- ifelse(nchar(Accidents.sf$Dbscan500) == 1, paste0(&quot;0&quot;, Accidents.sf$Dbscan500), Accidents.sf$Dbscan500) Accidents.sf$Dbscan1000 &lt;- ifelse(nchar(Accidents.sf$Dbscan1000) == 1, paste0(&quot;0&quot;, Accidents.sf$Dbscan1000), Accidents.sf$Dbscan1000) Accidents.sf$Dbscan1500 &lt;- ifelse(nchar(Accidents.sf$Dbscan1500) == 1, paste0(&quot;0&quot;, Accidents.sf$Dbscan1500), Accidents.sf$Dbscan1500) Nous cartographions finalement les résultats pour les quatre solutions. tmap_mode(&quot;plot&quot;) tm_shape(Accidents.sf)+tm_dots(col=&quot;Dbscan250&quot;, title = &quot;DBSCAN 250&quot;, size = .5) tm_shape(Accidents.sf)+tm_dots(col=&quot;Dbscan500&quot;, title = &quot;DBSCAN 500&quot;, size = .5) tm_shape(Accidents.sf)+tm_dots(col=&quot;Dbscan1000&quot;, title = &quot;DBSCAN 1000&quot;, size = .5) tm_shape(Accidents.sf)+tm_dots(col=&quot;Dbscan1500&quot;, title = &quot;DBSCAN 1500&quot;, size = .5) 4.1.3.2 ST-DBSCAN Pour l’algorithme ST-DBSCAN, nous utilisons une fonction R proposée par Colin Kerouanton. #&#39; Fonction pour l&#39;algorithme st-dbscan #&#39; Source : https://github.com/CKerouanton/ST-DBSCAN_sensitivity #&#39; @param x vecteur pour les coordonnées X des points. #&#39; @param y vecteur pour les coordonnées Y des points. #&#39; @param eps1 valeur de epsilon pour la distance. #&#39; @param eps2 valeur de epsilon pour le temps. #&#39; @param minpts nombre de points minimum pour les points centraux. stdbscan = function (x, y, time, eps1, eps2, minpts) { countmode = 1:length(x) seeds = TRUE data_spatial &lt;- as.matrix(dist(cbind(y, x))) data_temporal &lt;- as.matrix(dist(time)) n &lt;- nrow(data_spatial) classn &lt;- cv &lt;- integer(n) isseed &lt;- logical(n) cn &lt;- integer(1) for (i in 1:n) { if (i %in% countmode) #cat(&quot;Processing point &quot;, i, &quot; of &quot;, n, &quot;.\\n&quot;) unclass &lt;- (1:n)[cv &lt; 1] if (cv[i] == 0) { reachables &lt;- intersect(unclass[data_spatial[i, unclass] &lt;= eps1], unclass[data_temporal[i, unclass] &lt;= eps2]) if (length(reachables) + classn[i] &lt; minpts) cv[i] &lt;- (-1) else { cn &lt;- cn + 1 cv[i] &lt;- cn isseed[i] &lt;- TRUE reachables &lt;- setdiff(reachables, i) unclass &lt;- setdiff(unclass, i) classn[reachables] &lt;- classn[reachables] + 1 while (length(reachables)) { cv[reachables] &lt;- cn ap &lt;- reachables reachables &lt;- integer() for (i2 in seq(along = ap)) { j &lt;- ap[i2] jreachables &lt;- intersect(unclass[data_spatial[j, unclass] &lt;= eps1], unclass[data_temporal[j, unclass] &lt;= eps2]) if (length(jreachables) + classn[j] &gt;= minpts) { isseed[j] &lt;- TRUE cv[jreachables[cv[jreachables] &lt; 0]] &lt;- cn reachables &lt;- union(reachables, jreachables[cv[jreachables] == 0]) } classn[jreachables] &lt;- classn[jreachables] + 1 unclass &lt;- setdiff(unclass, j) } } } } if (!length(unclass)) break } if (any(cv == (-1))) { cv[cv == (-1)] &lt;- 0 } out &lt;- list(cluster = cv, eps1 = eps1, eps2 = eps2, minpts = minpts, density = classn) rm(classn) if (seeds &amp;&amp; cn &gt; 0) { out$isseed &lt;- isseed } class(out) &lt;- &quot;stdbscan&quot; return(out) } Calculons ST-DBSCAN avec une distance spatiale de 1000 mètres et une distance temporelle de 21 jours. Nous obtenons 26 agrégats et 584 points identifiés comme aberrants. ## Importation des accidents Accidents.sf &lt;- st_read(dsn = &quot;data/chap04/DataAccidentsSherb.shp&quot;, quiet=TRUE) ## Coordonnées géographiques xy &lt;- st_coordinates(Accidents.sf) Accidents.sf$x &lt;- xy[,1] Accidents.sf$y &lt;- xy[,2] ## Vérifions que le champ DATEINCIDE est bien au format date str(Accidents.sf$DATEINCIDE) ## Date[1:1106], format: &quot;2021-12-11&quot; &quot;2022-05-16&quot; &quot;2021-08-12&quot; &quot;2019-08-02&quot; &quot;2020-03-02&quot; ... ## Calcul de st-dbscan avec une distance de 1000 mètres et 21 jours Resultats.stdbscan &lt;- stdbscan(x = Accidents.sf$x, y = Accidents.sf$y, time = Accidents.sf$DATEINCIDE, eps1 = 1000, eps2 = 21, minpts = 4) ## Enregistrement des résultats de ST-DBSCAN dans la couche de points sf Accidents.sf$stdbscan.1000_21 &lt;- as.character(Resultats.stdbscan$cluster) Accidents.sf$stdbscan.1000_21 &lt;- ifelse(nchar(Accidents.sf$stdbscan.1000_21) == 1, paste0(&quot;0&quot;, Accidents.sf$stdbscan.1000_21), Accidents.sf$stdbscan.1000_21) ## Nombre de points par agrégat table(Accidents.sf$stdbscan.1000_21) ## ## 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 ## 584 4 6 5 4 4 7 7 4 178 156 13 17 4 22 32 6 7 8 8 ## 20 21 22 23 24 25 26 ## 5 6 4 3 5 4 3 Pour faciliter l’analyse des résultats de ST-DBSCAN, nous conseillons de : Construire un tableau récapitulatif pour les agrégats avec le nombre de points, les dates de début et de fin et l’intervalle temporel. Construire un graphique avec les agrégats (axe des y) et la dimension temporelle (axe des x). Cartographier les résultats. Le code ci-dessous génère le tableau récapitulatif. Nous constatons ainsi que les agrégats 9 et 10 incluent respectivement 178 et 156 points avec des intervalles temporels importants (respectivement 251 et 319 jours). library(dplyr) ## Sélection des points appartenant à un agrégat Agregats &lt;- subset(Accidents.sf, Accidents.sf$stdbscan.1000_21 != &quot;00&quot;) ## Conversion de la date au format POSIXct Agregats$dtPOSIXct &lt;- as.POSIXct(Agregats$DATEINCIDE, format = &quot;%Y/%m/%d&quot;) ## Tableau récapitulatif library(&quot;dplyr&quot;) Tableau.stdbscan &lt;- st_drop_geometry(Agregats) %&gt;% group_by(stdbscan.1000_21) %&gt;% summarize(points = n(), date.min = min(DATEINCIDE), date.max = max(DATEINCIDE), intervalle.jours = as.numeric(max(DATEINCIDE)-min(DATEINCIDE))) ## Affichage du tableau print(Tableau.stdbscan, n = nrow(Tableau.stdbscan)) ## # A tibble: 26 × 5 ## stdbscan.1000_21 points date.min date.max intervalle.jours ## &lt;chr&gt; &lt;int&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; ## 1 01 4 2019-08-08 2019-09-04 27 ## 2 02 6 2021-12-15 2022-01-25 41 ## 3 03 5 2019-07-21 2019-08-30 40 ## 4 04 4 2020-11-10 2020-12-12 32 ## 5 05 4 2022-01-08 2022-02-13 36 ## 6 06 7 2021-06-09 2021-07-02 23 ## 7 07 7 2020-06-23 2020-08-07 45 ## 8 08 4 2021-09-30 2021-10-27 27 ## 9 09 178 2019-07-02 2020-03-09 251 ## 10 10 156 2021-03-13 2022-01-26 319 ## 11 11 13 2021-07-24 2021-09-11 49 ## 12 12 17 2021-10-21 2022-01-12 83 ## 13 13 4 2021-06-16 2021-07-07 21 ## 14 14 22 2022-04-11 2022-06-27 77 ## 15 15 32 2020-09-11 2020-12-18 98 ## 16 16 6 2020-01-17 2020-02-08 22 ## 17 17 7 2022-05-07 2022-05-30 23 ## 18 18 8 2021-04-01 2021-05-27 56 ## 19 19 8 2020-07-15 2020-09-11 58 ## 20 20 5 2019-07-05 2019-07-31 26 ## 21 21 6 2022-01-15 2022-03-02 46 ## 22 22 4 2020-06-17 2020-06-30 13 ## 23 23 3 2022-03-12 2022-03-18 6 ## 24 24 5 2021-06-29 2021-07-21 22 ## 25 25 4 2021-09-20 2021-10-25 35 ## 26 26 3 2021-04-13 2021-04-26 13 La figure 4.10 présente les points et l’étendue temporelle de chaque agrégat. ## Construction du graphique ggplot(Agregats) + geom_point(aes(x = dtPOSIXct, y = stdbscan.1000_21, color = stdbscan.1000_21), show.legend = FALSE) + scale_x_datetime(date_labels = &quot;%Y/%m&quot;)+ labs(x= &quot;Temps&quot;, y= &quot;Identifiant de l&#39;agrégat&quot;, title = &quot;ST-DBSCAN avec Esp1 = 1000, Esp2 = 21 et MinPts = 4&quot;) Figure 4.10: Intervalles temporels des agrégats ST-DBSCAN La cartographie des agrégats est présentée à la figure 4.11 avec en noir les points aberrants. ## Création de deux couches : l&#39;une pour les agrégats, l&#39;autre pour les points aberrants stdbcan.Agregats &lt;- subset(Accidents.sf, Accidents.sf$stdbscan.1000_21 != &quot;00&quot;) stdbcan.Bruit &lt;- subset(Accidents.sf, Accidents.sf$stdbscan.1000_21 == &quot;00&quot;) ## Cartographie tmap_mode(&quot;plot&quot;) tm_shape(Arrondiss)+tm_fill(col=&quot;#f7f7f7&quot;)+tm_borders(col=&quot;black&quot;)+ tm_shape(stdbcan.Bruit)+ tm_dots(shape = 21, col=&quot;black&quot;, size=.2)+ tm_shape(stdbcan.Agregats)+ tm_dots(shape = 21, col=&quot;stdbscan.1000_21&quot;, size=.2, title = &quot;Agrégat&quot;)+ tm_layout(frame = FALSE, legend.position = c(&quot;center&quot;, &quot;bottom&quot;), legend.text.size = .85, legend.outside = TRUE) Figure 4.11: Agrégats identifiés avec ST-DBSCAN References "],["sect042.html", "4.2 Méthodes de balayage de Kulldorff", " 4.2 Méthodes de balayage de Kulldorff 4.2.1 Objectifs de la méthode, types d’analyses, de modèles et d’agrégats 4.2.2 Principes de base de la méthode 4.2.2.1 Type de balayage (cercle ou ellipse) 4.2.2.2 Variable de contrôle 4.2.3 Mise en oeuvre dans R 4.2.3.1 Agrégats temporels, spatiaux et spatio-temporels 4.2.3.2 Introduction de variables de contrôle 4.2.3.3 Exploration d’autre types de modèles "],["sect044.html", "4.3 Quiz de révision du chapitre", " 4.3 Quiz de révision du chapitre L’algorithme DBSCAN est basé sur : Relisez au besoin la section 4.1.1.1. La distance entre les points La densité des points Avec l’algorithme DBSCAN, vous devez spécifier le nombre de groupes (agrégats). Relisez au besoin la section 4.1.1.1. Vrai Faux Quels sont les trois types de points identifiés par DBSCAN? Relisez au besoin l’encadré à la section 4.1.1.1. Points centraux Points médians Points frontières Points aberrants Quels sont les deux paramètres de l’algorithme DBSCAN? Relisez au besoin la section 4.1.1.1. Le nombre de points minimum pour identifier les points centraux (MinPts) Le rayon de recherche (epsilon) La distance standard (DS) Plus la valeur d’epsilon est grande, plus le nombre d’agrégats diminue. Relisez au besoin la section 4.1.1.2. Vrai Faux Quels sont les trois paramètres de l’algorithme ST-DBSCAN? Relisez au besoin la section 4.1.2. Le nombre de points minimum pour identifier les points centraux (MinPts) Le rayon de recherche pour la proximité spatiale (eps1) Le rayon de recherche pour la proximité temporelle (eps2) La région d’étude Vérifier votre résultat "],["sect045.html", "4.4 Exercices de révision", " 4.4 Exercices de révision Exercice 1. Application de l’algorithme DBSCAN sur les accidents impliquant des personnes à vélo sur l’île de Montréal (voir la section 4.1.3.1). Ces données ouvertes sur les collisions routières et leur documentation sont disponibles au lien suivant. library(sf) library(tmap) library(dbscan) library(ggplot2) ## Importation des données Collissions &lt;- st_read(dsn = &quot;data/chap04/collisions.gpkg&quot;, layer = &quot;CollisionsRoutieres&quot;, quiet = TRUE) ## Collisions impliquant au moins une personne à vélo en 2020 et 2021 Coll.Velo &lt;- subset(Collissions, Collissions$NB_VICTIMES_VELO &gt; 0 &amp; Collissions$AN %in% c(2020, 2021)) ## Coordonnées géographiques xy &lt;- st_coordinates(Coll.Velo) ## Graphique pour la distance au quatrième voisin le plus proche DistKplusproche &lt;- kNNdist(xy, k = 4) DistKplusproche &lt;- as.data.frame(sort(DistKplusproche, decreasing = FALSE)) names(DistKplusproche) &lt;- &quot;distance&quot; ggplot(à compléter)+ geom_path(à compléter)+ labs(à compléter)+ geom_hline(yintercept=250, color = &quot;#08306b&quot;, linetype=&quot;dashed&quot;, size=1)+ geom_hline(yintercept=500, color = &quot;#00441b&quot;, linetype=&quot;dashed&quot;, size=1)+ geom_hline(yintercept=1000, color = &quot;#67000d&quot;, linetype=&quot;dashed&quot;, size=1) ## DBSCAN avec les quatre distances set.seed(123456789) dbscan250 &lt;- à compléter dbscan500 &lt;- à compléter dbscan1000 &lt;- à compléter ## Affichage des résultats dbscan250 dbscan500 dbscan1000 ## Enregistrement dans la couche de points sf Coll.Velo Coll.Velo$dbscan250 &lt;- à compléter Coll.Velo$dbscan500 &lt;- à compléter Coll.Velo$dbscan1000 &lt;- à compléter Coll.Velo$dbscan250 &lt;- ifelse(nchar(Coll.Velo$dbscan250) == 1, paste0(&quot;0&quot;, Coll.Velo$dbscan250), Coll.Velo$dbscan250) Coll.Velo$dbscan500 &lt;- ifelse(nchar(Coll.Velo$dbscan500) == 1, paste0(&quot;0&quot;, Coll.Velo$dbscan500), Coll.Velo$dbscan500) Coll.Velo$dbscan1000 &lt;- ifelse(nchar(Coll.Velo$dbscan1000) == 1, paste0(&quot;0&quot;, Coll.Velo$dbscan1000), Coll.Velo$dbscan1000) # Extraction des agrégats Agregats.dbscan250 &lt;- subset(Coll.Velo, dbscan250 != &quot;00&quot;) Agregats.dbscan500 &lt;- subset(Coll.Velo, dbscan500 != &quot;00&quot;) Agregats.dbscan1000 &lt;- subset(Coll.Velo, dbscan1000 != &quot;00&quot;) ## Cartographie tmap_mode(&quot;plot&quot;) à compléter Correction à la section 10.4.1. Exercice 2. Avec le même jeu de données, réaliser un ST-DBSCAN avec les paramètres suivants : distance spatiale de 500 mètres, distance temporelle de 30 jours et quatre points minimum (voir la section 4.1.3.2). library(sf) library(tmap) library(dbscan) library(ggplot2) ## Importation des données Collissions &lt;- st_read(dsn = &quot;data/chap04/collisions.gpkg&quot;, layer = &quot;CollisionsRoutieres&quot;, quiet = TRUE) ## Collisions impliquant au moins une personne à vélo en 2020 et 2021 Coll.Velo &lt;- subset(Collissions, Collissions$NB_VICTIMES_VELO &gt; 0 &amp; Collissions$AN %in% c(2020, 2021)) ## Coordonnées géographiques xy &lt;- st_coordinates(Coll.Velo) Coll.Velo$x &lt;- xy[,1] Coll.Velo$y &lt;- xy[,2] ## Conversion du champ DT_ACCDN au format Date Coll.Velo$DT_ACCDN &lt;- as.Date(Coll.Velo$DT_ACCDN) ## ST-DBSCAN avec eps1 = 500, esp2 = 30 et minpts = 4 Resultats.stdbscan &lt;- stdbscan(À compléter) ## Enregistrement des résultats ST-DBSCAN dans la couche de points sf Coll.Velo$stdbscan &lt;- as.character(Resultats.stdbscan$cluster) Coll.Velo$stdbscan &lt;- ifelse(nchar(Coll.Velo$stdbscan) == 1, paste0(&quot;0&quot;, Coll.Velo$stdbscan), Coll.Velo$stdbscan) ## Nombre de points par agrégat avec la fonction table table(Coll.Velo$stdbscan) ## Sélection des points appartenant à un agrégat avec la fonction subset Agregats &lt;- subset(Coll.Velo, stdbscan != &quot;00&quot;) ## Conversion de la date au format POSIXct Agregats$dtPOSIXct &lt;- as.POSIXct(Agregats$DT_ACCDN, format = &quot;%Y/%m/%d&quot;) ## Tableau récapitulatif library(&quot;dplyr&quot;) Tableau.stdbscan &lt;- À compléter ## Affichage du tableau print(Tableau.stdbscan, n = nrow(Tableau.stdbscan)) ## Construction du graphique À compléter ## Création d&#39;une couche pour les agrégats stdbcan.Agregats &lt;- subset(Coll.Velo, stdbscan != &quot;00&quot;) ## Cartographie À compléter Correction à la section 10.4.2. "],["chap05.html", "Chapitre 5 Mesures d’accessibilité spatiale selon les différents de modes de transport", " Chapitre 5 Mesures d’accessibilité spatiale selon les différents de modes de transport Dans ce chapitre, nous voyons comment construire un réseau multimode (voiture, marche, vélo, transport en commun) pour calculer différentes mesures d’accessibilité dans R. Aussi, nous discutons de la notion d’accessibilité à un service, notamment des cinq dimensions identifiées par Penchansky et Thomas (1981), de l’accessibilité réelle versus l’accessibilité potentielle et de l’accessibilité spatiale versus l’accessibilité aspatiale (Luo et Wang 2003). Dans ce chapitre, nous utilisons les packages suivants : Pour importer et manipuler des fichiers géographiques : sf pour importer et manipuler des données vectorielles. terra pour importer et manipuler des données matricielles. Pour construire des cartes et des graphiques : tmap est certainement le meilleur package pour la cartographie. ggplot2 pour construire des graphiques. Pour construire un réseau : osmextract pour extraire des fichiers OpenStreetMap. R5R pour calculer des trajets et des matrices origines-destinations selon différents modes de transport. Pour manipuler les données : dplyr pour calculer des moyennes pondérées. References "],["sect051.html", "5.1 Notions relatives à l’analyse de réseau", " 5.1 Notions relatives à l’analyse de réseau 5.1.1 Définition d’un réseau Un réseau est un ensemble de lignes connectées par des nœuds – voies ferrées, voies routières, canalisations d’eau ou de gaz, fleuves et affluents drainant une région, etc. – reliant un territoire (figure 5.1). Pour un réseau routier, l’information sémantique rattachée tant aux lignes (sens de circulation, vitesse autorisée, rues piétonnières, pistes cyclables, etc.) qu’aux nœuds (types d’intersection, autorisation de virage à gauche, etc.) est utilisée pour modéliser un réseau. Figure 5.1: Réseau : un ensemble de lignes connectées par des nœuds 5.1.2 Principaux problèmes résolus en analyse de réseau L’analyse de réseau permet de résoudre trois principaux problèmes (figure 5.2) : Trouver le trajet le plus court ou le plus rapide entre deux points, basé sur l’algorithme de Dijkstra (1959). Trouver la route optimale comprenant plusieurs arrêts (problème du voyageur de commerce). Définir des zones de desserte autour d’une origine, basé aussi sur l’algorithme de Dijkstra. Figure 5.2: Trois principaux problèmes résolus en analyse de réseau À cela s’ajoutent quatre autres problèmes : Trouver les k services les plus proches à partir d’une origine (figure 5.3.a). Construire une matrice de distance origines-destinations (figure 5.3.b), dont l’intérêt principal est de permettre par la suite le calcul n’importe quelle mesure d’accessibilité. Résoudre le problème de tournées de véhicules. L’objectif est d’optimiser les tournées d’une flotte de véhicules en fonction des ressources disponibles, de la localisation des clients, des lieux de dépôt de marchandises, etc. Réaliser un modèle localisation-affectation dont l’objectif est d’optimiser la localisation d’un ou plusieurs nouveaux équipements en fonction de la demande, et ce, en minimisant la distance agrégée entre les points d’offre et de demande. Par exemple, une région ayant 15 hôpitaux desservant deux millions d’habitants souhaiterait ajouter trois autres établissements. En fonction de l’offre existante (hôpitaux pondérés par le nombre de lits), de la distribution spatiale de la population et de la localisation des sites potentiels des trois hôpitaux, il s’agit de choisir ceux qui minimisent la distance entre les points d’offre et de demande. Figure 5.3: Autres problèmes résolus en analyse de réseau Ces problèmes peuvent être résolus selon différents modes de transport, à savoir le chemin le plus rapide en véhicule motorisé, à pied, en vélo et en transport en commun (figure 5.4). Figure 5.4: Chemin le plus rapide selon différents modes de transport 5.1.3 Analyse de réseau et entités polygonales Vous avez compris que les problèmes présentés plus haut sont réalisés à partir d’entités spatiales ponctuelles. Pour estimer le trajet le plus court ou le plus rapide entre un point et un polygone (un parc urbain par exemple), il faut préalablement le convertir en point. Plusieurs solutions sont envisageables (Apparicio et Séguin 2006) : Calculer le trajet entre le point et le centroïde du parc (figure 5.5.a). Le centroïde est alors rattaché au tronçon de rue le plus proche. Cette solution est peu précise : plus la superficie du polygone est grande, plus l’imprécision augmente. Si le parc a plusieurs entrées, il suffit de les positionner le long du périmètre et de calculer les trajets à partir de ces points. Si le parc n’a pas d’entrée, il convient de positionner des points le long du périmètre, espacés d’une distance prédéterminée (figure 5.5.b). Bien qu’elle soit longue à calculer, cette solution est bien plus précise. Par exemple, avec des points espacés de 20 mètres le long du périmètre du parc, l’erreur maximale est de 10 mètres. Figure 5.5: Méthode pour déterminer le trajet le plus court entre une entité ponctuelle et une entité polygonale Modélisation d’un réseau dans un logiciel SIG (systèmes d’information géographique) Il est aussi possible de construire un réseau dans un logiciel SIG (QGIS et ArcGIS Pro par exemple). Pour une description détaillée de la construction d’un réseau selon différents modes de transport (voiture, marche, vélo et transport en commun) dans un SIG, consultez l’article d’Apparicio et al. (2017). References "],["sect052.html", "5.2 Construction d’un réseau avec R5R", " 5.2 Construction d’un réseau avec R5R Pour construire un réseau pour différents modes de transport dans R, nous utilisons le package R5R (Pereira et al. 2021). Il existe d’autres packages, notamment opentripplanner (Morgan et al. 2019) qui a été largement utilisé ces dernières années. Étant plus rapide, R5R s’impose actuellement comme la solution la plus efficace pour calculer des trajets à travers un réseau de rues selon différents modes de transport. Documentation du package R5R Nous vous conseillons vivement de lire la documentation de R5R sur le site de CRAN, notamment les nombreuses vignettes présentant des exemples d’analyses avec du code R très bien documenté. 5.2.1 Extraction des données spatiales pour R5R Pour construire un réseau multimode, R5R a besoin de trois fichiers qui doivent être localisés dans le même dossier (figure 5.6) : un fichier pbf (Protocolbuffer Binary Format) pour les données d’OpenStreetMap. un ou plusieurs fichiers GTFS (General Transit Feed Specification) pour les flux relatifs aux transports en commun. un fichier GeoTIFF d’élévation. Notez que ce dernier fichier est optionnel. Toutefois, pour calculer des trajets à pied ou à vélo, il est préférable de tenir compte de la pente, surtout dans une ville comme Sherbrooke! Figure 5.6: Trois types de données nécessaires pour modéliser un réseau dans R5R 5.2.1.1 Extraction d’un fichier OpenStreetMap Pour récupérer un fichier OpenStreetMap (OSM, format pbf), nous utilisons deux fonctions du package osmextract (Gilardi et Lovelace 2022) : oe_match pour repérer le fichier OSM pour la région de l’Estrie. oe_download pour télécharger le fichier OSM pour la région de l’Estrie. library(osmextract) ## Identification du fichier OSM (format pbf) pour l&#39;Estrie Estrie = oe_match(place=&quot;Estrie&quot;, provider = &quot;openstreetmap_fr&quot;) ## Téléchargement du fichier OSM (format pbf) pour l&#39;Estrie oe_download( file_url = Estrie$url, file_size = Estrie$file_size, provider = &quot;openstreetmap_fr&quot;, download_directory = &quot;data/chap05/__TempOSM&quot;) Le fichier OSM téléchargé plus haut couvre la région de l’Estrie. À notre connaissance, il n’existe pas de solution pour découper un fichier pbf dans R. Par conséquent, nous récupérons les coordonnées minimales et maximales de l’enveloppe d’une zone tampon de 5000 mètres autour de la couche de la ville de Sherbrooke. Puis, pour découper le fichier pbf, nous utilisons l’outil osmconvert64-0.8.8p.exe. Le code ci-dessous renvoie les quatre coordonnées de l’enveloppe. library(sf) library(tmap) ## Importation du polygone pour la ville de Sherbrooke avec sf Sherbrooke &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Sherbrooke.shp&quot;, quiet=TRUE) ## Création d&#39;une zone tampon de 5 km zone &lt;- st_buffer(Sherbrooke, dist = 5000) ## Changement de la projection en 4326 zone &lt;- st_transform(zone, 4326) ## Création de l&#39;enveloppe autour de la couche enveloppe = st_bbox(zone) ## Visualisation des coordonnées minimales et maximales cat(paste0(&quot;Minimum longitude : &quot;, round(enveloppe[1],4), &quot;\\n Minimum latitude : &quot;, round(enveloppe[2],4), &quot;\\n Maximum longitude : &quot;, round(enveloppe[3],4), &quot;\\n Maximum latitude : &quot;, round(enveloppe[4],4) )) ## Minimum longitude : -72.1537 ## Minimum latitude : 45.2683 ## Maximum longitude : -71.7576 ## Maximum latitude : 45.5554 L’application Osmconvert est disponible pour Windows et Linux. Pour découper un fichier avec les coordonnées latitude/longitude minimales et maximale avec Osmconvert, visionnez la courte vidéo ci-dessous. 5.2.1.2 Construction d’un fichier GeoTIFF pour l’élévation Pour créer un fichier GeoTIFF pour l’élévation, nous utilisons les modèles numériques de terrain (MNT) du ministère des Ressources naturelles et des Forêts. library(terra) ## Couche polygonale sf pour la ville de Sherbrooke # Shapefile pour les régions du Québec au 1/20000 # https://www.donneesquebec.ca/recherche/dataset/decoupages-administratifs/resource/b368d470-71d6-40a2-8457-e4419de2f9c0 Sherbrooke &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Sherbrooke.shp&quot;, quiet=TRUE) Sherbrooke &lt;- st_buffer(Sherbrooke, dist = 5000) ## Feuillets pour les MNT au 1/20000 ## Importation du shapefile des feuillets ## (https://www.donneesquebec.ca/recherche/dataset/modeles-numeriques-d-altitude-a-l-echelle-de-1-20-000/resource/2df157af-74cf-4b53-af9d-1d3ccee0d6e1) Feuillets &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Index_MNT20k.shp&quot;, quiet=TRUE) ### Nous nous assurons que les deux couches ont la même projection, soit EPSG 4326 Sherbrooke &lt;- st_transform(Sherbrooke, st_crs(Feuillets)) ### Sélection des feuillets qui intersectent le polygone de l&#39;Estrie RequeteSpatiale &lt;- st_intersects(Feuillets, Sherbrooke, sparse = FALSE) Feuillets$Intersect &lt;- RequeteSpatiale[, 1] FeuilletsSherbrooke &lt;- subset(Feuillets,Intersect == TRUE) ## Téléchargement des GRIDS ### Création d&#39;un dossier temporaire pour les MNT dir.create(paste0(&quot;data/chap05/AutresDonnees/MNT&quot;)) grids &lt;- FeuilletsSherbrooke$GRID i = 0 for (e in grids) { i = i+1 # Téléchargement Fichier &lt;- substr(e, 88, nchar(e)) Chemin &lt;- &quot;data/chap05/AutresDonnees/MNT&quot; CheminFichier &lt;- paste0(Chemin, &quot;/&quot;, Fichier) download.file(e, destfile = CheminFichier) # Décompression du fichier zip unzip(CheminFichier, exdir = Chemin) } ## Importation des GRIDS avec le package Terra Fichier1 &lt;- substr(grids, 88, 98) Fichier2 &lt;- paste0(substr(tolower(Fichier1), 1, 7), substr(Fichier1, 9, 11)) NomsFichiers &lt;- paste0(&quot;data/chap05/AutresDonnees/MNT/&quot;, Fichier1, &quot;/&quot;, Fichier2) rlist &lt;- list() for(e in NomsFichiers) { print(e) rasterGrid &lt;- terra::rast(e) rlist[[length(rlist)+1]] &lt;- rasterGrid } ## Création d&#39;une mosaique avec les GRIDS rsrc &lt;- terra::sprc(rlist) MosaicSherbrooke &lt;- mosaic(rsrc) MosaicSherbrooke ## Exporter en GeoTIFF terra::writeRaster(MosaicSherbrooke, &quot;data/chap05/_DataReseau/Elevation.tif&quot;, filetype = &quot;GTiff&quot;, overwrite = TRUE) ## Suppression des GRIDS dossier &lt;- &quot;data/chap05/AutresDonnees/MNT/&quot; f &lt;- list.files(dossier, include.dirs = FALSE, full.names = TRUE, recursive = TRUE) file.remove(f) d &lt;- list.dirs(dossier) unlink(d, recursive = TRUE) Le fichier d’élévation ainsi construit est présenté à la figure 5.7. MosaicSherbrooke &lt;- terra::rast(&quot;data/chap05/_DataReseau/Elevation.tif&quot;) terra::plot(MosaicSherbrooke) Figure 5.7: Modèle numérique d’élévation au 1/20000 pour la région de Sherbrooke 5.2.1.3 Extraction d’un fichier GTFS Le fichier GTFS pour la Société de Transport de Sherbrooke est disponible à l’adresse suivante : https://gtfs.sts.qc.ca:8443/gtfs/client/GTFS_clients.zip. Pour le télécharger, nous utilisons la fonction download.file. url &lt;- &quot;https://gtfs.sts.qc.ca:8443/gtfs/client/GTFS_clients.zip&quot; destfile &lt;- &quot;data/chap05/_DataReseau/GTFS_clients.zip&quot; download.file(url, destfile) 5.2.2 Construction du réseau avec R5R R5R et JDK Java et allocation de la mémoire Le package R5R utilise la version 11 de la JDK de Java (Java Development Kit). Vous devez préalablement la télécharger et l’installer sur votre ordinateur. Les deux lignes de code ci-dessous permettent de vérifier si elle est bien installée sur votre ordinateur. ## Vérification que la JDK version 11 est bien installée rJava::.jinit() ## [1] 0 rJava::.jcall(&quot;java.lang.System&quot;, &quot;S&quot;, &quot;getProperty&quot;, &quot;java.version&quot;) ## [1] &quot;11.0.19&quot; Il est fortement conseillé d’augmenter la mémoire allouée au processus JAVA, surtout si vous souhaitez calculer des matrices origines-destinations de grande taille. Par exemple, la commande options(java.parameters = \"-Xmx2G\") permet d’augmenter la mémoire disponible pour JAVA à deux gigaoctets. Si votre ordinateur ne manque pas de mémoire vive (16, 32 ou 64 gigaoctets), n’hésitez pas à augmenter ce paramètre (par exemple, options(java.parameters = \"-Xmx8G\")). Nous utilisons la fonction setup_r5 pour construire un réseau multimode à partir des trois fichiers : Le fichier OSM (openstreetmap_fr_sherbrooke.pbf). Le fichier GTFS pour la Société de Transport de Sherbrooke (GTFS_clients.zip). Le fichier d’élévation pour la région de Sherbrooke (Elevation.tif). Notez les paramètres suivants pour la fonction setup_r5 : data_path pour définir le dossier dans lequel sont présents les trois fichiers. elevation = \"TOBLER\" pour utiliser la fonction d’impédance de Tobler pour la marche et le vélo qui prend en compte la pente. overwrite = FALSE pour ne pas écraser le réseau s’il est déjà construit. La construction d’un réseau peut être très longue dépendamment de la taille des trois fichiers (OSM, GTFS, élévation). Par conséquent, n’oubliez pas de spécifier cette option si votre réseau a été construit. library(r5r) ## Allocation de la mémoire pour Java options(java.parameters = &quot;-Xmx2G&quot;) ## Construction du réseau R5R r5r_core &lt;- setup_r5(data_path = &quot;data/chap05/_DataReseau/&quot;, elevation = &quot;TOBLER&quot;, verbose = FALSE, overwrite = FALSE) La fonction setup_r5 a créé deux nouveaux fichiers (network.dat et network_settings.json) qui sont utilisés par le package r5r pour les analyses de réseau. ## Liste des fichiers dans le dossier list.files(&quot;data/chap05/_DataReseau/&quot;) ## [1] &quot;Elevation.tif&quot; ## [2] &quot;GTFS_clients.zip&quot; ## [3] &quot;network.dat&quot; ## [4] &quot;network_settings.json&quot; ## [5] &quot;openstreetmap_fr_sherbrooke.pbf&quot; ## [6] &quot;openstreetmap_fr_sherbrooke.pbf.mapdb&quot; ## [7] &quot;openstreetmap_fr_sherbrooke.pbf.mapdb.p&quot; 5.2.3 Calcul d’itinéraires avec R5R selon le mode de transport Pour calculer un trajet, nous utilisons la fonction detailed_itineraries dont les paramètres sont décrits dans l’encadré ci-dessous. Paramètres de la fonction detailed_itineraries r5r_core: le réseau créé avec la fonction setup_r5() décrite plus haut. origins: un point sf projeté en WGS84 ou un data.frame comprenant les colonnes id, lon et lat. destinations: un point sf projeté en WGS84 ou un data.frame comprenant les colonnes id, lon et lat. mode: un vecteur de type caractères définissant les modes de transport dont les principaux sont : \"WALK\" pour la marche. \"BICYCLE\" pour le vélo. \"CAR\" pour l’automobile. \"c(\"WALK\",TRANSIT\") pour la marche et le transport en commun. departure_datetime: un objet POSIXct définissant la date et l’heure de départ à utiliser si vous souhaitez calculer un trajet en transport en commun. max_walk_time=inf: un nombre entier définissant le temps de marche maximal en minutes pour chaque segment du trajet. La valeur par défaut est sans limite (Inf). max_bike_time=Inf: un nombre entier définissant le temps maximal à vélo en minutes. max_car_time=Inf: un nombre entier définissant le temps maximal en automobile en minutes. max_trip_duration=120: un nombre entier définissant le temps maximal du trajet qui est fixé à 120 minutes par défaut. Par conséquent, tout trajet d’une durée supérieure à ce seuil ne sera pas calculé. walk_speed = 3.6: une valeur numérique définissant la vitesse moyenne de marche qui est fixée par défaut à 3,6 km/h. Ce seuil est très conservateur et pourrait être augmenté à 4,5 km/h. bike_speed = 12: une valeur numérique définissant la vitesse moyenne à vélo qui est fixée par défaut à 12 km/h. Ce seuil est aussi très conservateur et pourrait être augmenté à 15 ou 16 km/h. max_lts = 2: un nombre entier de 1 à 4 qui indique le niveau de stress lié à la circulation que les cyclistes sont prêts à tolérer. Une valeur de 1 signifie que les cyclistes n’emprunteront que les rues les plus calmes, tandis qu’une valeur de 4 indique que les cyclistes peuvent emprunter n’importe quelle route. max_lts = 1: tolérable pour les enfants. max_lts = 2: tolérable pour la population adulte en général. max_lts = 3: tolérable pour les cyclistes enthousiastes et confiants. max_lts = 4: tolérable uniquement pour les cyclistes intrépides. drop_geometry = FALSE: si ce paramètre est fixé à TRUE, la géométrie du trajet ne sera pas incluse. 5.2.3.1 Calcul d’itinéraires selon les modes de transport actif Dans un premier temps, nous calculons des trajets à vélo entre les deux localisations suivantes : Le point Pt.W situé à l’intersection de la rue Wellington et de la Côte de l’Acadie (45,38947; -71,88393). Le point Pt.D situé à l’intersection des rues Darche et Dorval (45,38353; -71,89169). ## Création d&#39;une couche sf avec les deux points Pts &lt;- data.frame(id = c(&quot;Rue Wellington S.&quot;, &quot;Rue Darche&quot;), lat = c( 45.38947, 45.38353), lon = c(-71.88393, -71.89169) ) Pts &lt;- st_as_sf(Pts, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) Pt.W &lt;- Pts[1,] Pt.D &lt;- Pts[2,] ## Trajets en vélo velo.1 &lt;- detailed_itineraries(r5r_core = r5r_core, origins = Pt.W, destinations = Pt.D, mode = &quot;BICYCLE&quot;, # Vélo bike_speed = 12, shortest_path = FALSE, drop_geometry = FALSE) velo.2 &lt;- detailed_itineraries(r5r_core = r5r_core, origins = Pt.D, destinations = Pt.W, mode = &quot;BICYCLE&quot;, # Vélo bike_speed = 12, shortest_path = FALSE, drop_geometry = FALSE) velo.1 ## Simple feature collection with 1 feature and 16 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -71.89191 ymin: 45.3835 xmax: -71.88387 ymax: 45.38955 ## Geodetic CRS: WGS 84 ## from_id from_lat from_lon to_id to_lat to_lon option ## 1 Rue Wellington S. 45.38947 -71.88393 Rue Darche 45.38353 -71.89169 1 ## departure_time total_duration total_distance segment mode segment_duration ## 1 14:48:16 8.1 1215 1 BICYCLE 8.1 ## wait distance route geometry ## 1 0 1215 LINESTRING (-71.88396 45.38... velo.2 ## Simple feature collection with 1 feature and 16 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -71.89191 ymin: 45.3835 xmax: -71.88387 ymax: 45.38955 ## Geodetic CRS: WGS 84 ## from_id from_lat from_lon to_id to_lat to_lon option ## 1 Rue Darche 45.38353 -71.89169 Rue Wellington S. 45.38947 -71.88393 1 ## departure_time total_duration total_distance segment mode segment_duration ## 1 14:48:17 6.5 1215 1 BICYCLE 6.5 ## wait distance route geometry ## 1 0 1215 LINESTRING (-71.89191 45.38... Avec la fonction detailed_itineraries, les durées sont estimées à respectivement six et huit minutes (figure 5.8). Bien que le chemin emprunté soit le même, cet écart s’explique par la Côte de l’Acadie, soit l’un des tronçons de rue les plus pentues de la ville de Sherbrooke. Pour visualiser les trajets, nous utilisons le package tmap en mode view. Notez qu’un clic sur le trajet fait apparaître une fenêtre surgissante. library(tmap) # Cartographie des trajets avec tmap tmap_mode(&quot;view&quot;) Carte1 &lt;- tm_shape(velo.1)+ tm_lines(col=&quot;black&quot;, lwd = 2, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;))+ tm_shape(Pt.W)+tm_dots(col=&quot;green&quot;, size = .15)+ tm_shape(Pt.D)+tm_dots(col=&quot;red&quot;, size = .15) Carte2 &lt;- tm_shape(velo.2)+ tm_lines(col=&quot;black&quot;, lwd = 2, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;))+ tm_shape(Pt.D)+tm_dots(col=&quot;green&quot;, size = .15)+ tm_shape(Pt.W)+tm_dots(col=&quot;red&quot;, size = .15) tmap_arrange(Carte1, Carte2) Figure 5.8: Trajets à vélo entre les deux destinations Dans un second temps, nous calculons les trajets à pied avec les deux mêmes localisations qui sont estimées à 16 et 21 minutes (figure 5.9). marche.1 &lt;- detailed_itineraries(r5r_core = r5r_core, origins = Pt.W, destinations = Pt.D, mode = &quot;WALK&quot;, # Marche walk_speed = 4.5, # par défaut 3.6 shortest_path = FALSE, drop_geometry = FALSE) marche.2 &lt;- detailed_itineraries(r5r_core = r5r_core, origins = Pt.D, destinations = Pt.W, mode = &quot;WALK&quot;, # Marche walk_speed = 4.5, shortest_path = FALSE, drop_geometry = FALSE) # Cartographie des trajets avec tmap Carte3 &lt;- tm_shape(marche.1)+ tm_lines(col=&quot;black&quot;, lwd = 2, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;))+ tm_shape(Pt.W)+tm_dots(col=&quot;green&quot;, size = .15)+ tm_shape(Pt.D)+tm_dots(col=&quot;red&quot;, size = .15) Carte4 &lt;- tm_shape(marche.2)+ tm_lines(col=&quot;black&quot;, lwd = 2, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;))+ tm_shape(Pt.D)+tm_dots(col=&quot;green&quot;, size = .15)+ tm_shape(Pt.W)+tm_dots(col=&quot;red&quot;, size = .15) tmap_arrange(Carte3, Carte4) Figure 5.9: Trajets à pied entre les deux destinations 5.2.3.2 Calcul d’itinéraires en automobile Nous calculons ici l’itinéraire entre le campus principal de l’Université de Sherbrooke et une localisation (45,4220308; -71,881828). Nous fixons alors mode = \"CAR\" pour la fonction detailed_itineraries. Les trajets sont respectivement estimés à 21,7 et 18,8 minutes à destination et au départ du campus principal. UDeS &lt;- data.frame(id = &quot;Campus principal&quot;, lon = -71.929526, lat = 45.378017) UDeS &lt;- st_as_sf(UDeS, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) Point &lt;- data.frame(id = &quot;Départ&quot;, lon = -71.881828, lat = 45.4220308) Point &lt;- st_as_sf(Point, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) Auto.Aller &lt;- detailed_itineraries(r5r_core = r5r_core, origins = Point, destinations = UDeS, mode = &quot;CAR&quot;, # Automobile shortest_path = FALSE, drop_geometry = FALSE) Auto.Retour &lt;- detailed_itineraries(r5r_core = r5r_core, origins = UDeS, destinations = Point, mode = &quot;CAR&quot;, # Automobile shortest_path = FALSE, drop_geometry = FALSE) Auto.Aller ## Simple feature collection with 1 feature and 16 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -71.93373 ymin: 45.37722 xmax: -71.88129 ymax: 45.42365 ## Geodetic CRS: WGS 84 ## from_id from_lat from_lon to_id to_lat to_lon option ## 1 Départ 45.42203 -71.88183 Campus principal 45.37802 -71.92953 1 ## departure_time total_duration total_distance segment mode segment_duration ## 1 14:48:17 21.7 11352 1 CAR 21.7 ## wait distance route geometry ## 1 0 11352 LINESTRING (-71.88129 45.42... Auto.Retour ## Simple feature collection with 1 feature and 16 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -71.93312 ymin: 45.37759 xmax: -71.87607 ymax: 45.42234 ## Geodetic CRS: WGS 84 ## from_id from_lat from_lon to_id to_lat to_lon option ## 1 Campus principal 45.37802 -71.92953 Départ 45.42203 -71.88183 1 ## departure_time total_duration total_distance segment mode segment_duration ## 1 14:48:17 18.8 9685 1 CAR 18.8 ## wait distance route geometry ## 1 0 9685 LINESTRING (-71.92952 45.37... Bien entendu, les deux trajets sont différents en raison des sens de circulation (figure 5.10). # Cartographie des trajets avec tmap Carte1 &lt;- tm_shape(Auto.Aller)+ tm_lines(col=&quot;black&quot;, lwd = 2, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;))+ tm_shape(Point)+tm_dots(col=&quot;green&quot;, size = .15)+ tm_shape(UDeS)+tm_dots(col=&quot;red&quot;, size = .15) Carte2 &lt;- tm_shape(Auto.Retour)+ tm_lines(col=&quot;black&quot;, lwd = 2, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;))+ tm_shape(Point)+tm_dots(col=&quot;red&quot;, size = .15)+ tm_shape(UDeS)+tm_dots(col=&quot;green&quot;, size = .15) # Figure avec les deux cartes tmap_arrange(Carte1, Carte2) Figure 5.10: Trajets en voiture entre les deux destinations 5.2.3.3 Calcul d’itinéraires en transport en commun Pour le calcul d’itinéraires en transport en commun, nous devons fixer une heure de départ et un temps de marche maximal pour chaque segment du trajet réalisé à pied, soit : du domicile à l’arrêt de bus le plus proche; entre deux arrêts de bus de lignes différentes; du dernier arrêt de bus à la destination finale. Dans le code ci-dessous, nous fixons les heures de départ et d’arrivée à 8 h et à 18 h pour le 4 mai 2023 et un temps de marche maximal de 20 minutes. ### Définition de la journée et de l&#39;heure de départ dateheure.matin &lt;- as.POSIXct(&quot;04-05-2023 08:00:00&quot;, format = &quot;%d-%m-%Y %H:%M:%S&quot;) dateheure.soir &lt;- as.POSIXct(&quot;04-05-2023 18:00:00&quot;, format = &quot;%d-%m-%Y %H:%M:%S&quot;) ### Définition du temps de marche maximal minutes_marches_max &lt;- 20 Toujours avec la fonction detailed_itineraries, nous modifions les paramètres comme suit : mode = c(\"WALK\", \"TRANSIT\") pour le transport en commun. walk_speed = 4.5 pour une vitesse moyenne à la marche de 4,5 km/h. departure_datetime = dateheure.matin pour un départ le 4 mai 2023 à 8 h. departure_datetime = dateheure.soir pour un départ le 4 mai 2023 à 18 h. max_walk_time = minutes_marches_max pour un temps maximal de marche de 20 minutes. TC.Aller &lt;- detailed_itineraries(r5r_core = r5r_core, origins = Point, destinations = UDeS, mode = c(&quot;WALK&quot;, &quot;TRANSIT&quot;), max_walk_time = minutes_marches_max, walk_speed = 4.5, departure_datetime = dateheure.matin, shortest_path = FALSE, drop_geometry = FALSE) TC.Retour &lt;- detailed_itineraries(r5r_core = r5r_core, origins = UDeS, destinations = Point, mode = c(&quot;WALK&quot;, &quot;TRANSIT&quot;), max_walk_time = minutes_marches_max, walk_speed = 4.5, departure_datetime = dateheure.soir, shortest_path = FALSE, drop_geometry = FALSE) La durée du trajet à 8 h vers l’Université de Sherbrooke est estimée à 46,9 minutes avec trois segments : Un premier segment de 17,2 minutes et 1,4 km à pied. Un second de 25 minutes en autobus. Un troisième de 1,8 minute et 127 mètres en autobus. TC.Aller ## Simple feature collection with 3 features and 16 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -71.92952 ymin: 45.37756 xmax: -71.88129 ymax: 45.42204 ## Geodetic CRS: WGS 84 ## from_id from_lat from_lon to_id to_lat to_lon option ## 1 Départ 45.42203 -71.88183 Campus principal 45.37802 -71.92953 1 ## 2 Départ 45.42203 -71.88183 Campus principal 45.37802 -71.92953 1 ## 3 Départ 45.42203 -71.88183 Campus principal 45.37802 -71.92953 1 ## departure_time total_duration total_distance segment mode segment_duration ## 1 08:00:55 46.9 9141 1 WALK 17.2 ## 2 08:00:55 46.9 9141 2 BUS 25.0 ## 3 08:00:55 46.9 9141 3 WALK 1.8 ## wait distance route geometry ## 1 0.0 1419 LINESTRING (-71.88129 45.42... ## 2 2.9 7595 9 LINESTRING (-71.88504 45.41... ## 3 0.0 127 LINESTRING (-71.92938 45.37... La durée du trajet à 18 h au départ de l’Université de Sherbrooke est estimée à 57,6 minutes avec cinq segments dont trois à la marche et deux en bus. TC.Retour ## Simple feature collection with 5 features and 16 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -71.9331 ymin: 45.3776 xmax: -71.88195 ymax: 45.42204 ## Geodetic CRS: WGS 84 ## from_id from_lat from_lon to_id to_lat to_lon option ## 1 Campus principal 45.37802 -71.92953 Départ 45.42203 -71.88183 1 ## 2 Campus principal 45.37802 -71.92953 Départ 45.42203 -71.88183 1 ## 3 Campus principal 45.37802 -71.92953 Départ 45.42203 -71.88183 1 ## 4 Campus principal 45.37802 -71.92953 Départ 45.42203 -71.88183 1 ## 5 Campus principal 45.37802 -71.92953 Départ 45.42203 -71.88183 1 ## departure_time total_duration total_distance segment mode segment_duration ## 1 18:00:55 57.6 11482 1 WALK 1.9 ## 2 18:00:55 57.6 11482 2 BUS 20.0 ## 3 18:00:55 57.6 11482 3 WALK 0.8 ## 4 18:00:55 57.6 11482 4 BUS 14.2 ## 5 18:00:55 57.6 11482 5 WALK 12.3 ## wait distance route geometry ## 1 0.0 150 LINESTRING (-71.92953 45.37... ## 2 4.2 7649 14 LINESTRING (-71.92949 45.37... ## 3 0.0 107 LINESTRING (-71.8939 45.399... ## 4 4.2 2769 3 LINESTRING (-71.89436 45.39... ## 5 0.0 807 LINESTRING (-71.88207 45.41... Carte1 &lt;- tm_shape(TC.Aller)+tm_lines(col=&quot;mode&quot;, lwd = 3, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;, &quot;total_duration&quot;, &quot;total_distance&quot;))+ tm_shape(Point)+tm_dots(col=&quot;green&quot;, size = .15)+ tm_shape(UDeS)+tm_dots(col=&quot;red&quot;, size = .15) Carte2 &lt;- tm_shape(TC.Retour)+tm_lines(col=&quot;mode&quot;, lwd = 3, popup.vars = c(&quot;mode&quot;, &quot;from_id&quot;, &quot;to_id&quot;, &quot;segment_duration&quot;, &quot;distance&quot;, &quot;total_duration&quot;, &quot;total_distance&quot;))+ tm_shape(Point)+tm_dots(col=&quot;red&quot;, size = .15)+ tm_shape(UDeS)+tm_dots(col=&quot;green&quot;, size = .15) tmap_arrange(Carte1, Carte2) Figure 5.11: Trajets en transport en commun 5.2.4 Calcul de matrices OD selon différents modes de transport Pour calculer des matrices origines-destinations selon différents modes de transport, nous utilisons la fonction travel_time_matrix dont les paramètres sont quasi les mêmes que la detailed_itineraries (section 5.2.3). Dans le code ci-dessous, nous importons 284 adresses tirées aléatoirement et les supermarchés présents sur le territoire de la ville de Sherbrooke. ## Importation des couches Adresses &lt;- st_read(dsn = &quot;data/Chap05/AutresDonnees/Commerces.gpkg&quot;, layer = &quot;AdressesAleatoires&quot;, quiet = TRUE) supermarches &lt;- st_read(dsn = &quot;data/Chap05/AutresDonnees/Commerces.gpkg&quot;, layer = &quot;supermarche&quot;, quiet = TRUE) ## Nombre de distances à calculer nO = nrow(Adresses) nD = nrow(supermarches) NOD = nO * nD cat(&quot;Origines (O) :&quot;, nO, &quot;adresses&quot;, &quot;\\n Destinations (D) :&quot;, nD, &quot;supermarchés&quot;, &quot;\\n Distances OD à calculer = &quot;, NOD) ## Origines (O) : 184 adresses ## Destinations (D) : 27 supermarchés ## Distances OD à calculer = 4968 ## Origines et destinations Origines &lt;- Adresses Origines$id &lt;- as.character(Adresses$id) Origines$lat &lt;- st_coordinates(Adresses)[,2] Origines$lon &lt;- st_coordinates(Adresses)[,1] Destinations &lt;- supermarches Destinations$id &lt;- supermarches$osm_id Destinations$lat &lt;- st_coordinates(supermarches)[,2] Destinations$lon &lt;- st_coordinates(supermarches)[,1] names(Destinations)[1] &lt;- &quot;id&quot; Par la suite, nous calculons les différentes matrices OD : matriceOD.Auto avec mode = \"CAR\". matriceOD.Marche avec mode = \"WALK\", walk_speed = 4.5 et max_trip_duration = 60. La durée du trajet est limitée à 60 minutes avec une vitesse moyenne de 4,5 km/h. matriceOD.Velo avec mode = \"BICYCLE\", bike_speed = 15 et max_trip_duration = 60. La durée du trajet est limitée à 60 minutes avec une vitesse moyenne de 15 km/h. matriceOD.Auto avec mode = \"c(\"WALK\", \"TRANSIT\")\", walk_speed = 4.5, max_walk_time = 30, max_trip_duration = 120 et departure_datetime = dateheure.soir. La durée du trajet est limitée à 60 minutes avec une vitesse moyenne de marche de 4,5 km/h et une durée maximale de 30 minutes pour chaque segment à la marche. L’heure de départ a été fixée comme suit : dateheure.soir &lt;- as.POSIXct(\"04-05-2023 18:00:00\", format = \"%d-%m-%Y %H:%M:%S\"). ## Matrice OD en voiture t1 &lt;-Sys.time() matriceOD.Auto &lt;- travel_time_matrix(r5r_core = r5r_core, origins = Origines, destinations = Destinations, mode = &quot;CAR&quot;) t2 &lt;-Sys.time() duree.auto = as.numeric(difftime(t2, t1), units = &quot;secs&quot;) ## Matrice OD à la marche t1 &lt;-Sys.time() matriceOD.Marche &lt;- travel_time_matrix(r5r_core = r5r_core, origins = Origines, destinations = Destinations, mode = &quot;WALK&quot;, walk_speed = 4.5, # valeur par défaut 3.6 max_trip_duration = 60, # 1 heure de marche maximum max_walk_time = Inf) t2 &lt;-Sys.time() duree.marche = as.numeric(difftime(t2, t1), units = &quot;secs&quot;) ## Matrice OD en vélo t1 &lt;-Sys.time() matriceOD.Velo &lt;- travel_time_matrix(r5r_core = r5r_core, origins = Origines, destinations = Destinations, mode = &quot;BICYCLE&quot;, bike_speed = 15, max_trip_duration = 60, max_bike_time = Inf) t2 &lt;-Sys.time() duree.velo = as.numeric(difftime(t2, t1), units = &quot;secs&quot;) ## Matrice OD en transport en commun dateheure.soir &lt;- as.POSIXct(&quot;04-05-2023 18:00:00&quot;, format = &quot;%d-%m-%Y %H:%M:%S&quot;) t1 &lt;-Sys.time() matriceOD.TC &lt;- travel_time_matrix(r5r_core = r5r_core, origins = Origines, destinations = Destinations, mode = c(&quot;WALK&quot;, &quot;TRANSIT&quot;), walk_speed = 4.5, max_walk_time = 30, max_trip_duration = 120, departure_datetime = dateheure.soir) t2 &lt;-Sys.time() duree.tc = as.numeric(difftime(t2, t1), units = &quot;secs&quot;) Les temps de calcul des différentes matrices sont reportés ci-dessous. cat(&quot;Temps de calcul des matrices :&quot;, &quot;\\n Voiture : &quot;, round(duree.auto,2), &quot;secondes&quot;, &quot;\\n Marche : &quot;, round(duree.marche,2), &quot;secondes&quot;, &quot;\\n Vélo : &quot;, round(duree.velo,2), &quot;secondes&quot;, &quot;\\n Transport en commun : &quot;, round(duree.tc,2), &quot;secondes&quot;) ## Temps de calcul des matrices : ## Voiture : 11.02 secondes ## Marche : 0.34 secondes ## Vélo : 3.12 secondes ## Transport en commun : 0.36 secondes Une fois les matrices obtenues, nous les enregistrons dans un fichier Rdata. save(matriceOD.Auto, matriceOD.Marche, matriceOD.Velo, matriceOD.TC, file=&quot;data/chap05/Resultats/MatricesOD.Rdata&quot;) Libération de la mémoire allouée à JAVA Une fois les calculs avec R5R terminés, il convient de détruire l’objet r5r_core et d’arrêter le processus JAVA avec les deux lignes de code ci-dessous. r5r::stop_r5(r5r_core) rJava::.jgc(R.gc = TRUE) À partir de ces matrices, nous extrayons la valeur minimale pour chacune des adresses pour les quatre modes de transport. Puis, nous opérons une jointure attributaire avec la couche des adresses aléatoires avec la fonction merge. ## Création d&#39;un vecteur pour la distance au supermarché le plus proche SupermarchePlusProche.Auto &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.Auto, min) SupermarchePlusProche.Pied &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.Marche, min) SupermarchePlusProche.Velo &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.Velo, min) SupermarchePlusProche.tc &lt;- aggregate(travel_time_p50 ~ from_id, matriceOD.TC, min) ## Changement des noms des champs names(SupermarchePlusProche.Auto) &lt;- c(&quot;id&quot;, &quot;SupPlusProcheAuto&quot;) names(SupermarchePlusProche.Pied) &lt;- c(&quot;id&quot;, &quot;SupPlusProchePied&quot;) names(SupermarchePlusProche.Velo) &lt;- c(&quot;id&quot;, &quot;SupPlusProcheVelo&quot;) names(SupermarchePlusProche.tc) &lt;- c(&quot;id&quot;, &quot;SupPlusProcheTC&quot;) ## Jointure avec la couche des adresses Adresses &lt;- merge(Adresses, SupermarchePlusProche.Auto, by =&quot;id&quot;, all.x=TRUE) Adresses &lt;- merge(Adresses, SupermarchePlusProche.Pied, by =&quot;id&quot;, all.x=TRUE) Adresses &lt;- merge(Adresses, SupermarchePlusProche.Velo, by =&quot;id&quot;, all.x=TRUE) Adresses &lt;- merge(Adresses, SupermarchePlusProche.tc, by =&quot;id&quot;, all.x=TRUE) Finalement, nous utilisons le package tmap pour cartographier les résultats et réaliser une figure avec la fonction tmap_arrange. ## Importation des arrondissements de la ville de Sherbrooke arrondissements &lt;- st_read(dsn = &quot;data/Chap05/AutresDonnees/Arrondissements.shp&quot;, quiet=TRUE) ## Construction des cartes tmap_mode(&quot;plot&quot;) max.auto &lt;- max(Adresses$SupPlusProcheAuto,na.rm=TRUE) Carte1 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(Adresses)+ tm_dots(col=&quot;SupPlusProcheAuto&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(0,5,10,max.auto), palette=&quot;YlOrRd&quot;, size = .2, title=&quot;Voiture&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) max.pied &lt;- max(Adresses$SupPlusProchePied,na.rm=TRUE) Carte2 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(Adresses)+ tm_dots(col=&quot;SupPlusProchePied&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(0,5,10,15,20,max.pied), palette=&quot;YlOrRd&quot;, size = .2, title=&quot;Marche&quot;, textNA = &quot;Plus de 60&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) max.velo &lt;- max(Adresses$SupPlusProcheVelo,na.rm=TRUE) Carte3 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(Adresses)+ tm_dots(col=&quot;SupPlusProcheVelo&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(0,5,10,15,20,max.velo), palette=&quot;YlOrRd&quot;, size = .2, title=&quot;Vélo&quot;, textNA = &quot;Plus de 60&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) max.tc &lt;- max(Adresses$SupPlusProcheTC,na.rm=TRUE) Carte4 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(Adresses)+ tm_dots(col=&quot;SupPlusProcheTC&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(0,10,20,30,40,50,max.tc), palette=&quot;YlOrRd&quot;, size = .2, title=&quot;Bus&quot;, textNA = &quot;Plus de 60&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) ## Figure avec les quatre cartes tmap_arrange(Carte1,Carte2, Carte3, Carte4) Figure 5.12: Supermarché le plus proche en minutes selon le mode de transport References "],["sect053.html", "5.3 Mesures d’accessibilité", " 5.3 Mesures d’accessibilité 5.3.1 Notion d’accessibilité Dans un article fondateur intitulé The concept of access: definition and relationship to consumer satisfaction, Roy Penchansky et William Thomas (1981) ont identifié cinq dimensions fondamentales au concept d’accessibilité aux services de santé : L’accessibilité spatiale (accessibility) renvoie à la proximité géographique du service par rapport à la population. La disponibilité (availability) renvoie à la quantité et aux types de services offerts selon les besoins des individus. L’organisation (accommodation) renvoie au fonctionnement du service (horaires, délais d’attente, prises de rendez-vous, etc.). L’accessibilité financière (affordability) renvoie aux coûts du service qui peuvent constituer une barrière financière pour les personnes défavorisées. L’accessibilité socioculturelle (acceptability) renvoie à l’acceptation et à l’adaptation des services aux différences sociales, culturelles ou linguistiques des personnes. Les cinq dimensions de l’accessibilité et le type de service analysé L’importance accordée à chacune des cinq dimensions identifiées par Roy Penchansky et William Thomas (1981) varie en fonction du type de service à l’étude. Prenons l’exemple des parcs urbains : L’accessibilité spatiale, soit la proximité géographique est sans aucun doute une dimension très importante. La disponibilité (availability) renvoie à différents équipements (aires de jeu pour enfants, terrains de sports, etc.) présents dans le parc. La dimension de l’organisation (accommodation) risque d’être moins importante puisque les heures d’ouverture et les modalités de réservation de certains types de terrain de sport (comme un terrain de tennis) ne varient habituellement pas d’un parc à l’autre au sein d’une même ville. La dimension de l’accessibilité financière (affordability) risque aussi d’être moins importante puisque l’accès au parc et à ses équipements est gratuit, à l’exception de certains terrains de sport très spécialisés. L’accessibilité socioculturelle (acceptability) peut être une dimension très importante et renvoie à l’acceptation des différences sociales, générationnelles et ethnoculturelles des personnes utilisatrices des parcs. Plus récemment, la notion d’accessibilité à un service a été définie selon deux dimensions, soit réelle (ou révélée) ou potentielle et spatiale ou aspatiale (Guagliardo 2004; Luo et Wang 2003; Khan 1992) : L’accessibilité réelle renvoie à l’utilisation effective des services tandis que l’accessibilité potentielle renvoie à leur utilisation probable. L’accessibilité spatiale renvoie à l’importance de la séparation spatiale entre l’offre et la demande de services en tant que barrière ou facilitateur, et l’accessibilité aspatiale (dimensions financière, socioculturelle, organisationnelle) se concentre sur les barrières ou facilitateurs non géographiques (Luo et Wang 2003; Ngui et Apparicio 2011). Par conséquent, la notion d’accessibilité aux services de santé englobe quatre catégories principales : l’accessibilité spatiale réelle, l’accessibilité aspatiale réelle, l’accessibilité spatiale potentielle et l’accessibilité aspatiale potentielle (Khan 1992). Par exemple, si nous questionnons un groupe de personnes sur la localisation des parcs qu’elles fréquentent habituellement, nous pourrions dresser un portrait sur leur accessibilité spatiale réelle aux parcs. Par contre, si nous calculons le nombre d’hectares de parcs présents dans un rayon de 20 minutes de marche autour de leur domicile, nous pourrions évaluer leur accessibilité spatiale potentielle aux parcs. 5.3.2 Accessibilité spatiale potentielle Dans le cadre de cette section, nous abordons l’accessibilité spatiale potentielle qui suppose de paramétrer quatre éléments : l’unité spatiale de référence, la méthode d’agrégation, la ou les mesures d’accessibilité et le type de distance (Apparicio et al. 2017). 5.3.2.1 Unité spatiale de référence L’entité spatiale de référence correspond aux entités spatiales pour lesquelles l’accessibilité sera évaluée et cartographiée qui pourrait être : Les centroïdes des bâtiments résidentiels d’une ville donnée. Des entités polygonales représentant des zones résidentielles comme des aires de diffusion (comprenant de 400 à 700 habitants) ou des secteurs de recensement (de 2500 à 8000 habitants). L’aire de diffusion et surtout le secteur de recensement sont souvent choisis puisqu’une panoplie de variables socioéconomiques, sociodémographiques et relatives au logement sont rattachées à ces entités spatiales pour les différents recensements de Statistique Canada. La sélection de ces entités spatiales (aire de diffusion ou secteur de recensement) permet alors d’évaluer les relations entre les mesures d’accessibilité et les variables socioéconomiques ou sociodémographiques. Néanmoins, cela nécessite de recourir à méthodes d’agrégation afin de limiter les erreurs dans la mesure de l’accessibilité spatiale potentielle (Apparicio et al. 2017; Hewko, Smoyer-Tomic et Hodgson 2002). 5.3.2.2 Méthodes d’agrégation Dans un article méthodologique sur la comparaison des approches pour évaluer l’accessibilité spatiale potentielle, Apparicio et al. (2017) ont répertorié quatre principales méthodes d’agrégation pour évaluer une mesure d’accessibilité pour les secteurs de recensement. Ces approches, de la moins à la plus précise, sont les suivantes : La première approche consiste à calculer la distance entre le centroïde du secteur de recensement et le service (figure 5.13.a). Plus la taille du secteur de recensement est grande, plus l’erreur d’agrégation (l’imprécision de la mesure d’accessibilité) risque d’être importante puisqu’elle ne tient pas compte de la distribution spatiale de la population à l’intérieur du secteur de recensement. Autrement dit, cette approche revient à supposer que toute la population réside en son centroïde. La seconde approche consiste à calculer la distance entre les services et les centroïdes d’entités spatiales entièrement incluses dans les secteurs de recensement, puis à calculer la moyenne de ces distances pondérée par la population totale de chaque entité spatiale. Cette approche est réalisée avec les aires de diffusion et les îlots inclus dans les secteurs de recensement (figure 5.13.b et c). Bien entendu, les résultats sont plus précis avec les îlots de diffusion que les aires de diffusion puisqu’ils sont de taille plus réduite. La troisième approche consiste à ajuster la localisation des centroïdes des îlots en ne retenant que la partie résidentielle avec une carte d’occupation du sol (figure 5.13.d). Finalement, la quatrième approche consiste à utiliser le rôle d’évaluation foncière. Nous calculons alors les distances entre chaque unité d’évaluation foncière et les services, puis la moyenne pondérée de ces distances par le nombre de logements. Cette approche est sans aucun doute la plus précise, mais elle est bien plus chronophage. En effet, à la figure 5.13, nous avons respectivement 4 secteurs de recensement (a), 23 aires de diffusion (b), 69 îlots (c et d) et 3497 unités d’évaluation foncière. Figure 5.13: Méthodes d’agrégation et erreurs potentielles 5.3.2.3 Mesures d’accessibilité Différentes mesures renvoyant à différentes conceptualisations de l’accessibilité peuvent être utilisées pour évaluer l’accessibilité spatiale potentielle; les principales sont reportées au tableau 5.1. Pour une description détaillée de ces mesures et de leurs formules, consultez l’article d’Apparicio et al. (2017). Tableau 5.1: Conceptualisation et mesures de l’accessibilité spatiale potentielle aux services Conceptualisation Mesures d’accessibilité Proximité immédiate Distance entre l’origine et le service le plus proche Offre de services dans l’environnement immédiat Nombre de services présents à moins de n mètres ou minutes Coût moyen pour atteindre toutes les services Distance moyenne entre une origine et tous les services Coût moyen pour atteindre toutes les n destinations Distance moyenne entre une origine et n services Accessibilité en fonction de l’offre et la demande Modèles gravitaires et méthodes two-step floating catchment area (2SFCA) Source : Apparicio et al. (2017). Pour poser un diagnostic d’accessibilité spatiale potentielle à un service pour un territoire donné, plusieurs chercheures et chercheurs recommandent d’utiliser plusieurs mesures d’accessibilité. Par exemple, dans une étude sur les déserts alimentaires à Montréal, Apparicio et al. (2007) utilisent trois mesures d’accessibilité : la distance au supermarché le plus proche (proximité immédiate), le nombre de supermarchés dans un rayon de 1000 mètres (offre dans l’environnement immédiat) et la distance moyenne aux trois supermarchés d’enseignes différentes (diversité en termes d’offre et de prix) à travers le réseau de rues. Dans une autre étude portant sur l’accessibilité spatiale potentielle aux parcs urbains à Montréal, Jepson et al. (2022) utilisent deux mesures d’accessibilité : la distance au parc le plus proche (proximité immédiate) et la mesure E2FCA (congestion potentielle en fonction de l’offre et la demande) calculées pour les aires de diffusion de la Communauté métropolitaine de Montréal (figure 5.14). Concernant la proximité immédiate, le niveau d’accessibilité est bien élevé sur l’île de Montréal et inversement, plus faible à Laval et dans la Rive-Nord et la Rive-Sud (figure 5.14.a). En effet, la quasi-totalité des aires de diffusion de l’île de Montréal a un parc à moins de 200 mètres de marche. Concernant la congestion potentielle des parcs, le portrait est tout autre : le niveau de congestion potentielle est faible dans les zones suburbaines (Laval et les deux Rives) tandis qu’il est élevé dans les quartiers centraux de l’île de Montréal (figure 5.14.b). Autrement dit, les habitants des quartiers centraux de la ville de Montréal vivent plus proche d’un parc, mais cer dernier est potentiellement plus congestionné. Or, une surutilisation des parcs peut entraîner une dégradation accélérée des équipements (aires de jeu, terrains de sports, etc.), voire décourager certaines personnes à visiter un parc durant les périodes plus achalandées. Figure 5.14: Deux mesures d’accessibilité spatiale potentielle aux parcs, aires de diffusion de la Communauté métropolitaine de Montréal, 2016 5.3.2.4 Types de distance Tel que décrit à la section 5.1.2, les mesures d’accessibilité peuvent être calculées en fonction de trajets les plus rapides selon différents modes de transport, soit en automobile, à pied, en vélo et en transport en commun (figure 5.4). References "],["sect054.html", "5.4 Mesures d’accessibilité spatiale potentielle dans R", " 5.4 Mesures d’accessibilité spatiale potentielle dans R 5.4.1 Accessibilité spatiale potentielle aux supermarchés Dans ce premier exemple applicatif dans R, nous élaborons un diagnostic de l’accessibilité spatiale potentielle aux supermarchés dans la ville de Sherbrooke avec les quatre paramètres suivants : Unité spatiale de référence : aires de diffusion (AD) de 2021 de la ville de Sherbrooke. Méthode d’agrégation : calcul des moyennes pondérées par le nombre de logements des immeubles du rôle d’évaluation foncière comprises dans les AD. Trois mesures d’accessibilité : le supermarché le plus proche (en minutes), le nombre de supermarchés à 30 minutes et moins, la distance moyenne aux trois supermarchés les plus proches. Type de distance : chemin le plus rapide à la marche. Étape 1. Importation des trois couches géographiques. ## Unités d&#39;évaluation foncière Role &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Role2022Sherb.gdb&quot;, layer = &quot;rol_unite_Sherbrooke&quot;, quiet = TRUE) ## Aires de diffusion AD &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Commerces.gpkg&quot;, layer = &quot;AD_Sherbrooke&quot;, quiet=TRUE) ## Supermarchés Supermarches &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Commerces.gpkg&quot;, layer = &quot;supermarche&quot;, quiet=TRUE) ## Changement de projection Supermarches &lt;- st_transform(Supermarches, crs = 4326) Role &lt;- st_transform(Role, crs = 4326) AD &lt;- st_transform(AD, crs = 4326) Étape 2. Réalisation d’une jointure spatiale pour attribuer à chaque unité d’évaluation l’identifiant de l’aire de diffusion (champ ADIDU) dans laquelle elle est comprise. ## Jointure spatiale entre le Role et les AD Role &lt;- st_join(Role, AD[,&quot;ADIDU&quot;], join = st_intersects) Role &lt;- subset(Role, is.na(ADIDU)==FALSE) ## Nombre de distances à calculer nO = nrow(Role) nD = nrow(Supermarches) NOD = nO * nD cat(&quot;Origines (O) :&quot;, nO, &quot;îlots&quot;, &quot;\\n Destinations (D) :&quot;, nD, &quot;supermarchés&quot;, &quot;\\n Distances OD = &quot;, NOD) ## Origines (O) : 46534 îlots ## Destinations (D) : 27 supermarchés ## Distances OD = 1256418 Étape 3. Création des points d’origine et de destination. Rattacher les points aux tronçons de rue. Puisque nous utilisons le trajet le plus court à pied, les points d’origine et de destination doivent être rattachés à des tronçons de rues qui ne sont pas des autoroutes ou tout autre tronçon interdit à la marche. Pour ce faire, le package R5R dispose d’une fonction très intéressante : find_snap(r5r_core, points, mode = \"WALK\"). Sans le recours à cette fonction, un point d’origine ou de destination risque d’être rattaché à un tronçon autoroutier, faisant en sorte que le trajet ne pourra être calculé à la marche. ## Origines Origines &lt;- Role Origines$lat &lt;- st_coordinates(Origines)[,2] Origines$lon &lt;- st_coordinates(Origines)[,1] Origines$id &lt;- Origines$mat18 Origines &lt;- find_snap(r5r_core, Origines, mode = &quot;WALK&quot;) Origines$lat &lt;- Origines$snap_lat Origines$lon &lt;- Origines$snap_lon Origines &lt;- Origines[, c(&quot;point_id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;)] names(Origines) &lt;- c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;) ## Destinations Destinations &lt;- Supermarches Destinations$lat &lt;- st_coordinates(Destinations)[,2] Destinations$lon &lt;- st_coordinates(Destinations)[,1] names(Destinations)[1] &lt;- &quot;id&quot; Destinations &lt;- find_snap(r5r_core, Destinations, mode = &quot;WALK&quot;) Destinations$lat &lt;- Destinations$snap_lat Destinations$lon &lt;- Destinations$snap_lon Destinations &lt;- Destinations[, c(&quot;point_id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;)] names(Destinations) &lt;- c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;) Étape 4. Construction de la matrice origines-destinations avec la fonction travel_time_matrix et sauvegarde dans un fichier Rdata. ## Matrice OD à la marche t1 &lt;-Sys.time() matriceOD.Marche &lt;- travel_time_matrix(r5r_core = r5r_core, origins = Origines, destinations = Destinations, mode = &quot;WALK&quot;, walk_speed = 4.5, # valeur par défaut 3.6 max_trip_duration = 240, max_walk_time = Inf) t2 &lt;-Sys.time() duree.marche = as.numeric(difftime(t2, t1), units = &quot;mins&quot;) cat(&quot;Temps de calcul :&quot;, round(duree.marche,2), &quot;minutes&quot;) ## Enregistrement des résultats dans un fichier Rdata save(duree.marche, matriceOD.Marche, file=&quot;data/chap05/Resultats/MatricesMarcheRoleSupermarche.Rdata&quot;) Étape 5. Calcul des trois mesures d’accessibilité pour les unités d’évaluation. ## Chargement du fichier Rdata load(&quot;data/chap05/Resultats/MatricesMarcheRoleSupermarche.Rdata&quot;) cat(&quot;Temps de calcul pour la matrice :&quot;, round(duree.marche,2), &quot;minutes&quot;) ## Temps de calcul pour la matrice : 4.53 minutes ## Supermarché le plus proche Supermarche.PlusProche &lt;- aggregate(travel_time_p50 ~ # champ numérique from_id, # champ pour le group by matriceOD.Marche, # DataFrame FUN = min) # Minimum # Modification des noms des champs names(Supermarche.PlusProche) &lt;- c(&quot;mat18&quot;, &quot;SupPlusProcheMin&quot;) ## Nombre de supermarchés à 30 minutes # Matrice avec des valeurs inférieures ou égale à 30 matriceOD.30min &lt;- subset(matriceOD.Marche, travel_time_p50 &lt;= 30) # Agrégation Supermarche.N30mins &lt;- aggregate(travel_time_p50 ~ # champ numérique from_id, # champ pour le group by matriceOD.30min, # DataFrame FUN = length) # Nombre d&#39;observations # Modification des noms des champs names(Supermarche.N30mins) &lt;- c(&quot;mat18&quot;, &quot;SupN30min&quot;) ## Distance moyenne aux trois supermarchés les plus proches # Tri de la matrice en fonction du from_id et du travel_time_p50 matriceOD.Marche &lt;- matriceOD.Marche[order(matriceOD.Marche$from_id, matriceOD.Marche$travel_time_p50), ] # Ajout d&#39;un champ pour le rang par from_id matriceOD.Marche$Rang &lt;- ave(matriceOD.Marche$travel_time_p50, matriceOD.Marche$from_id, FUN = seq_along) # Création d&#39;une matrice avec les trois supermarchés les plus proches matriceOD.3sup &lt;- subset(matriceOD.Marche, Rang &lt;= 3) # Agrégation Supermarche.Moy3Sup &lt;- aggregate(travel_time_p50 ~ # champ numérique from_id, # champ pour le group by matriceOD.3sup, # DataFrame FUN = mean) # Moyenne # Modification des noms des champs names(Supermarche.Moy3Sup) &lt;- c(&quot;mat18&quot;, &quot;Moy3Sup&quot;) ## Fusion avec la couche Role Role &lt;- merge(Role, Supermarche.PlusProche, by=&quot;mat18&quot;) Role &lt;- merge(Role, Supermarche.N30mins, by=&quot;mat18&quot;, all.x=TRUE) Role &lt;- merge(Role, Supermarche.Moy3Sup, by=&quot;mat18&quot;, all.x=TRUE) # Certaines observations n&#39;ont pas de supermarchés à 30 minutes. # Nous mettons alors les valeurs à 0 Role$SupN30min[is.na(Role$SupN30min)] &lt;- 0 Étape 6. Calcul des moyennes pondérées par le nombre de logements (champ Logements) pour les aires de diffusion avec le package dplyr. library(dplyr) ## Création d&#39;un DataFrame temporaire sans la géométrie Role.Temp &lt;- st_drop_geometry(Role) ## Moyennes pondérées pour le supermarché le plus proche MesureAcc1 &lt;- as.data.frame(Role.Temp %&gt;% group_by(ADIDU) %&gt;% summarize(SupPlusProcheMin = weighted.mean(SupPlusProcheMin, Logements))) ## Moyennes pondérées pour le nombre de supermarchés à 30 minutes MesureAcc2 &lt;- as.data.frame(Role.Temp %&gt;% group_by(ADIDU) %&gt;% summarize(SupN30min = weighted.mean(SupN30min, Logements))) ## Moyennes pondérées pour les trois supermarchés les plus proches MesureAcc3 &lt;- as.data.frame(Role.Temp %&gt;% group_by(ADIDU) %&gt;% summarize(Moy3Sup = weighted.mean(Moy3Sup, Logements))) ## Fusion avec la couche des îlots AD &lt;- merge(AD, MesureAcc1, by=&quot;ADIDU&quot;) AD &lt;- merge(AD, MesureAcc2, by=&quot;ADIDU&quot;) AD &lt;- merge(AD, MesureAcc3, by=&quot;ADIDU&quot;) Étape 7. Cartographie des résultats avec le package tmap. Tout d’abord, nous analysons les statistiques univariées pour repérer les valeurs minimales et maximales pour les trois mesures d’accessibilité avec la fonction summary. TroisMesures &lt;- c(&quot;SupPlusProcheMin&quot;,&quot;SupN30min&quot;,&quot;Moy3Sup&quot;) summary(AD[, TroisMesures]) ## SupPlusProcheMin SupN30min Moy3Sup geometry ## Min. : 4.528 Min. :0.000 Min. : 6.149 MULTIPOLYGON :254 ## 1st Qu.: 11.009 1st Qu.:1.000 1st Qu.: 17.441 epsg:4326 : 0 ## Median : 16.304 Median :2.536 Median : 23.641 +proj=long...: 0 ## Mean : 22.616 Mean :2.774 Mean : 32.867 ## 3rd Qu.: 24.858 3rd Qu.:4.518 3rd Qu.: 34.188 ## Max. :130.615 Max. :7.333 Max. :165.006 Une fois les valeurs maximales et minimales analysées, réalisons les cartes (figure 5.15). ## Importation des arrondissements de la ville de Sherbrooke arrondissements &lt;- st_read(dsn = &quot;data/Chap05/AutresDonnees/Arrondissements.shp&quot;, quiet=TRUE) ## Construction des cartes tmap_mode(&quot;plot&quot;) # Carte pour les supermarchés Carte0 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(Supermarches)+ tm_dots(col=&quot;red&quot;, size=0.25)+ tm_layout(frame = FALSE, legend.outside = TRUE, title = &quot;Supermarché&quot;, title.size = 1) # Carte pour le supermarché le plus proche max.acc1 &lt;- max(AD$SupPlusProcheMin,na.rm=TRUE) Carte1 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(AD)+ tm_fill(col=&quot;SupPlusProcheMin&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(0,10,20,30,40,60,max.acc1), palette=&quot;-YlOrRd&quot;, size = .2, title=&quot;Plus proche&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) # Carte pour le nombre de supermarchés à 30 minutes ou moins max.acc2 &lt;- max(AD$SupN30min,na.rm=TRUE) Carte2 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(AD)+ tm_fill(col=&quot;SupN30min&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(0,1,2,3,4,5,max.acc2), palette=&quot;YlOrRd&quot;, size = .2, title=&quot;Sup. à 30 min&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) # Carte pour la distance moyenne aux trois supermarchés les plus proches max.acc3 &lt;- max(AD$Moy3Sup,na.rm=TRUE) Carte3 &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(AD)+ tm_fill(col=&quot;Moy3Sup&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(5,10,20,30,40,60,max.acc3), palette=&quot;-YlOrRd&quot;, size = .2, title=&quot;Moy. 3 plus proches&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) ## Figure avec les quatre cartes tmap_arrange(Carte0, Carte1, Carte2, Carte3) Figure 5.15: Accessibilité spatiale potentielle à pied aux supermarchés (en minutes), aires de diffusion de la ville de Sherbrooke, 2021 5.4.2 Accessibilité spatiale potentielle aux patinoires extérieures Dans ce second exemple applicatif dans R, nous élaborons un diagnostic de l’accessibilité spatiale potentielle aux patinoires extérieures dans la ville de Sherbrooke avec les quatre paramètres suivants : Unité spatiale de référence : aires de diffusion (AD) de 2021 de la ville de Sherbrooke. Méthode d’agrégation : calcul des moyennes pondérées par la population totale des îlots compris dans les AD. Deux mesures d’accessibilité : patinoire la plus proche (en minutes); E2SFCA (Enhanced two-step floating catchment area), soit le nombre de patinoire pour 1000 habitants dans un rayon de 30 minutes de marche. Type de distance : chemin le plus rapide à la marche. Étape 1. Importation des trois couches géographiques. ## Unités d&#39;évaluation foncière Patinoires &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Patinoires.shp&quot;, quiet = TRUE) ## Aires de diffusion AD &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Commerces.gpkg&quot;, layer = &quot;AD_Sherbrooke&quot;, quiet=TRUE) ## Ilots de recensements Ilots &lt;- st_read(dsn = &quot;data/chap05/AutresDonnees/Commerces.gpkg&quot;, layer = &quot;Ilots&quot;, quiet=TRUE) ## Changement de projection Patinoires &lt;- st_transform(Patinoires, crs = 4326) AD &lt;- st_transform(AD, crs = 4326) Ilots &lt;- st_transform(Ilots, crs = 4326) Étape 2. Réalisation d’une jointure spatiale pour attribuer à chaque îlot l’identifiant de l’aire de diffusion (champ ADIDU) dans laquelle il est compris. ## Jointure spatiale entre le Rôle et les AD Ilots &lt;- st_join(Ilots, AD[,&quot;ADIDU&quot;], join = st_intersects) Ilots &lt;- Ilots[,c(&quot;id&quot;,&quot;pop2021&quot;,&quot;ADIDU&quot;)] Étape 3. Création des points d’origine et de destination. ## Origines Origines &lt;- Ilots Origines$lat &lt;- st_coordinates(Origines)[,2] Origines$lon &lt;- st_coordinates(Origines)[,1] Origines &lt;- find_snap(r5r_core, Origines, mode = &quot;WALK&quot;) Origines$lat &lt;- Origines$snap_lat Origines$lon &lt;- Origines$snap_lon Origines &lt;- Origines[, c(&quot;point_id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;)] names(Origines) &lt;- c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;) ## Destinations Destinations &lt;- Patinoires Destinations$lat &lt;- st_coordinates(Destinations)[,2] Destinations$lon &lt;- st_coordinates(Destinations)[,1] Destinations$id &lt;- as.character(1:nrow(Destinations)) Destinations &lt;- find_snap(r5r_core, Destinations, mode = &quot;WALK&quot;) Destinations$lat &lt;- Destinations$snap_lat Destinations$lon &lt;- Destinations$snap_lon Destinations &lt;- Destinations[, c(&quot;point_id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;)] names(Destinations) &lt;- c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;distance&quot;) Étape 4. Construction de la matrice origines-destinations avec la fonction travel_time_matrix et sauvegarde dans un fichier Rdata. ## Matrice OD à la marche t1 &lt;-Sys.time() matriceODPatinoire.Marche &lt;- travel_time_matrix(r5r_core = r5r_core, origins = Origines, destinations = Destinations, mode = &quot;WALK&quot;, walk_speed = 4.5, max_trip_duration = 240, max_walk_time = Inf) t2 &lt;-Sys.time() duree.marche = as.numeric(difftime(t2, t1), units = &quot;mins&quot;) cat(&quot;Temps de calcul :&quot;, round(duree.marche,2), &quot;minutes&quot;) ## Enregistrement des résultats dans un fichier Rdata save(duree.marche, matriceODPatinoire.Marche, file=&quot;data/chap05/Resultats/matriceODPatinoire.Rdata&quot;) Étape 5. Calcul des deux mesures d’accessibilité pour les îlots. ## Chargement du fichier Rdata load(&quot;data/chap05/Resultats/matriceODPatinoire.Rdata&quot;) cat(&quot;Temps de calcul pour la matrice :&quot;, round(duree.marche,2), &quot;minutes&quot;) ## Temps de calcul pour la matrice : 0.26 minutes Le code ci-dessous permet de calculer la distance à la patinoire la plus proche pour les aires de diffusion. ## Patinoire la plus proche Patinoire.PlusProche &lt;- aggregate(travel_time_p50 ~ # champ numérique from_id, # champ pour le group by matriceODPatinoire.Marche, # DataFrame FUN = min) # Minimum # Modification des noms des champs names(Patinoire.PlusProche) &lt;- c(&quot;id&quot;, &quot;PatinoirePlusProche&quot;) ## Fusion avec la couche des îlots Ilots &lt;- merge(Ilots, Patinoire.PlusProche, by=&quot;id&quot;) Puis, nous calculons la version de la méthode du E2SFCA avec une fonction de gradient continue (McGrail et Humphreys 2009). library(sqldf) ## Le chargement a nécessité le package : gsubfn ## Le chargement a nécessité le package : proto ## Le chargement a nécessité le package : RSQLite ## Ajout des champs de population dans la matrice TempIlots &lt;- st_drop_geometry(Ilots) matriceODPatinoire &lt;- merge(matriceODPatinoire.Marche, TempIlots[,c(&quot;id&quot;,&quot;pop2021&quot;)], by.x = &quot;from_id&quot;, by.y=&quot;id&quot;) matriceODPatinoire$Wd &lt;- 1 names(matriceODPatinoire) &lt;- c(&quot;from_id&quot;, &quot;to_id&quot;, &quot;Marche&quot;, &quot;Wo&quot;, &quot;Wd&quot;) head(matriceODPatinoire, n=2) ## from_id to_id Marche Wo Wd ## 1: 24430011001 1 90 180 1 ## 2: 24430011001 2 159 180 1 ## Fonction pour la méthode E2SFCA avec une fonction de gradient continue #&#39; Fonction pour tracer le rectangle #&#39; @param MatriceOD matrice de distance OD. #&#39; @param IDorigine Champ identifiant pour l&#39;origine dans la matrice OD. #&#39; @param IDdestination Champ identifiant pour la destination dans la matrice OD. #&#39; @param CoutDistance Coût pour la distance dans la matrice OD. #&#39; @param RatioHabitants Ratio pour le nombre d&#39;habitants (1000 par défaut). #&#39; @param Wo pondération pour la demande (population à l&#39;origine). #&#39; @param Wd pondération pour l&#39;offre (taille du service à la destination). GE2SFCA &lt;- function(MatriceOD, IDorigine, IDdestination, CoutDistance, RatioHabitants = 1000, Rayon, Palier, Wo, Wd, ChampSortie){ ## Suppression des observations avec un coût supérieure au rayon Matrice &lt;- subset(MatriceOD, MatriceOD[[CoutDistance]] &lt;= Rayon) ## Calcul de Pk, soit ((60 - CoutDistance)/(60 - 10))^1.5; Matrice$W &lt;-ifelse(Matrice[[CoutDistance]] &lt; Palier, 1, ((Rayon - Matrice[[CoutDistance]]) / (Rayon - Palier))**1.5) Matrice$Pk &lt;- Matrice$W * Matrice[[Wo]] print(head(Matrice)) ## Étape 1 : Ratio Population/service dans le rayon autour des destinations RequeteSQL &lt;- paste( &quot;Select &quot; , IDdestination, &quot;, Min(&quot;,Wd,&quot;) as &quot;, Wd, &quot; , Sum(Pk) as &quot;, Wo, &quot; from Matrice Group by &quot;, IDdestination, sep=&quot;&quot;) Step1 &lt;- sqldf(RequeteSQL) Step1$Rj &lt;- Step1[[Wd]] / (Step1[[Wo]] / RatioHabitants) Step1 &lt;- Step1[, c(1,4)] ## Étape 2 : ramener la somme de ces ratios pour les origines Jointure &lt;- merge(Matrice,Step1, by = IDdestination, all.x=FALSE, all.y=FALSE) Query&lt;- paste( &quot;Select &quot; ,IDorigine, &quot;, sum(Rj) as &quot;, ChampSortie, &quot; from Jointure Group by &quot;, IDorigine, sep=&quot;&quot;) gD2SFCA &lt;- sqldf(Query) return(gD2SFCA) } MesureE2SFCA &lt;- GE2SFCA(MatriceOD = matriceODPatinoire, IDorigine = &quot;from_id&quot;, IDdestination = &quot;to_id&quot;, CoutDistance = &quot;Marche&quot;, RatioHabitants = 1000, Rayon = 30, Palier = 5, Wo = &quot;Wo&quot;, Wd = &quot;Wd&quot;, ChampSortie = &quot;E2SFCA_G&quot;) ## from_id to_id Marche Wo Wd W Pk ## 1: 24430011001 21 23 180 1 0.14816207 26.669173 ## 2: 24430011001 22 23 180 1 0.14816207 26.669173 ## 3: 24430011001 23 22 180 1 0.18101934 32.583480 ## 4: 24430011001 26 27 180 1 0.04156922 7.482459 ## 5: 24430011001 28 16 180 1 0.41906563 75.431813 ## 6: 24430011001 29 18 180 1 0.33255376 59.859676 MesureE2SFCA$E2SFCA_G[is.na(Role$E2SFCA_G)] &lt;- 0 Ilots &lt;- merge(Ilots, MesureE2SFCA, by.x =&quot;id&quot;, by=&quot;from_id&quot;, all.x = TRUE) Ilots$E2SFCA_G[is.na(Ilots$E2SFCA_G)] &lt;- 0 Étape 6. Calcul des moyennes pondérées par la population des îlots (champ pop2021) pour les aires de diffusion avec le package dplyr. ## Création d&#39;un DataFrame temporaire sans la géométrie Ilots.Temp &lt;- st_drop_geometry(Ilots) ## Moyenne pondérées pour la patinoire la plus proche MesureAcc1 &lt;- as.data.frame(Ilots.Temp %&gt;% group_by(ADIDU) %&gt;% summarize(PatinoirePlusProche = weighted.mean(PatinoirePlusProche, pop2021))) ## Moyenne non pondérée pour le E2SFCA, car la population est déjà prise en compte MesureAcc2 &lt;- aggregate(E2SFCA_G ~ ADIDU, Ilots.Temp, FUN = mean) ## Fusion avec la couche des îlots AD &lt;- merge(AD, MesureAcc1, by=&quot;ADIDU&quot;) AD &lt;- merge(AD, MesureAcc2, by=&quot;ADIDU&quot;) Étape 7. Cartographie des résultats avec le package tmap. Tout d’abord, nous analysons les statistiques univariées pour repérer les valeurs minimales et maximales pour les trois mesures d’accessibilité. DeuxMesures &lt;- c(&quot;PatinoirePlusProche&quot;,&quot;E2SFCA_G&quot;) summary(AD[, DeuxMesures]) ## PatinoirePlusProche E2SFCA_G geometry ## Min. : 1.000 Min. :0.0000 MULTIPOLYGON :254 ## 1st Qu.: 9.312 1st Qu.:0.4162 epsg:4326 : 0 ## Median : 14.775 Median :0.5760 +proj=long...: 0 ## Mean : 17.901 Mean :0.6553 ## 3rd Qu.: 20.648 3rd Qu.:0.8939 ## Max. :124.895 Max. :3.3559 Une fois avoir pris connaissance des valeurs maximales et minimales, nous pouvons réaliser les cartes (figure 5.16). ## Importation des arrondissements de la ville de Sherbrooke arrondissements &lt;- st_read(dsn = &quot;data/Chap05/AutresDonnees/Arrondissements.shp&quot;, quiet=TRUE) ## Construction des cartes tmap_mode(&quot;plot&quot;) # Carte pour les patinoires Carte0p &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(Patinoires)+ tm_dots(col=&quot;red&quot;, size=0.25)+ tm_layout(frame = FALSE, legend.outside = TRUE, title = &quot;Patinoire extérieure&quot;, title.size = 1) # Carte pour la patinoire la plus proche max.acc1 &lt;- max(AD$PatinoirePlusProche,na.rm=TRUE) Carte1p &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(AD)+ tm_fill(col=&quot;PatinoirePlusProche&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(0,10,20,30,40,60,max.acc1), palette=&quot;-YlOrRd&quot;, size = .2, title=&quot;Plus proche (min)&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) # Carte pour le 2ESFCA AD2 &lt;- subset(AD, E2SFCA_G !=0) min.acc2 &lt;- min(AD2$E2SFCA_G,na.rm=TRUE) max.acc2 &lt;- max(AD2$E2SFCA_G,na.rm=TRUE) Carte2p &lt;- tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_shape(AD)+tm_fill(col=&quot;gray&quot;)+ tm_shape(AD2)+ tm_fill(col=&quot;E2SFCA_G&quot;, border.lwd = 1, style = &quot;fixed&quot;, breaks = c(min.acc2,0.25,0.50,0.75,1,1.5,max.acc2), palette=&quot;YlOrRd&quot;, size = .2, title=&quot;Patinoire pour 1000 hab.&quot;)+ tm_shape(arrondissements)+tm_borders(lwd = 2)+ tm_layout(frame = FALSE, legend.outside = TRUE) ## Figure avec les trois cartes tmap_arrange(Carte0p, Carte1p, Carte2p) Figure 5.16: Accessibilité spatiale potentielle à pied aux patinoires extérieures, aires de diffusion de la ville de Sherbrooke, 2021 ## Arrêt de Java r5r::stop_r5(r5r_core) rJava::.jgc(R.gc = TRUE) References "],["sect055.html", "5.5 Quiz de révision du chapitre", " 5.5 Quiz de révision du chapitre Quels sont les trois principaux problèmes résolus en analyse de réseau? Relisez au besoin la section 5.1.2. Trouver le trajet le plus court ou le plus rapide entre deux points (algorithme de Dijkstra) Trouver la route optimale comprenant plusieurs arrêts (problème du voyageur de commerce) Définir des zones de desserte autour d’une origine (algorithme de Dijkstra) Mesurer la dépendance spatiale Quels sont les quatre autres problèmes résolus en analyse de réseau? Relisez au besoin la section 5.1.2. Trouver les k services les plus proches à partir d’une origine Construire une matrice de distance origines-destinations Répartir aléatoirement des points dans un territoire donné Résoudre le problème de tournées de véhicules Réaliser un modèle localisation-affectation Quels sont les trois fichiers nécessaires pour construire un réseau multimode avec R5R? Relisez au besoin le début de la section 5.1.2. Un fichier pbf pour les données d’OpenStreetMap Un ou plusieurs fichiers GTFS Un fichier pour les bâtiments Un fichier GeoTIFF d’élévation Quelle fonction de R5R permet de construire un réseau multimode? Relisez au besoin la section 5.2.3. detailed_itineraries() setup_r5() travel_time_matrix() Quelle fonction de R5R permet de construire un itinéraire? Relisez au besoin la section 5.2.3. detailed_itineraries() setup_r5() travel_time_matrix() Quelle fonction de R5R permet de construire une matrice origine-destination? Relisez au besoin la section 5.2.3. detailed_itineraries() setup_r5() travel_time_matrix() Pour évaluer l’accessibilité selon la conceptualisation de la proximité immédiate, quelle mesure d’accessibilité utilisez-vous? Relisez au besoin la section 5.3.2.3. Distance entre l’origine et le service le plus proche Nombre de services présents à moins de n mètres ou minutes Distance moyenne entre une origine et n services Modèles gravitaires et méthodes two-step floating catchment area (2SFCA) Pour évaluer l’accessibilité selon la conceptualisation de l’offre et la demande, quelle mesure d’accessibilité utilisez-vous? Relisez au besoin la section 5.3.2.3. Distance entre l’origine et le service le plus proche Nombre de services présents à moins de n mètres ou minutes Distance moyenne entre une origine et n services Modèles gravitaires et méthodes two-step floating catchment area (2SFCA) Pour évaluer l’accessibilité selon l’offre de services dans l’environnement immédiat, quelle mesure d’accessibilité utilisez-vous? Relisez au besoin la section 5.3.2.3. Distance entre l’origine et le service le plus proche Nombre de services présents à moins de n mètres ou minutes Distance moyenne entre une origine et n services Modèles gravitaires et méthodes two-step floating catchment area (2SFCA) Vérifier votre résultat "],["chap06.html", "Chapitre 6 Introduction aux modèles de régression spatiale", " Chapitre 6 Introduction aux modèles de régression spatiale Depuis une trentaine d’années, économètres, épidémiologistes et géographes développent et utilisent abondamment des méthodes de modélisation intégrant l’espace : modèles économétriques spatiaux, modèles géographiquement pondérés, analyses multiniveaux, etc. L’objectif de chapitre est donné un aperçu de ces méthodes. Nous décrirons principalement les différents modèles économétriques spatiaux, les modèles généralisés additifs avec une spline bivariée sur les coordonnées géographiques et les modèles géographiquement pondérés. Dans ce chapitre, nous utilisons les packages suivants : Pour importer et manipuler des fichiers géographiques : sf pour importer et manipuler des données vectorielles. raster et terra pour manipuler des données images. Pour construire des cartes et des graphiques : tmap est certainement le meilleur package pour la cartographie. ggplot2 pour construire des graphiques. Pour construire des modèles spatiaux : spdep pour construire des matrices de pondération spatiales et calculer le I de Moran. spatialreg pour construire des modèles d’économétriques spatiaux. mgcv pour construire des modèles généralisés additifs avec une spline sur les sur les coordonnées géographiques. spgwr pour construire des régressions géographiquement pondérées. Pour décrire les différents modèles, nous proposons d’utiliser le jeu de données spatiales LyonIris du package geocmeans. Ce jeu de données spatiales pour l’agglomération lyonnaise (France) comprend dix variables, dont quatre environnementales (EN) et six socioéconomiques (SE), pour les îlots regroupés pour l’information statistique (IRIS) de l’agglomération lyonnaise (tableau 6.1 et figure 6.1). Tableau 6.1: Statistiques descriptives du jeu de données LyonIris Nom Intitulé Type Moy. E.-T. Min. Max. Lden Bruit routier (Lden dB(A)) EN 55,6 4,9 33,9 71,7 NO2 Dioxyde d’azote (ug/m3) EN 28,7 7,9 12,0 60,2 PM25 Particules fines (PM\\(_{2,5}\\)) EN 16,8 2,1 11,3 21,9 VegHautPrt Canopée (%) EN 18,7 10,1 1,7 53,8 Pct0_14 Moins de 15 ans (%) SE 18,5 5,7 0,0 54,0 Pct_65 65 ans et plus (%) SE 16,2 5,9 0,0 45,1 Pct_Img Immigrants (%) SE 14,5 9,1 0,0 59,8 TxChom1564 Taux de chômage SE 14,8 8,1 0,0 98,8 Pct_brevet Personnes à faible scolarité (%) SE 23,5 12,6 0,0 100,0 NivVieMed Médiane du niveau de vie (milliers d’euros) SE 21,8 4,9 11,3 38,7 Figure 6.1: Cartographie des variables du jeu de données LyonIris "],["sect061.html", "6.1 Modèles économétriques spatiaux", " 6.1 Modèles économétriques spatiaux Régression linéaire multiple et modèles économétriques spatiaux Dans cette section, nous décrivons uniquement les modèles économétriques spatiaux dont la variable dépendante est continue. Sommairement, ces modèles sont des extensions de la régression linéaire multiple dans laquelle est intégrée l’autocorrélation spatiale. Avant de lire cette section, il faut donc bien maîtriser la régression linéaire multiple. Si ce n’est pas le cas, nous vous invitons vivement à lire le chapitre suivant (Apparicio et Gelb 2022). Ces deux dernières décennies, plusieurs ouvrages traitant des modèles économétriques spatiaux ont été publiés, surtout en anglais (LeSage et Pace 2008; Anselin et Rey 2014; Bivand et al. 2008). Ils méritent grandement d’être consultés, tout comme l’excellent livre en français de Jean Dubé et Diègo Legros (2014). Pourquoi recourir à des modèles économétriques spatiaux? Dans un modèle, les résidus (\\(\\epsilon\\)) sont la différence entre les valeurs observées (\\(y_i\\)) et les valeurs prédites par le modèle (\\(\\widehat{y_i}\\)). Une des hypothèses de la régression linéaire multiple est que les observations doivent être indépendantes les unes des autres (indépendance du terme d’erreur). Le non-respect de cette hypothèse produit des résultats biaisés, notamment pour les coefficients de régression. Lorsque les observations sont des entités spatiales (polygones, points par exemple), si les résidus du modèle sont autocorrélées spatialement, il y a un problème de dépendance spatiale du modèle. Autrement dit, les observations ne sont pas spatialement indépendantes les unes des autres. Pour vérifier la dépendance spatiale d’un modèle, il suffit de calculer le I de Moran sur les résidus du modèle, comme décrit au chapitre 2 (section 2.3). Autrement dit, un modèle régression construit à des données spatiales ne devrait pas avoir des résidus spatialement autocorrélés. Or, les modèles économétriques spatiaux permettent justement d’intégrer l’autocorrélation spatiale de différentes manières afin de s’assurer que l’hypothèse de l’indépendance du terme d’erreur soit respectée. 6.1.1 Bref retour sur la régression linéaire multiple À titre de rappel, la régression linéaire multiple permet de prédire et d’expliquer une variable dépendante (\\(Y\\)) en fonction de plusieurs variables indépendantes (\\(X\\)). L’équation de régression s’écrit alors : \\[\\begin{equation} y_i = \\beta_{0} + \\beta_{1}x_{1i} + \\beta_{2}x_{2i} +\\ldots+ \\beta_{k}x_{ki} + \\epsilon_{i} \\tag{6.1} \\end{equation}\\] avec : \\(y_i\\), la valeur de la variable dépendante Y pour l’observation i \\(\\beta_{0}\\), la constante, soit la valeur prédite pour Y quand toutes les variables indépendantes sont égales à 0 \\(k\\) le nombre de variables indépendantes \\(\\beta_{1}\\) à \\(\\beta_{k}\\), les coefficients de régression pour les variables indépendantes de 1 à k (\\(X_{1}\\) à \\(X_{k}\\)) \\(\\epsilon_{i}\\), le résidu pour l’observation de i, soit la partie de la valeur de \\(y_i\\) qui n’est pas expliquée par le modèle de régression. il existe plusieurs écritures simplifiées de cette équation. Dans le cadre de ce chapitre, nous utilisons la forme matricielle suivante : \\[\\begin{equation} y = X\\beta + \\epsilon \\tag{6.2} \\end{equation}\\] avec : \\(y\\), un vecteur de dimension \\(n \\times 1\\) pour la variable dépendante, soit une colonne avec n observations \\(X\\), une matrice de dimension \\(n \\times (k + 1)\\) pour les k variables indépendantes, incluant une autre colonne (avec la valeur de 1 pour les n observations) pour la constante d’où \\(k + 1\\) \\(\\beta\\), un vecteur de dimension \\(k + 1\\), soit les coefficients de régression pour les k variables et la constante \\(\\epsilon\\), un vecteur de dimension \\(n \\times 1\\) pour les résidus. Construction du modèle MCO dans R Avec la fonction lm(), il est facile de construire un modèle de régression linéaire multiple, basé sur la méthodes des moindres carrés ordinaires (MCO). Dans le code ci-dessous, la formule de l’équation du modèle est donc NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed. Notez que la variable dépendante et les variables indépendantes sont séparées avec un tilde (~). Quant à la fonction summary(NomDuModèle), elle affiche les résultats du modèle. # Appel des différents packages utilisés dans le chapitre library(spdep) ## Construction du modèle Modele.MCO &lt;- lm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, data = LyonIris) ## Résultats du modèle summary(Modele.MCO) ## ## Call: ## lm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.733 -4.457 -0.499 3.507 29.160 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 49.43296 2.99550 16.502 &lt; 2e-16 *** ## Pct0_14 -0.53352 0.06305 -8.461 2.94e-16 *** ## Pct_65 -0.15047 0.05627 -2.674 0.00774 ** ## Pct_Img 0.28287 0.05113 5.532 5.12e-08 *** ## Pct_brevet -0.24004 0.03721 -6.451 2.63e-10 *** ## NivVieMed -0.31625 0.10180 -3.107 0.00200 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.685 on 500 degrees of freedom ## Multiple R-squared: 0.2832, Adjusted R-squared: 0.276 ## F-statistic: 39.5 on 5 and 500 DF, p-value: &lt; 2.2e-16 Dépendance spatiale du modèle MCO? Pour vérifier si ce modèle linéaire multiple a un problème de dépendance spatiale, nous calculons le I de Moran sur ses résidus avec la fonction lm.morantest, puis nous les cartographions. ## Matrice de contiguïté selon le partage d&#39;un segment (Rook) Rook &lt;- poly2nb(LyonIris, queen=FALSE) W.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = &quot;W&quot;) # Autocorrélation spatiale globale des résidus lm.morantest(Modele.MCO, W.Rook, alternative=&quot;two.sided&quot;) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris) ## weights: W.Rook ## ## Moran I statistic standard deviate = 21.266, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## sample estimates: ## Observed Moran I Expectation Variance ## 0.587312061 -0.005375800 0.000776745 Avec une valeur du I de Moran de 0,587 (p &lt; 0,001), les résidus sont fortement autocorrélés spatialement, traduisant ainsi un problème de dépendance spatiale du modèle MCO et la nécessité de recourir à des modèles économétriques spatiaux. La cartographie des résidus à la figure 6.2 corrobore ce résultat. library(tmap) ## Ajout du colonne dans LyonIris avec les valeurs des résidus LyonIris$ModeleMCO.Residus &lt;- residuals(Modele.MCO) ## Cartographie tmap_mode(&quot;plot&quot;) tm_shape(LyonIris)+ tm_fill(col=&quot;ModeleMCO.Residus&quot;, n = 5, style = &quot;quantile&quot;, palette = &quot;-RdBu&quot;, title = &quot;Résidus&quot;) + tm_layout(frame=FALSE) + tm_scale_bar(breaks = c(0,5)) Figure 6.2: Cartographie des résidus du modèle de régression multiple 6.1.2 Les différents modèles spatiaux autorégressifs Selon Jean Dubé et Diègo Legros, « cinq raisons peuvent motiver le choix d’un modèle autorégressif : la présence d’externalités, les effets d’entraînement, l’omission de variables importantes, la présence d’hétérogénéité spatiale des comportements, les effets mixtes » (2014, 120). Les effets mixtes peuvent être la combinaison d’externalités avec des effets d’entraînement ou encore d’externalités avec l’omission d’une ou plusieurs variables importantes spatialement structurées. 6.1.2.1 Modèle SLX : autocorrélation spatiale sur les variables indépendantes Dans un modèle SLX, l’autocorrélation spatiale est intégrée au niveau des variables indépendantes. Autrement dit, les variables indépendantes spatialement décalées (\\(WX\\)) sont introduites aussi dans le modèle. Par conséquent, la valeur de chaque unité spatiale du modèle est ainsi expliquée à la fois par ses propres caractéristiques et celles dans le voisinage ou à proximité en fonction de la matrice de pondération spatiale (\\(W\\)). Rappel sur les variables spatialement décalées Dans chapitre 2 sur l’autocorrélation spatiale, nous avons vu comment calculer une variable spatialement décalée avec un matrice de pondération spatiale (figure 2.20). À titre de rappel, lorsque cette dernière est standardisée en ligne, elle correspond à la valeur moyenne dans le voisinage. L’idée est alors d’introduire des externalités puisque les caractéristiques des entités spatiales proches ou voisines peuvent avoir un effet sur la variable dépendante (Dubé et Legros 2014). L’équation du modèle SLX, qui est estimée selon la méthode des moindres carrés ordinaires (comme la régression linéaire multiple), s’écrit alors : \\[\\begin{equation} y = X\\beta + WX\\theta + \\epsilon \\tag{6.3} \\end{equation}\\] avec : \\(y\\), la variable dépendante \\(X\\), les variables indépendantes \\(\\beta\\), les coefficients des variables dépendantes \\(W\\), la matrice de pondération spatiale \\(WX\\), les variables dépendantes spatiales décalés \\(\\theta\\), les coefficients des variables dépendantes spatiales décalées \\(\\epsilon\\), les résidus. Construction du modèle SLX dans R Le modèle SLX est construit avec la fonction lmSLX du package spatialreg (Bivand, Millo et Piras 2021). Remarquez dans le code ci-dessous le paramètre listw=W.Rook qui est utilisé pour spécifier la matrice de pondération spatiale. library(spatialreg) ## Construction du modèle Modele.SLX &lt;- lmSLX(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, listw=W.Rook, # matrice de pondération spatiale data = LyonIris) # dataframe ## Résultats du modèle # Résultats du modèles summary(Modele.SLX) ## ## Call: ## lm(formula = formula(paste(&quot;y ~ &quot;, paste(colnames(x)[-1], collapse = &quot;+&quot;))), ## data = as.data.frame(x), weights = weights) ## ## Residuals: ## Min 1Q Median 3Q Max ## -18.6087 -3.5084 -0.7178 2.9128 26.8724 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 50.67786 4.18791 12.101 &lt; 2e-16 *** ## Pct0_14 -0.20404 0.06268 -3.255 0.00121 ** ## Pct_65 -0.03771 0.05361 -0.703 0.48217 ## Pct_Img 0.10406 0.04849 2.146 0.03235 * ## Pct_brevet -0.07363 0.03550 -2.074 0.03857 * ## NivVieMed -0.18441 0.11063 -1.667 0.09617 . ## lag.Pct0_14 -0.77591 0.10295 -7.537 2.32e-13 *** ## lag.Pct_65 -0.06454 0.09115 -0.708 0.47924 ## lag.Pct_Img 0.64654 0.08593 7.524 2.53e-13 *** ## lag.Pct_brevet -0.30128 0.06157 -4.893 1.34e-06 *** ## lag.NivVieMed -0.01805 0.17499 -0.103 0.91790 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.77 on 495 degrees of freedom ## Multiple R-squared: 0.4713, Adjusted R-squared: 0.4606 ## F-statistic: 44.13 on 10 and 495 DF, p-value: &lt; 2.2e-16 Effets directs, indirects et totaux Les effets des variables indépendantes sont donc décomposés en trois composantes : les effets directs, soit ceux des caractéristiques des entités spatiales. Ils correspondent aux coefficients \\(\\beta\\) des variables dépendantes (\\(X\\)) les effets indirects, soit ceux des caractéristiques des entités spatiales voisines ou proches définies selon la matrice de pondération spatiale. Ils correspondent aussi aux coefficients \\(\\theta\\) des variables dépendantes spatialement décalées (\\(WX\\)) les effets totaux soit la somme deux précédents. Le code suivant permet de les obtenir. ## Effets directs, indirects et totaux (uniquement les coefficients) impacts(Modele.SLX) ## Impact measures (SlX, glht): ## Direct Indirect Total ## Pct0_14 -0.20403803 -0.77590830 -0.9799463 ## Pct_65 -0.03770918 -0.06453809 -0.1022473 ## Pct_Img 0.10406359 0.64653923 0.7506028 ## Pct_brevet -0.07363272 -0.30128171 -0.3749144 ## NivVieMed -0.18440960 -0.01804718 -0.2024568 ## Effets directs, indirects et totaux (coefficients, valeurs de z et de p) summary(impacts(Modele.SLX)) ## Impact measures (SlX, glht, n-k): ## Direct Indirect Total ## Pct0_14 -0.20403803 -0.77590830 -0.9799463 ## Pct_65 -0.03770918 -0.06453809 -0.1022473 ## Pct_Img 0.10406359 0.64653923 0.7506028 ## Pct_brevet -0.07363272 -0.30128171 -0.3749144 ## NivVieMed -0.18440960 -0.01804718 -0.2024568 ## ======================================================== ## Standard errors: ## Direct Indirect Total ## Pct0_14 0.06268202 0.10295210 0.10045332 ## Pct_65 0.05361420 0.09114695 0.08556272 ## Pct_Img 0.04849085 0.08593145 0.08060028 ## Pct_brevet 0.03549819 0.06157121 0.05975821 ## NivVieMed 0.11063207 0.17499339 0.14911021 ## ======================================================== ## Z-values: ## Direct Indirect Total ## Pct0_14 -3.2551283 -7.5365951 -9.755241 ## Pct_65 -0.7033432 -0.7080664 -1.194998 ## Pct_Img 2.1460460 7.5238953 9.312658 ## Pct_brevet -2.0742665 -4.8932234 -6.273857 ## NivVieMed -1.6668729 -0.1031306 -1.357766 ## ## p-values: ## Direct Indirect Total ## Pct0_14 0.0011334 4.8184e-14 &lt; 2.22e-16 ## Pct_65 0.4818419 0.47890 0.23209 ## Pct_Img 0.0318693 5.3069e-14 &lt; 2.22e-16 ## Pct_brevet 0.0380546 9.9198e-07 3.5221e-10 ## NivVieMed 0.0955397 0.91786 0.17454 Dépendance du modèle spatiale SLX? Ce modèle a-t-il corrigé le problème de dépendance spatiale du modèle de régression linéaire classique? Avec une valeur du I de Moran de 0,605 (p &lt; 0,001), les résidus sont toujours fortement autocorrélés spatialement (figure 6.2). lm.morantest(Modele.SLX, W.Rook, alternative=&quot;two.sided&quot;) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = formula(paste(&quot;y ~ &quot;, paste(colnames(x)[-1], ## collapse = &quot;+&quot;))), data = as.data.frame(x), weights = weights) ## weights: W.Rook ## ## Moran I statistic standard deviate = 21.951, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided ## sample estimates: ## Observed Moran I Expectation Variance ## 0.6046602748 -0.0072844321 0.0007771643 LyonIris$SLX.Residus &lt;- residuals(Modele.SLX) tm_shape(LyonIris)+ tm_fill(col=&quot;SLX.Residus&quot;, n = 5, style = &quot;quantile&quot;, palette = &quot;-RdBu&quot;, title = &quot;Résidus&quot;) + tm_layout(frame=FALSE) + tm_scale_bar(breaks = c(0,5)) Figure 6.3: Cartographie des résidus du modèle SLX 6.1.2.2 Modèle SAR : autocorrélation spatiale sur la variable dépendante Dans le modèle SAR, l’autocorrélation spatiale est intégrée au niveau de variable indépendante (\\(Wy\\)), qui est ainsi spatialement décalée. L’idée générale est que la valeur de la variable dépendante pour une observation (\\(y_i\\)) peut être influencée par les valeurs de \\(y\\) des observations voisines et proches. L’exemple le plus classique est le prix de vente des maisons : il est influencé à la fois par les caractéristiques intrinsèques de la maison (\\(X\\), par exemple, la superficie habitable, le nombre de chambres à coucher, de salles de bains, etc.) et par le prix de vente des maison voisines (\\(Wy\\)). Dubé et Legros (2014) qualifie ce phénomène « d’effets d’entraînement ou d’effets de débordement (spillover effects) » (2014, 123). L’équation du modèle SAR s’écrit alors : \\[\\begin{equation} y = Wy\\rho + X\\beta + \\epsilon \\tag{6.4} \\end{equation}\\] avec : \\(y\\), la variable dépendante \\(W\\), la matrice de pondération spatiale \\(Wy\\), la variable indépendante spatialement décalée \\(\\rho\\) (prononcez rho), le coefficient de la variable indépendante spatialement décalée. Il varie de -1 à 1. \\(X\\), les variables indépendantes \\(\\beta\\), les coefficients des variables dépendantes \\(\\epsilon\\), les résidus. Construction du modèle SAR dans R Le modèle SAR est construit avec la fonction lagsarlm du package spatialreg. ## Construction du modèle Modele.SAR &lt;- lagsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, listw=W.Rook, # matrice de pondération spatiale data = LyonIris, # dataframe type = &#39;lag&#39;) # Modèle lag par défaut ## Résultats du modèle summary(Modele.SAR, Nagelkerke=TRUE) ## ## Call:lagsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris, listw = W.Rook, type = &quot;lag&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.86859 -1.88111 -0.49760 0.94464 18.21351 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 7.838906 1.646232 4.7617 1.919e-06 ## Pct0_14 -0.098708 0.030554 -3.2306 0.001235 ## Pct_65 -0.034543 0.026957 -1.2814 0.200044 ## Pct_Img 0.030241 0.024491 1.2348 0.216917 ## Pct_brevet -0.019234 0.017855 -1.0772 0.281384 ## NivVieMed -0.098413 0.048985 -2.0090 0.044534 ## ## Rho: 0.87939, LR test value: 620.31, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.01942 ## z-value: 45.283, p-value: &lt; 2.22e-16 ## Wald statistic: 2050.5, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1366.157 for lag model ## ML residual variance (sigma squared): 10.181, (sigma: 3.1908) ## Nagelkerke pseudo-R-squared: 0.78962 ## Number of observations: 506 ## Number of parameters estimated: 8 ## AIC: 2748.3, (AIC for lm: 3366.6) ## LM test for residual autocorrelation ## test value: 0.6198, p-value: 0.43112 Dans les résultats ci-dessous, la valeur de rho est de 0,88 (LR = 620, p &lt; 0,001), traduisant une très fort effet d’entraînement. Autrement dit, lorsqu’en moyenne la concentration de dioxyde d’azote augmente dans les IRIS voisines (\\(Wy\\)), elle augmente aussi fortement chaque IRIS (\\(y\\)). Effets directs, indirects et totaux ## Effets directs, indirects et totaux (uniquement les coefficients) impacts(Modele.SAR, listw = W.Rook, R = 999) ## Impact measures (lag, exact): ## Direct Indirect Total ## Pct0_14 -0.13878038 -0.6796248 -0.8184052 ## Pct_65 -0.04856624 -0.2378349 -0.2864012 ## Pct_Img 0.04251743 0.2082131 0.2507306 ## Pct_brevet -0.02704205 -0.1324283 -0.1594703 ## NivVieMed -0.13836534 -0.6775923 -0.8159576 ## Effets directs, indirects et totaux (coefficients, valeurs de z et de p) summary(impacts(Modele.SAR, listw = W.Rook, R = 999), zstats = TRUE, short = TRUE) ## Impact measures (lag, exact): ## Direct Indirect Total ## Pct0_14 -0.13878038 -0.6796248 -0.8184052 ## Pct_65 -0.04856624 -0.2378349 -0.2864012 ## Pct_Img 0.04251743 0.2082131 0.2507306 ## Pct_brevet -0.02704205 -0.1324283 -0.1594703 ## NivVieMed -0.13836534 -0.6775923 -0.8159576 ## ======================================================== ## Simulation results ( variance matrix): ## ======================================================== ## Simulated standard errors ## Direct Indirect Total ## Pct0_14 0.04106249 0.2369711 0.2734005 ## Pct_65 0.03690214 0.1888120 0.2246685 ## Pct_Img 0.03399363 0.1775616 0.2105140 ## Pct_brevet 0.02448167 0.1268029 0.1506836 ## NivVieMed 0.06940295 0.3573234 0.4234237 ## ## Simulated z-values: ## Direct Indirect Total ## Pct0_14 -3.369976 -2.899864 -3.019612 ## Pct_65 -1.298650 -1.251854 -1.265367 ## Pct_Img 1.237738 1.184090 1.198609 ## Pct_brevet -1.132257 -1.084836 -1.096867 ## NivVieMed -1.988261 -1.906942 -1.935145 ## ## Simulated p-values: ## Direct Indirect Total ## Pct0_14 0.00075175 0.0037332 0.002531 ## Pct_65 0.19406409 0.2106230 0.205740 ## Pct_Img 0.21581308 0.2363775 0.230680 ## Pct_brevet 0.25752652 0.2779945 0.272700 ## NivVieMed 0.04678287 0.0565281 0.052973 Dépendance du modèle spatiale SAR? Ce modèle a-t-il corrigé le problème de dépendance spatiale du modèle de régression linéaire classique? Avec une valeur du I de Moran de -0,014 (p = 0,654), les résidus ne sont plus spatialement autocorrélés (figure 6.4). ## Autocorrélation spatiale des résidus moran.mc(resid(Modele.SAR), W.Rook, nsim=999) ## ## Monte-Carlo simulation of Moran I ## ## data: resid(Modele.SAR) ## weights: W.Rook ## number of simulations + 1: 1000 ## ## statistic = -0.014281, observed rank = 355, p-value = 0.645 ## alternative hypothesis: greater ## Cartographie des résidus LyonIris$SAR.Residus &lt;- resid(Modele.SAR) tm_shape(LyonIris)+ tm_fill(col=&quot;SAR.Residus&quot;, n = 5, style = &quot;quantile&quot;, palette = &quot;-RdBu&quot;, title = &quot;Résidus&quot;) + tm_layout(frame=FALSE) + tm_scale_bar(breaks = c(0,5)) Figure 6.4: Cartographie des résidus du modèle SAR 6.1.2.3 Modèle SEM : autocorrélation spatiale sur le terme d’erreur Dans le modèle SEM (spatial erreur model), l’intégration de l’autocorrélation spatiale est réalisée sur le terme d’erreur, ce qui pourrait se justifier par l’omission d’une variable dépendante spatialement structurée (Dubé et Legros 2014, 126). L’équation du modèle SEM s’écrit : \\[\\begin{equation} y = X\\beta + u \\textrm{, } u = \\lambda Wu + \\epsilon \\tag{6.5} \\end{equation}\\] avec : \\(y\\), la variable dépendante \\(W\\), la matrice de pondération spatiale \\(\\lambda\\) (prononcez lambda), le coefficient sur le terme d’erreur spatialement décalée. Il varie de -1 à 1. \\(X\\), les variables indépendantes \\(\\beta\\), les coefficients des variables dépendantes \\(\\epsilon\\), les résidus. Construction du modèle SAR dans R Le modèle SEM est construit avec la fonction lmSLX du package spatialreg. ## Construction du modèle Modele.SEM &lt;- errorsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, listw=W.Rook, # matrice de pondération spatiale data = LyonIris) # dataframe ## Résultats du modèle summary(Modele.SEM, Nagelkerke=TRUE) ## ## Call:errorsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris, listw = W.Rook) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.86150 -1.83161 -0.44106 0.91029 17.94924 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 30.544576 2.358173 12.9526 &lt; 2e-16 ## Pct0_14 -0.035019 0.033393 -1.0487 0.29431 ## Pct_65 -0.026039 0.028970 -0.8988 0.36874 ## Pct_Img -0.016770 0.026176 -0.6407 0.52175 ## Pct_brevet 0.023708 0.019074 1.2430 0.21388 ## NivVieMed -0.146309 0.060273 -2.4274 0.01521 ## ## Lambda: 0.91138, LR test value: 613.15, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.01651 ## z-value: 55.201, p-value: &lt; 2.22e-16 ## Wald statistic: 3047.2, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1369.737 for error model ## ML residual variance (sigma squared): 9.9971, (sigma: 3.1618) ## Nagelkerke pseudo-R-squared: 0.78662 ## Number of observations: 506 ## Number of parameters estimated: 8 ## AIC: NA (not available for weighted model), (AIC for lm: 3366.6) Dans les résultats ci-dessous, la valeur de lambda est de 0,91 (LR = 613, p &lt; 0,001), traduisant une très forte autocorrélation spatiale sur le terme d’erreur. Dépendance du modèle spatiale SEM? Ce modèle a-t-il corrigé le problème de dépendance spatiale du modèle de régression linéaire classique? Avec une valeur du I de Moran de -0,013 (p = 0,614), les résidus ne sont plus spatialement autocorrélés. ## Autocorrélation spatiale des résidus moran.mc(resid(Modele.SEM), W.Rook, nsim=999) ## ## Monte-Carlo simulation of Moran I ## ## data: resid(Modele.SEM) ## weights: W.Rook ## number of simulations + 1: 1000 ## ## statistic = -0.011827, observed rank = 386, p-value = 0.614 ## alternative hypothesis: greater 6.1.2.4 Modèle SDM : autocorrélation spatiale sur la variable dépendante et les variables indépendantes Le modèle SDM (Spatial Durbin Model en anglais) est un modèle mixte qui intègre à la fois l’autocorrélation spatiale sur la variable dépendante (\\(Wy\\), effets d’entraînement ou de débordement) et sur les variables indépendantes (\\(WX\\), externalités). Il s’écrit alors : \\[\\begin{equation} y = Wy\\rho + X\\beta + WX\\theta + \\epsilon \\tag{6.6} \\end{equation}\\] avec : \\(y\\), la variable dépendante \\(W\\), la matrice de pondération spatiale \\(Wy\\), la variable indépendante spatialement décalée \\(\\rho\\), le coefficient de la variable indépendante spatialement décalée \\(X\\), les variables indépendantes \\(\\beta\\), les coefficients des variables dépendantes \\(WX\\), les variables dépendantes spatiales décalés \\(\\theta\\), les coefficients des variables dépendantes spatiales décalées \\(\\epsilon\\), les résidus. Construction du modèle SDM dans R Le modèle SDM est construit avec la fonction lagsarlm du package spatialreg. Notez que le paramètre type = \"mixed\" qui spécifie l’utilisation d’un modèle mixte. ## Construction du modèle Modele.DurbinSpatial &lt;- lagsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, listw = W.Rook, # matrice de pondération spatiale data = LyonIris, # dataframe type = &quot;mixed&quot;) ## 7.2. Résultats du modèle # Résultats du modèles summary(Modele.DurbinSpatial, Nagelkerke=TRUE) ## ## Call:lagsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris, listw = W.Rook, type = &quot;mixed&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.60922 -1.77753 -0.43909 0.99252 18.15526 ## ## Type: mixed ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.1130457 2.5671301 3.1604 0.001576 ## Pct0_14 -0.0574046 0.0344908 -1.6643 0.096043 ## Pct_65 -0.0238715 0.0293647 -0.8129 0.416256 ## Pct_Img 0.0048364 0.0266560 0.1814 0.856025 ## Pct_brevet 0.0112746 0.0195259 0.5774 0.563656 ## NivVieMed -0.1463876 0.0605853 -2.4162 0.015682 ## lag.Pct0_14 -0.1242574 0.0581170 -2.1381 0.032512 ## lag.Pct_65 0.0255480 0.0499646 0.5113 0.609125 ## lag.Pct_Img 0.1559952 0.0482138 3.2355 0.001214 ## lag.Pct_brevet -0.0883930 0.0342496 -2.5809 0.009856 ## lag.NivVieMed 0.1032469 0.0960201 1.0753 0.282257 ## ## Rho: 0.84127, LR test value: 492.38, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.023363 ## z-value: 36.009, p-value: &lt; 2.22e-16 ## Wald statistic: 1296.7, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1353.106 for mixed model ## ML residual variance (sigma squared): 9.9845, (sigma: 3.1598) ## Nagelkerke pseudo-R-squared: 0.8002 ## Number of observations: 506 ## Number of parameters estimated: 13 ## AIC: 2732.2, (AIC for lm: 3222.6) ## LM test for residual autocorrelation ## test value: 0.0748, p-value: 0.78447 Effets directs, indirects et totaux # Effets directs, indirects et totaux (uniquement les coefficients) impacts(Modele.DurbinSpatial, listw = W.Rook, R = 999) ## Impact measures (mixed, exact): ## Direct Indirect Total ## Pct0_14 -0.12369039 -1.02079497 -1.14448536 ## Pct_65 -0.02177191 0.03233406 0.01056215 ## Pct_Img 0.06632543 0.94692639 1.01325182 ## Pct_brevet -0.01903815 -0.46681402 -0.48585217 ## NivVieMed -0.15403413 -0.11775603 -0.27179016 # Effets directs, indirects et totaux (coefficients, valeurs de z et de p) summary(impacts(Modele.DurbinSpatial, listw = W.Rook, R = 999), zstats = TRUE, short = TRUE) ## Impact measures (mixed, exact): ## Direct Indirect Total ## Pct0_14 -0.12369039 -1.02079497 -1.14448536 ## Pct_65 -0.02177191 0.03233406 0.01056215 ## Pct_Img 0.06632543 0.94692639 1.01325182 ## Pct_brevet -0.01903815 -0.46681402 -0.48585217 ## NivVieMed -0.15403413 -0.11775603 -0.27179016 ## ======================================================== ## Simulation results ( variance matrix): ## ======================================================== ## Simulated standard errors ## Direct Indirect Total ## Pct0_14 0.04387620 0.3238569 0.3511112 ## Pct_65 0.03564555 0.2798638 0.3023572 ## Pct_Img 0.03208904 0.2730492 0.2910092 ## Pct_brevet 0.02419487 0.1929199 0.2082737 ## NivVieMed 0.06923741 0.5025663 0.5366255 ## ## Simulated z-values: ## Direct Indirect Total ## Pct0_14 -2.8402032 -3.20678815 -3.31279040 ## Pct_65 -0.6312146 0.06884458 -0.01069228 ## Pct_Img 2.0462274 3.47880743 3.48974241 ## Pct_brevet -0.7646549 -2.38010280 -2.29347248 ## NivVieMed -2.2104748 -0.21362006 -0.48526546 ## ## Simulated p-values: ## Direct Indirect Total ## Pct0_14 0.0045085 0.00134226 0.00092370 ## Pct_65 0.5279002 0.94511333 0.99146896 ## Pct_Img 0.0407340 0.00050365 0.00048349 ## Pct_brevet 0.4444770 0.01730781 0.02182082 ## NivVieMed 0.0270722 0.83084335 0.62748806 Dépendance du modèle spatiale SDM? moran.mc(resid(Modele.DurbinSpatial), W.Rook, nsim=999) ## ## Monte-Carlo simulation of Moran I ## ## data: resid(Modele.DurbinSpatial) ## weights: W.Rook ## number of simulations + 1: 1000 ## ## statistic = -0.0046127, observed rank = 470, p-value = 0.53 ## alternative hypothesis: greater 6.1.2.5 Modèle SDEM : autocorrélation spatiale sur les variables indépendantes et sur le terme d’erreur Le modèle SDEM (Spatial Durbin Error Model en anglais) est un autre modèle mixte qui intègre à la fois l’autocorrélation spatiale sur les valeurs indépendantes (\\(WX\\), externalités) et sur le terme d’erreur (\\(u = \\lambda Wu + \\epsilon\\)). Il s’écrit alors : \\[\\begin{equation} y = X\\beta + WX\\theta + u \\textrm{, } u = \\lambda Wu + \\epsilon \\tag{6.7} \\end{equation}\\] avec : \\(y\\), la variable dépendante \\(W\\), la matrice de pondération spatiale \\(X\\), les variables indépendantes \\(\\beta\\), les coefficients des variables dépendantes \\(WX\\), les variables dépendantes spatiales décalés \\(\\theta\\), les coefficients des variables dépendantes spatiales décalées \\(\\lambda\\) (prononcez lambda), le coefficient sur le terme d’erreur spatialement décalée \\(X\\), les variables indépendantes \\(\\beta\\), les coefficients des variables dépendantes \\(\\epsilon\\), les résidus. Construction du modèle SDEM dans R Le modèle SDEM est construit avec la fonction errorsarlm du package spatialreg. Notez que le paramètre etype = \"mixed\" qui spécifie l’utilisation d’un modèle mixte. ## Construction du modèle Modele.DurbinErreur &lt;- errorsarlm(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, listw=W.Rook, # matrice de pondération spatiale data = LyonIris, # dataframe etype = &#39;emixed&#39;) ## Résultats du modèle summary(Modele.DurbinErreur, Nagelkerke=TRUE) ## ## Call:errorsarlm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris, listw = W.Rook, etype = &quot;emixed&quot;) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.99324 -1.82407 -0.45644 1.06084 18.21108 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 37.0610133 6.5010177 5.7008 1.192e-08 ## Pct0_14 -0.0819977 0.0416988 -1.9664 0.04925 ## Pct_65 -0.0263294 0.0347137 -0.7585 0.44817 ## Pct_Img 0.0046560 0.0310281 0.1501 0.88072 ## Pct_brevet 0.0097849 0.0238839 0.4097 0.68203 ## NivVieMed -0.1678555 0.0680048 -2.4683 0.01358 ## lag.Pct0_14 -0.1767469 0.1023451 -1.7270 0.08417 ## lag.Pct_65 0.0105325 0.0891835 0.1181 0.90599 ## lag.Pct_Img 0.0927851 0.0797036 1.1641 0.24437 ## lag.Pct_brevet -0.0380482 0.0566883 -0.6712 0.50210 ## lag.NivVieMed -0.1025307 0.1724055 -0.5947 0.55204 ## ## Lambda: 0.8976, LR test value: 464.09, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.018242 ## z-value: 49.204, p-value: &lt; 2.22e-16 ## Wald statistic: 2421, p-value: &lt; 2.22e-16 ## ## Log likelihood: -1367.25 for error model ## ML residual variance (sigma squared): 10.046, (sigma: 3.1696) ## Nagelkerke pseudo-R-squared: 0.78871 ## Number of observations: 506 ## Number of parameters estimated: 13 ## AIC: NA (not available for weighted model), (AIC for lm: 3222.6) Effets directs, indirects et totaux ## Effets directs, indirects et totaux (uniquement les coefficients) impacts(Modele.DurbinErreur, listw = W.Rook, R = 999) ## Impact measures (SDEM, glht): ## Direct Indirect Total ## Pct0_14 -0.081997661 -0.17674688 -0.25874455 ## Pct_65 -0.026329377 0.01053246 -0.01579692 ## Pct_Img 0.004656049 0.09278514 0.09744119 ## Pct_brevet 0.009784949 -0.03804816 -0.02826321 ## NivVieMed -0.167855503 -0.10253070 -0.27038620 ## Effets directs, indirects et totaux (coefficients, valeurs de z et de p) summary(impacts(Modele.DurbinErreur, listw = W.Rook, R = 999), zstats = TRUE, short = TRUE) ## Impact measures (SDEM, glht, n): ## Direct Indirect Total ## Pct0_14 -0.081997661 -0.17674688 -0.25874455 ## Pct_65 -0.026329377 0.01053246 -0.01579692 ## Pct_Img 0.004656049 0.09278514 0.09744119 ## Pct_brevet 0.009784949 -0.03804816 -0.02826321 ## NivVieMed -0.167855503 -0.10253070 -0.27038620 ## ======================================================== ## Standard errors: ## Direct Indirect Total ## Pct0_14 0.04169878 0.10234506 0.13146452 ## Pct_65 0.03471367 0.08918350 0.11192948 ## Pct_Img 0.03102807 0.07970364 0.09949722 ## Pct_brevet 0.02388387 0.05668833 0.07344175 ## NivVieMed 0.06800483 0.17240548 0.21172909 ## ======================================================== ## Z-values: ## Direct Indirect Total ## Pct0_14 -1.9664284 -1.7269703 -1.9681701 ## Pct_65 -0.7584729 0.1180987 -0.1411328 ## Pct_Img 0.1500592 1.1641268 0.9793358 ## Pct_brevet 0.4096885 -0.6711815 -0.3848384 ## NivVieMed -2.4682881 -0.5947067 -1.2770386 ## ## p-values: ## Direct Indirect Total ## Pct0_14 0.049249 0.084173 0.049048 ## Pct_65 0.448168 0.905989 0.887765 ## Pct_Img 0.880718 0.244373 0.327414 ## Pct_brevet 0.682034 0.502105 0.700357 ## NivVieMed 0.013576 0.552040 0.201589 Dépendance du modèle spatiale SDEM? moran.mc(resid(Modele.DurbinErreur), W.Rook, nsim=999) ## ## Monte-Carlo simulation of Moran I ## ## data: resid(Modele.DurbinErreur) ## weights: W.Rook ## number of simulations + 1: 1000 ## ## statistic = -0.010362, observed rank = 381, p-value = 0.619 ## alternative hypothesis: greater 6.1.3 Quel modèle choisir? 6.1.3.1 Tests du multiplicateur de Lagrange sur le modèle MCO L’utilisation des tests du multiplicateur de Lagrange (simple et robuste) a été largement popularisée par Anselin et al. (1996) pour vérifier si le recours à un modèle autorégressif est nécessaire, comparativement à un modèle de régression classique (MCO). Les tests sont calculés sur le modèle MCO avec le fonction lm.LMtests et une matrice pondération spatiale. Ces tests permettent aussi de choisir entre les modèles SAR et SEM. La démarche suivante peut être utilisée pour choisir un modèle : Si toutes les valeurs des tests (simples et robustes) sont non significatives (p &gt; 0,05) alors le recours à un modèle autorégressif n’est pas nécessaire. Nous pouvons conserver le modèle de régression classique (MCO). Si les valeurs de LMlag ou RLMlag sont non significatives (p &gt; 0,05) alors le recours au modèle SAR n’est pas nécessaire. Si les valeurs de LMerr ou RLMerr sont non significatives (p &gt; 0,05) alors le recours au modèle SEM n’est pas nécessaire. Si les valeurs de RLMlag et RLMerr sont significatives (p &lt; 0,001), nous choisissons le modèle ayant la plus forte statistique. Dans les résultats ci-dessous, nous ne retenons pas le modèle SEM car la valeur de 0,740 pour le RLMerr n’est pas significative (p = 0,3898). Par contre, les valeurs LMlag et de RLMlag (555 et 123) sont significatives, ce que justifie la sélection du modèle SAR. summary(lm.LMtests(model = Modele.MCO, listw = W.Rook, test = c(&quot;LMlag&quot;,&quot;LMerr&quot;,&quot;RLMlag&quot;,&quot;RLMerr&quot;))) ## Lagrange multiplier diagnostics for spatial dependence ## data: ## model: lm(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris) ## weights: W.Rook ## ## statistic parameter p.value ## LMlag 554.65778 1 &lt;2e-16 *** ## LMerr 432.83282 1 &lt;2e-16 *** ## RLMlag 122.56452 1 &lt;2e-16 *** ## RLMerr 0.73955 1 0.3898 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.1.3.2 Comparaison des modèles mixtes et non mixtes Nous avons vu qu’il existe deux modèles mixtes (SDM et SDEM). Il convient alors de vérifier si le recours d’un modèle mixte est justifié comparativement à un modèle non mixte. Dans le code ci-dessous, nous vérifions ci le modèle SDM est statistiquement différent du modèle SAR avec les fonctions LR.Sarlm et anova. Les résultats signalent un écart significatif des valeurs du log-vraisemblance (26,101, p &lt; 0,001). Par conséquent, ce modèle mixte a un apport significatif. ## SDM et SEM sont-ils significativement différents? LR.Sarlm(Modele.DurbinSpatial, Modele.SAR) ## ## Likelihood ratio for spatial linear models ## ## data: ## Likelihood ratio = 26.101, df = 5, p-value = 8.528e-05 ## sample estimates: ## Log likelihood of Modele.DurbinSpatial Log likelihood of Modele.SAR ## -1353.106 -1366.157 anova(Modele.DurbinSpatial, Modele.SAR) ## Model df AIC logLik Test L.Ratio p-value ## Modele.DurbinSpatial 1 13 2732.2 -1353.1 1 ## Modele.SAR 2 8 2748.3 -1366.2 2 26.101 8.5283e-05 À l’inverse, la différence entre les valeurs du log-vraisemblance du modèle SDEM et SEM n’est pas significative (4,9728, p = 0,42), signalant que l’utilisation d’un modèle SDEM comparativement à un modèle SEM n’est pas nécessaire. ## SDEM et SEM sont-ils significativement différents? LR.Sarlm(Modele.DurbinErreur, Modele.SEM) ## ## Likelihood ratio for spatial linear models ## ## data: ## Likelihood ratio = 4.9728, df = 5, p-value = 0.4192 ## sample estimates: ## Log likelihood of Modele.DurbinErreur Log likelihood of Modele.SEM ## -1367.250 -1369.737 anova(Modele.DurbinErreur, Modele.SEM) ## Model df AIC logLik Test L.Ratio p-value ## Modele.DurbinErreur 1 13 2760.5 -1367.2 1 ## Modele.SEM 2 8 2755.5 -1369.7 2 4.9728 0.4192 6.1.3.3 Mesures AIC et BIC et dépendance spatiale Les mesures AIC et BIC sont largement utilisées pour évaluer la qualité d’ajustement du modèle. Plus leurs valeurs sont faibles, meilleur est le modèle. Il est donc possible de comparer leurs valeurs pour les différents modèles (MCO, SLX, SAR, SEM, SDM et SDEM). Nous pouvons aussi comparer l’autocorrélation spatiale des résidus des modèles avec le I de Moran. ## Valeurs d&#39;AIC et de BIC AICs &lt;- AIC(Modele.MCO, Modele.SLX, Modele.SAR, Modele.SEM, Modele.DurbinSpatial, Modele.DurbinErreur) BICs &lt;- BIC(Modele.MCO, Modele.SLX, Modele.SAR, Modele.SEM, Modele.DurbinSpatial, Modele.DurbinErreur) ## AUtocorrélation spatiale des résidus IMoran.MCO &lt;- moran.mc(resid(Modele.MCO), W.Rook, nsim=999) IMoran.SLX &lt;- moran.mc(resid(Modele.SLX), W.Rook, nsim=999) IMoran.SLM &lt;- moran.mc(resid(Modele.SAR), W.Rook, nsim=999) IMoran.SEM &lt;- moran.mc(resid(Modele.SEM), W.Rook, nsim=999) IMoran.DurbinS &lt;- moran.mc(resid(Modele.DurbinSpatial), W.Rook, nsim=999) IMoran.DurbinE &lt;- moran.mc(resid(Modele.DurbinErreur), W.Rook, nsim=999) MoranI.s &lt;- c(IMoran.MCO$statistic, IMoran.SLX$statistic, IMoran.SLM$statistic, IMoran.SEM$statistic, IMoran.DurbinS$statistic, IMoran.DurbinE$statistic) MoranI.p &lt;- c(IMoran.MCO$p.value, IMoran.SLX$p.value, IMoran.SLM$p.value, IMoran.SEM$p.value, IMoran.DurbinS$p.value, IMoran.DurbinE$p.value) ## Tableau Comparaison &lt;- data.frame(Modele = c(&quot;MCO&quot;, &quot;SLX&quot;, &quot;SAR&quot;, &quot;SEM&quot;, &quot;Durbin S&quot;, &quot;Durbin E&quot;), AIC = AICs$AIC, BIC = BICs$BIC, dl = AICs$df, MoranI = MoranI.s, MoranIp = MoranI.p) Comparaison ## Modele AIC BIC dl MoranI MoranIp ## 1 MCO 3366.626 3396.212 7 0.587312061 0.001 ## 2 SLX 3222.594 3273.313 12 0.604660275 0.001 ## 3 SAR 2748.314 2782.126 8 -0.014281059 0.640 ## 4 SEM 2755.474 2789.286 8 -0.011826630 0.618 ## 5 Durbin S 2732.212 2787.157 13 -0.004612686 0.514 ## 6 Durbin E 2760.501 2815.446 13 -0.010361614 0.621 Quelques lignes de code suffissent pour créer deux graphiques permettant de comparer visuellement les résultats des différents modèles (figure 6.5). Les résultats démontrent que : les modèles MCO et SLX ont un problème de dépendance spatiale puisque leur résidus sont significativement autocorrélés. spatialement. Il ne devraient donc pas être retenus. Le modèle mixte SDM et le modèle non mixte SEM ont les valeurs sont les plus performants avec les valeurs d’AIC les plus faibles. Figure 6.5: Compararison des différents modèles References "],["sect062.html", "6.2 Modèles généralisés additifs (GAM) avec une spline bivariée sur les coordonnées géographiques", " 6.2 Modèles généralisés additifs (GAM) avec une spline bivariée sur les coordonnées géographiques 6.2.1 Principe de base d’un GAM intégrant l’espace Les modèles additifs généralisés (Generalized additive models en anglais) permettent d’intégrer à la fois des effets linéaires et des effets non linéaires avec des splines. Modèles généralisés additifs Pour une description détaillée des modèles généralisés additifs, nous vous invitons vivement à lire le chapitre suivant (Apparicio et Gelb 2022). Il est alors possible d’introduire dans le modèle une spline bivariée sur les coordonnées géographiques. L’équation du modèle s’écrit alors : \\[\\begin{equation} g(Y) \\ = \\beta_{0} \\ + X\\beta + s(CoordX,CoordY) \\ + \\epsilon \\tag{6.8} \\end{equation}\\] avec : \\(y\\), la variable dépendante \\(\\beta_{0}\\), la constante \\(X\\), les variables indépendantes \\(\\beta\\), les coefficients des variables dépendantes \\(s(CoordX,CoordY)\\), spline bivariée sur les coordonnées x et y \\(\\epsilon\\), les résidus. L’intérêt de recourir à une spline bivariée sur les coordonnées géographiques est double : Contrôler l’effet de la localisation sur la variable dépendante (\\(y\\)). Les coefficients des autres variables indépendantes sont ainsi obtenus une fois l’espace pris en compte. Évaluer l’effet de la localisation (patron spatial), une fois les autres variables indépendantes contrôlées. Autrement dit, toutes choses étant égales par ailleurs, quel est l’effet de la localisation sur la variable dépendante? 6.2.2 Construction d’un modèle GAM dans R 6.2.2.1 Réalisation du modèle GAM Pour construire des modèles GAM dans R, nous utilisons la fonction gam du package mgcv (Wood 2011). library(mgcv) ## Ajout des coordonnées X et Y dans LyonIris xy &lt;- st_coordinates(st_centroid(LyonIris)) ## Warning: st_centroid assumes attributes are constant over geometries LyonIris$X &lt;- xy[,1] LyonIris$Y &lt;- xy[,2] ## Construction du modèles GAM Modele.GAM1 &lt;- gam(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed+ s(X, Y), # spline sur les coordonnées X, Y data = LyonIris) # dataframe ## Résultat du modèle summary(Modele.GAM1) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed + s(X, ## Y) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.402313 2.156322 14.099 &lt;2e-16 *** ## Pct0_14 -0.053641 0.046224 -1.160 0.246 ## Pct_65 -0.056310 0.038476 -1.463 0.144 ## Pct_Img 0.008033 0.035397 0.227 0.821 ## Pct_brevet 0.043874 0.027518 1.594 0.112 ## NivVieMed -0.043282 0.072461 -0.597 0.551 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(X,Y) 26.5 28.62 27.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.715 Deviance explained = 73.3% ## GCV = 18.814 Scale est. = 17.605 n = 506 Les résultats ci-dessous signalent que la localisation a un effet très significatif puisque (s(X,Y) = 26,5 avec p &lt; 0,001). Notez que la valeur de p permet de déterminer si la spline bivariée (et donc l’espace) a ou non un effet significatif. Si la valeur de p est inférieure à 0,05 alors il n’est pas été nécessaire de conserver la spline bivariée sur les coordonnées géographiquement. De plus, le code ci-dessous permet de constater que le modèle GAM est performant que le modèle linéaire multiple classique (MCO). anova(Modele.MCO, Modele.GAM1) ## Analysis of Variance Table ## ## Model 1: NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed ## Model 2: NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed + s(X, ## Y) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 500.0 22346.0 ## 2 473.5 8336.1 26.5 14010 30.029 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Nous pouvons aussi introduire une spline plus complexe en augmentant le nombre de nœuds à 40. Modele.GAM2 &lt;- gam(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed+ s(X, Y, k= 40), data = LyonIris) summary(Modele.GAM2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + NivVieMed + s(X, ## Y, k = 40) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.372093 2.088160 14.545 &lt;2e-16 *** ## Pct0_14 -0.055294 0.044285 -1.249 0.2124 ## Pct_65 -0.049122 0.036870 -1.332 0.1834 ## Pct_Img 0.002226 0.033722 0.066 0.9474 ## Pct_brevet 0.052229 0.026315 1.985 0.0478 * ## NivVieMed -0.050991 0.070081 -0.728 0.4672 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(X,Y) 35.68 38.51 24.27 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.747 Deviance explained = 76.7% ## GCV = 17.014 Scale est. = 15.613 n = 506 La valeur plus faible d’AIC pour le second modèle GAM signale qu’il est plus performant que le premier. AIC(Modele.MCO, Modele.GAM1, Modele.GAM2) ## df AIC ## Modele.MCO 7.00000 3366.626 ## Modele.GAM1 33.50046 2920.682 ## Modele.GAM2 42.67520 2868.355 6.2.2.2 Visualisation de l’effet de l’espace Pour visualiser les prédictions du modèle dans l’espace, toutes choses étant égales par ailleurs, nous utilisons la fonction vis.gam (figure 6.6). Les contours signalent qu’au centre de Lyon, les valeurs de dioxyde d’azote sont les plus élevées et dépassent même 40 \\(\\mu\\)g/m3. vis.gam(Modele.GAM2, view=c(&quot;X&quot;, &quot;Y&quot;), plot.type = &quot;contour&quot;, color=&quot;terrain&quot;) Figure 6.6: Visualisation des prédictions dans l’espace avec la fonction vis.gam Toutefois, il est plus intéressant de la représenter dans un raster, une fois contrôlée les autres variables indépendantes. Pour ce faire, six étapes sont nécessaires : Créer d’une grid. Fixer les autres paramètres à leur moyenne respective. Calculer la prédiction pour la localisation. Centrer la prédiction. Construire le raster avec les prédictions. Découper et cartographier le raster. library(raster) library(terra) ## Étape 1 : création d&#39;une grid pour la prédiction de 100 mètres de résolution spatiale Xcoords &lt;- seq(min(LyonIris$X-100), max(LyonIris$X+100), by=100) Ycoords &lt;- seq(min(LyonIris$Y-100), max(LyonIris$Y+100), by=100) PredDF &lt;- expand.grid(Xcoords,Ycoords) names(PredDF) &lt;- c(&quot;X&quot;,&quot;Y&quot;) ## Étape 2 : fixons tous les autres paramètres à leur moyenne for(Var in c(&quot;VegHautPrt&quot;,&quot;Pct0_14&quot;,&quot;Pct_65&quot;,&quot;Pct_Img&quot;,&quot;Pct_brevet&quot;, &quot;NivVieMed&quot;)){ PredDF[[Var]] &lt;- mean(LyonIris[[Var]]) } ## Étape 3 : calcul de la prédiction PredDF$PM25 &lt;- predict(Modele.GAM2,newdata=PredDF) ## Étape 4 : centrage de la prédiction (sans la constante) PredDF$CenterPredPM25 &lt;- PredDF$PM25 - mean(PredDF$PM25) ### Étape 5 : construction du raster rasterGAM &lt;- rasterFromXYZ(PredDF[, c(&quot;X&quot;, &quot;Y&quot;, &quot;CenterPredPM25&quot;)]) crs(rasterGAM) &lt;- crs(as(LyonIris, &quot;Spatial&quot;)) rasterGAM &lt;- rast(rasterGAM) ### Étape 6 : découper et cartographier le raster LyonIris.SpatVector &lt;- vect(LyonIris) rasterGAM &lt;- terra::mask(rasterGAM, LyonIris.SpatVector) terra::plot(rasterGAM) Figure 6.7: Visualisation de l’effet de la localisation centrée sur zéro La figure 6.7 signale que dans le centre de Lyon, le dioxyde d’azote est plus élevée de 10 à 20 \\(\\mu\\)g/m3, toutes choses étant égales par ailleurs. À l’inverse, dans les zones périphériques, il est faible. Cela signale un net patron spatial décroissant du centre vers la périphérie. 6.2.2.3 Dépendance spatiale du modèle GAM Par contre, bien que l’autocorrélation spatiale des résidus du modèle GAM soit plus faible que pour le modèle MCO (I de Moran de 0,337 contre 0,570 avec p &lt; 0,001), il reste que le problème de la dépendance spatiale n’est pas corrigé. moran.mc(resid(Modele.GAM2), W.Rook, nsim=999) ## ## Monte-Carlo simulation of Moran I ## ## data: resid(Modele.GAM2) ## weights: W.Rook ## number of simulations + 1: 1000 ## ## statistic = 0.33664, observed rank = 1000, p-value = 0.001 ## alternative hypothesis: greater References "],["sect063.html", "6.3 Régression géographiquement pondérée", " 6.3 Régression géographiquement pondérée La régression géographiquement pondérée (geographically weighted regression - GWR, en anglais) a été proposée par Fotheringham et al. (2003) pour modéliser une variable continue. Depuis, plusieurs extensions ont été proposées, notamment des GWR mixtes, des GWR logistique ou Poisson. Dans le cadre de cette section, nous abordons uniquement sa forme classique. 6.3.1 Principe de base Pourquoi recourir à la GWR? Dans la section 6.1, nous avons vu que les modèles autorégressifs visent à contrôler la dépendance spatiale d’un modèle de régression classique (MCO), afin d’améliorer l’estimation des coefficients de régression. L’objectif des modèles de régression géographiquement pondérée est différent : ils visent à analyser les variations spatiales de la relation entre la variable dépendante et les variables indépendantes. Autrement dit, les modèles GWR visent à explorer l’instabilité spatiale du modèle MCO afin d’analyser localement la relation entre la variable dépendante et les variables indépendantes. Pour une description détaillée en français en la GWR, consultez Apparicio et al. (2007). Formulation de la GWR Contrairement à la régression linéaire classique et aux modèles spatiaux autorégressifs qui produisent une équation pour l’ensemble du tableau de données, la GWR produit une équation pour chaque unité spatiale i et ainsi, des valeurs locales de R2, \\(\\beta_0\\), \\(\\beta_k\\), t de Student, etc. La résolution de cette équation de régression locale est aussi basée sur la méthode des moindres carrés et sur une matrice de pondération W(i) dont les valeurs décroissent en fonction de la distance séparant les unités i et j. Autrement dit, plus j est proche de i, plus sa pondération est élevée et donc plus son « rôle » dans la détermination de l’équation de régression locale de i est important. De la sorte, la GWR est une extension de la régression linéaire multiple classique où \\((u_i, v_i)\\) représente les coordonnées géographiques du centroïde de l’unité spatiale et où les paramètres \\(\\beta_0\\) et \\(\\beta_k\\) peuvent varier dans l’espace (équation (6.9)). \\[\\begin{equation} y_i = \\beta_0(u_i, v_i)+ \\sum_{j=1}^k \\beta_j(u_i, v_i)x_{ij}+ \\epsilon_i \\tag{6.9} \\end{equation}\\] avec : \\((u_i, v_i)\\) sont les coordonnées géographiques de l’unité spatiale i. \\(y_i\\), la variable dépendante pour l’unité spatiale i. \\(\\beta_0(u_i, v_i)\\), la constante pour l’unité spatiale i aux coordonnées géographiques \\((u_i, v_i)\\). \\(\\beta_j(u_i, v_i)\\), le coefficient de régression pour la variable \\(x_j\\) (avec k variables indépendantes) pour pour l’unité spatiale i aux coordonnées géographiques \\((u_i, v_i)\\). \\(x_{ij}\\), la valeur de la variable indépendante \\(x_j\\) pour l’unité spatiale i. \\(\\epsilon_i\\), le terme d’erreur pour l’unité spatiale i.. Fotheringham et al. (2003) proposent deux fonctions kernel pour définir la pondération W(i) dans le modèle GWR : une fonction gaussienne (équation (6.10)) et une fonction bicarrée (équation (6.11)) où \\(d_{ij}\\) représente la distance euclidienne entre les points i et j et b, le rayon de zone d’influence autour du point i (bandwidth). Il existe une différence fondamentale entre les deux : la fonction gaussienne accorde un poids non nul à tous les points de l’espace d’étude aussi loin soient-ils, tandis que la fonction bicarrée ne tient pas compte des points distants à plus de b mètres de i, tel qu’illustré à la figure 6.8 avec une valeur fixée à 5000 mètres en guise d’exemple. \\[\\begin{equation} w_{ij} = exp[-.5(d_{ij}/b)^2] \\tag{6.10} \\end{equation}\\] \\[\\begin{equation} w_{ij} = [1-(d_{ij}/b)^2]^2 \\text{ si } d_{ij}&lt; b \\text{, sinon } w_{ij}=0 \\tag{6.11} \\end{equation}\\] Figure 6.8: Fonctions kernel pour définir la matrice de pondération W(i) Dans le modèle GWR, la valeur de b est soit fixée par la personne utilisatrice, soit optimisée avec la valeur de CV (cross-validation) ou celle de l’AIC. Notez qu’il est possible d’optimiser la taille de la zone d’influence à partir de la distance euclidienne ou du nombre de plus proches voisins. 6.3.2 Construction et analyse du modèle GWR dans R Pour construire un modèle GWR dans R, nous utilisons le package spgwr (Bivand et Yu 2023). La construction d’un modèle GWR comprend les étapes suivantes : Sélection de la taille de la zone d’influence (bandwith) optimale. Réalisation de la GWR avec la taille de la zone d’influence optimale. Comparaison des modèles MCO et GWR. Cartographie des résultats du modèle GWR (R2, coefficients, valeurs de t, etc.) 6.3.2.1 Définition de la taille de la zone d’influence La sélection de la taille de la zone d’influence optimale est réalisée avec la fonction gwr.sel pour laquelle : le paramètre gweight permet de spécifier une fonction kernel gaussienne (gwr.gauss) ou bicarrée (gwr.gauss). le paramètre adapt permet de spécifier si vous optimez le nombre de plus proches voisins (adapt=TRUE) ou la distance (adapt=FALSE). library(spgwr) ## Optimisation du nombre de voisins avec le CV bwaCV.voisins &lt;- gwr.sel(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, data = LyonIris, method = &quot;cv&quot;, # Méthode cv ou AIC gweight=gwr.bisquare, # gwr.gauss ou gwr.bisquare adapt=TRUE, verbose = FALSE, RMSE = TRUE, longlat = FALSE, coords=cbind(LyonIris$X,LyonIris$Y)) ## Optimisation du nombre de voisins avec l&#39;AIC bwaAIC.voisins &lt;- gwr.sel(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, data = LyonIris, method = &quot;AIC&quot;, # Méthode cv ou AIC gweight=gwr.bisquare, # gwr.gauss ou gwr.bisquare adapt=TRUE, # adaptatif verbose = FALSE, RMSE = TRUE, longlat = FALSE, coords=cbind(LyonIris$X,LyonIris$Y)) ## Optimisation de la distance avec le CV bwnaCV.dist &lt;- gwr.sel(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, data = LyonIris, method = &quot;cv&quot;, # méthode cv ou AIC gweight=gwr.Gauss, # gwr.gauss ou gwr.bisquare adapt=FALSE, # non adaptatif verbose = FALSE, RMSE = TRUE, longlat = FALSE, coords=cbind(LyonIris$X,LyonIris$Y)) ## Optimisation de la distance avec l&#39;AIC bwnaAIC.dist &lt;- gwr.sel(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, data = LyonIris, method = &quot;AIC&quot;, # méthode cv ou AIC gweight=gwr.Gauss, # gwr.gauss ou gwr.bisquare adapt=FALSE, # non adaptatif RMSE = TRUE, verbose = FALSE, longlat = FALSE, coords=cbind(LyonIris$X,LyonIris$Y)) ## Affichage des résultats d&#39;optimisation cat(&quot;Sélection de la taille de la zone optimale (bandwith)&quot;, &quot;\\n avec le nombre de plus proches voisins :&quot;, &quot;\\n CV =&quot;, round(bwaCV.voisins,4), &quot;nombre de voisins =&quot;, round(bwaCV.voisins*nrow(LyonIris)), &quot;\\n AIC =&quot;, round(bwaAIC.voisins,4), &quot;nombre de voisins =&quot;, round(bwaAIC.voisins*nrow(LyonIris)), &quot;\\nSélection de la taille de la zone optimale (bandwith) avec la distance :&quot;, &quot;\\n CV =&quot;, round(bwnaCV.dist, 0), &quot;mètres&quot;, &quot;\\n AIC =&quot;, round(bwnaAIC.dist, 0), &quot;mètres&quot;) ## Sélection de la taille de la zone optimale (bandwith) ## avec le nombre de plus proches voisins : ## CV = 0.1818 nombre de voisins = 92 ## AIC = 0.1067 nombre de voisins = 54 ## Sélection de la taille de la zone optimale (bandwith) avec la distance : ## CV = 1315 mètres ## AIC = 1662 mètres Les résultats ci-dessous montrent que le nombre de plus proche voisins pourrait être de 92 selon l’approche cross-validation et de 54 selon la méthode basée sur l’AIC. Si la valeur de b est basée sur la distance, elle serait alors optimale à 1315 et 1662 mètres selon les deux méthodes. 6.3.2.2 Réalisation de la GWR Avec la fonction gwr, nous estimons un modèle GWR avec un kernel bicarrée et un nombre de plus voisins optimisée selon la méthode CV, soit 92. Modele.GWR &lt;- gwr(NO2 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed, data = LyonIris, adapt=bwaCV.voisins, gweight=gwr.bisquare, hatmatrix=TRUE, se.fit=TRUE, coords=cbind(LyonIris$X,LyonIris$Y), longlat=F) Le code ci-dessous permet de renvoyer les statistiques univariées des coefficients des 506 régressions locales, réalisées pour chacune des 506 entités spatiales (IRIS) et les statistiques d’ajustement du modèle (AIC, R2 global, etc.) Modele.GWR ## Call: ## gwr(formula = NO2 ~ Pct0_14 + Pct_65 + Pct_Img + Pct_brevet + ## NivVieMed, data = LyonIris, coords = cbind(LyonIris$X, LyonIris$Y), ## gweight = gwr.bisquare, adapt = bwaCV.voisins, hatmatrix = TRUE, ## longlat = F, se.fit = TRUE) ## Kernel function: gwr.bisquare ## Adaptive quantile: 0.1818192 (about 92 of 506 data points) ## Summary of GWR coefficient estimates at data points: ## Min. 1st Qu. Median 3rd Qu. Max. Global ## X.Intercept. 12.429518 30.249511 38.619342 48.038863 60.098584 49.4330 ## Pct0_14 -1.094802 -0.360556 -0.215643 -0.047687 0.382801 -0.5335 ## Pct_65 -0.715331 -0.158253 -0.031353 0.076086 0.464992 -0.1505 ## Pct_Img -0.331892 -0.049146 0.077177 0.240755 0.670433 0.2829 ## Pct_brevet -0.655221 -0.221655 -0.084954 0.047835 0.598456 -0.2400 ## NivVieMed -1.140895 -0.560649 -0.214717 0.193768 1.311228 -0.3162 ## Number of data points: 506 ## Effective number of parameters (residual: 2traceS - traceS&#39;S): 107.1278 ## Effective degrees of freedom (residual: 2traceS - traceS&#39;S): 398.8722 ## Sigma (residual: 2traceS - traceS&#39;S): 4.118116 ## Effective number of parameters (model: traceS): 81.53263 ## Effective degrees of freedom (model: traceS): 424.4674 ## Sigma (model: traceS): 3.992025 ## Sigma (ML): 3.656286 ## AICc (GWR p. 61, eq 2.33; p. 96, eq. 4.21): 2945.674 ## AIC (GWR p. 96, eq. 4.22): 2829.504 ## Residual sum of squares: 6764.424 ## Quasi-global R2: 0.783008 6.3.2.3 Comparaison des modèles MCO et GWR Le R2 global du modèle GWR est bien supérieur au modèle classique MCO (0,783 contre 0,283). Fotheringham et al. (2003) proposent plusieurs tests pour comparer les modèles GWR et classique qui sont implémentées dans le package spgwr (fonctions anova(Modele.GWR), anova(Modele.GWR, approx=TRUE), LMZ.F1GWR.test(Modele.GWR), LMZ.F2GWR.test(Modele.GWR). Si les valeurs de p de ces tests sont inférieures à 0,05 alors le modèle GWR améliore de façon significative la capacité prédictive du modèle de régression globale, ce que confirment les résultats ci-dessous. anova(Modele.GWR) ## Analysis of Variance Table ## Df Sum Sq Mean Sq F value ## OLS Residuals 6.00 22346.0 ## GWR Improvement 101.13 15581.6 154.078 ## GWR Residuals 398.87 6764.4 16.959 9.0854 anova(Modele.GWR, approx=TRUE) ## Analysis of Variance Table ## approximate degrees of freedom (only tr(S)) ## Df Sum Sq Mean Sq F value ## OLS Residuals 6.000 22346.0 ## GWR Improvement 75.533 15581.6 206.290 ## GWR Residuals 424.467 6764.4 15.936 12.945 LMZ.F1GWR.test(Modele.GWR) ## ## Leung et al. (2000) F(1) test ## ## data: Modele.GWR ## F = 0.37946, df1 = 430.81, df2 = 500.00, p-value &lt; 2.2e-16 ## alternative hypothesis: less ## sample estimates: ## SS OLS residuals SS GWR residuals ## 22346.021 6764.424 LMZ.F2GWR.test(Modele.GWR) ## ## Leung et al. (2000) F(2) test ## ## data: Modele.GWR ## F = 3.4476, df1 = 142.92, df2 = 500.00, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## SS OLS residuals SS GWR improvement ## 22346.02 15581.60 Un autres test (LMZ.F3GWR.test) permet de répondre à la question suivante : est-ce que les coefficients de régression du modèle GWR varient spatialement de façon significative? Les résultats ci-dessous démontrent que c’est le cas pour toutes les variables indépendantes et la constante (p &lt; 0,001). LMZ.F3GWR.test(Modele.GWR) ## ## Leung et al. (2000) F(3) test ## ## F statistic Numerator d.f. Denominator d.f. Pr(&gt;) ## (Intercept) 2.2771 134.3880 430.81 1.629e-10 *** ## Pct0_14 2.7767 141.7244 430.81 5.636e-16 *** ## Pct_65 2.0918 169.0472 430.81 8.399e-10 *** ## Pct_Img 1.9486 106.4400 430.81 1.550e-06 *** ## Pct_brevet 2.4445 121.2830 430.81 1.629e-11 *** ## NivVieMed 3.6926 138.8118 430.81 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.3.2.4 Cartographie des résultats du modèle GWR Dans un premier temps, nous ajoutons les valeurs locales des R2, des coefficients de régression et des valeurs de t dans la couche sf. Notez que les résultats locaux de la GWR sont stockés dans l’objet Modele.GWR$SDF. ## Récupération du R2 local LyonIris$GWR.R2 &lt;- Modele.GWR$SDF$localR2 ## Récupération des coefficients de régression et calcul des valeurs de t locales names(Modele.GWR$SDF) ## [1] &quot;sum.w&quot; &quot;(Intercept)&quot; &quot;Pct0_14&quot; ## [4] &quot;Pct_65&quot; &quot;Pct_Img&quot; &quot;Pct_brevet&quot; ## [7] &quot;NivVieMed&quot; &quot;(Intercept)_se&quot; &quot;Pct0_14_se&quot; ## [10] &quot;Pct_65_se&quot; &quot;Pct_Img_se&quot; &quot;Pct_brevet_se&quot; ## [13] &quot;NivVieMed_se&quot; &quot;gwr.e&quot; &quot;pred&quot; ## [16] &quot;pred.se&quot; &quot;localR2&quot; &quot;(Intercept)_se_EDF&quot; ## [19] &quot;Pct0_14_se_EDF&quot; &quot;Pct_65_se_EDF&quot; &quot;Pct_Img_se_EDF&quot; ## [22] &quot;Pct_brevet_se_EDF&quot; &quot;NivVieMed_se_EDF&quot; &quot;pred.se&quot; VarsIndep &lt;- c(&quot;Pct0_14&quot;, &quot;Pct_65&quot;, &quot;Pct_Img&quot;, &quot;Pct_brevet&quot;, &quot;NivVieMed&quot;) for(e in VarsIndep){ # Nom des nouvelles variables var.coef &lt;- paste0(&quot;GWR.&quot;, &quot;B_&quot;, e) var.t &lt;- paste0(&quot;GWR.&quot;, &quot;T_&quot;, e) # Récupération des coefficients pour les variables indépendantes LyonIris[[var.coef]] &lt;- Modele.GWR$SDF[[e]] # Calcul des valeurs de t pour les variables indépendantes LyonIris[[var.t]] &lt;- Modele.GWR$SDF[[e]] / Modele.GWR$SDF[[paste0(e, &quot;_se&quot;)]] } Cartographie des R2 locaux Le code ci-dessous permet ensuite de cartographier les R2 locaux de la GWR (figure 6.9). library(tmap) tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.R2&quot;, palette=&quot;YlOrBr&quot;, n=5, style=&quot;quantile&quot;, title = &quot;R2 locaux&quot;)+ tm_layout(frame=FALSE)+ tm_scale_bar(breaks=c(0,5)) Figure 6.9: Cartographie des R2 locaux de la GWR Cartographie des coefficients de régression Le code ci-dessous permet ensuite de cartographier les coefficients locaux de la GWR (figure 6.10). Carte1 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.B_Pct0_14&quot;, palette=&quot;YlOrBr&quot;, n=4, style=&quot;pretty&quot;, title = &quot;Moins de 15 ans (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5)) Carte2 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.B_Pct_65&quot;, palette=&quot;YlOrBr&quot;, n=4, style=&quot;pretty&quot;, title = &quot;65 ans et plus (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5)) Carte3 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.B_Pct_Img&quot;, palette=&quot;YlOrBr&quot;, n=4, style=&quot;pretty&quot;, title = &quot;Immigrants (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5)) Carte4 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.B_Pct_brevet&quot;, palette=&quot;YlOrBr&quot;, n=4, style=&quot;pretty&quot;, title = &quot;Faible scolarité (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5)) Carte5 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.B_NivVieMed&quot;, palette=&quot;YlOrBr&quot;, n=4, style=&quot;pretty&quot;, title = &quot;Niveau de vie (€1000)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE)+tm_scale_bar(breaks=c(0,5)) tmap_arrange(Carte1, Carte2, Carte3, Carte4, Carte5, ncol = 2, nrow=3) Figure 6.10: Cartographie des coefficients de régression de la GWR Cartographie des valeurs de t Pour cartographier les valeurs de t, nous utilisons les seuils de ± 1,96, 2,58 et 3,29, indiquant des seuils de signification à 5%, 1% et 0,1% (figure 6.11). classes.intervalles = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf) Carte1 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.T_Pct0_14&quot;, palette=&quot;-RdBu&quot;, breaks = classes.intervalles, title = &quot;Moins de 15 ans (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE) Carte2 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.T_Pct_65&quot;, palette=&quot;-RdBu&quot;, breaks = classes.intervalles, title = &quot;65 ans et plus (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE) Carte3 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.T_Pct_Img&quot;, palette=&quot;-RdBu&quot;, breaks = classes.intervalles, title = &quot;Immigrants (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE) Carte4 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.B_Pct_brevet&quot;, palette=&quot;-RdBu&quot;, breaks = classes.intervalles, title = &quot;Faible scolarité (%)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE) Carte5 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;GWR.T_NivVieMed&quot;, palette=&quot;-RdBu&quot;, breaks = classes.intervalles, title = &quot;Niveau de vie (€1000)&quot;)+ tm_layout(frame=FALSE, legend.outside = TRUE)+ tm_scale_bar(breaks=c(0,5)) tmap_arrange(Carte1, Carte2, Carte3, Carte4, Carte5, ncol = 2, nrow=3) Figure 6.11: Cartographie des valeurs de t de la GWR Cartographie du nombre de variables significatives Nous pouvons aussi cartographier le nombre de variables localement significatives aux seuils de 5% et 1%. ## Identifier la variable plus significative avec les valeurs de t VarsT &lt;- paste0(&quot;GWR.T_&quot;, c(&quot;Pct0_14&quot;, &quot;Pct_65&quot;, &quot;Pct_Img&quot;, &quot;Pct_brevet&quot;, &quot;NivVieMed&quot;)) Lyon.df &lt;- st_drop_geometry(LyonIris) Lyon.df &lt;- abs(Lyon.df[,VarsT]) PlusSign &lt;- VarsT[apply(Lyon.df[VarsT],1,which.max)] PlusSign &lt;- substr(PlusSign, 7, nchar(PlusSign)) MaxAbsTvalue &lt;- apply(Lyon.df[VarsT], 1, max) PlusSign &lt;- ifelse(MaxAbsTvalue&lt;1.96, &quot;Aucune&quot;, PlusSign) ## Nombre de variables significatives au seuil de 5%, soit abs(t)= 1,96) LyonIris$NbSignif_1.96 &lt;- as.factor(rowSums(Lyon.df &gt; 1.96)) LyonIris$NbSignif_2.58 &lt;- as.factor(rowSums(Lyon.df &gt; 2.58)) LyonIris$PlusSign &lt;- as.factor(PlusSign) ## Cartographie Carte1 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;NbSignif_1.96&quot;, palette=&quot;Reds&quot;, title = &quot;Sign. au seuil de 5%&quot;)+ tm_layout(frame=FALSE)+ tm_scale_bar(breaks=c(0,5)) Carte2 &lt;- tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;NbSignif_2.58&quot;, palette=&quot;Reds&quot;, title = &quot;Sign. au seuil de 1%&quot;)+ tm_layout(frame=FALSE) tmap_arrange(Carte1, Carte2, ncol=2, nrow=1) Figure 6.12: Nombre de variables significatives aux seuils de 5% et 1% Cartographie de la variable la plus significative avec la valeur de t Finalement, le code ci-dessous permet de réperer la variable la plus significative au seuil de 5%, c’est-à-dire avec la plus forte valeur absolue pour la valeur de t. tm_shape(LyonIris)+ tm_borders(col=&quot;gray25&quot;, lwd=.5)+ tm_fill(col=&quot;PlusSign&quot;, palette=&quot;Set1&quot;, title = &quot;Variable la plus significative&quot;)+ tm_layout(frame=FALSE)+ tm_scale_bar(breaks=c(0,5)) Figure 6.13: Variable indépendante la plus significative au seuil de 5% References "],["sect064.html", "6.4 Quiz de révision du chapitre", " 6.4 Quiz de révision du chapitre Qu’est-ce que la dépendance spatiale d’un modèle de régression? Relisez au besoin la section 6.1.1. Lorsque les variables indépendantes sont fortement corrélées entre elles. Lorsque les résidus du modèles sont fortement autocorrélés spatialement. Dans un modèle SLX, l’autocorrélation est introduites au niveau de : Relisez au besoin la section 6.1.2.1. Variable dépendante Variables indépendantes Terme d’erreur Variable dépendante et variables indépendantes Variable dépendante et terme d’erreur Dans un modèle SAR, l’autocorrélation est introduites au niveau de : Relisez au besoin la section 6.1.2.2. Variable dépendante Variables indépendantes Terme d’erreur Dans un modèle SEM, l’autocorrélation est introduites au niveau de : Relisez au besoin la section 6.1.2.3. Variable dépendante Variables indépendantes Terme d’erreur Dans un modèle mixte SDM, l’autocorrélation est introduites au niveau de : Relisez au besoin la section 6.1.2.4. Variable dépendante Variables indépendantes Terme d’erreur Dans un modèle SDEM, l’autocorrélation est introduites au niveau de : Relisez au besoin la section 6.1.2.5. Variable dépendante Variables indépendantes Terme d’erreur Comment est intégré l’espace dans un modèle généralisé additif? Relisez au besoin la section 6.2.1. Avec une variable spatialement décalée Avec une spline bivariée sur les coordonnées x et y Un modèle de régression géographiquement pondérée permet d’explorer Relisez le deuxième encadré à la section 6.3.1. L’instabilité spatiale du modèle La dépendance du modèle Un modèle de régression géographiquement produit autant de régressions que d’entités spatiales dans le jeu de données à l’étude. Relisez le deuxième encadré à la section 6.3.1. Vrai Faux Vérifier votre résultat "],["sect065.html", "6.5 Exercices de révision", " 6.5 Exercices de révision Exercice 1. Réalisation de modèles de régression autorégressifs spatiaux library(sf) library(spatialreg) # Matrice de contiguïté selon le partage d&#39;un segment (Rook) load(&quot;data/chap06/DonneesLyon.Rdata&quot;) Rook &lt;- poly2nb(LyonIris, queen=FALSE) Rook &lt;- poly2nb(LyonIris, queen=FALSE) W.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = &quot;W&quot;) # Modèles formule &lt;- &quot;PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed&quot; Modele.SLX &lt;- à compléter Modele.SAR &lt;- à compléter Modele.SEM &lt;- à compléter Modele.DurbinSpatial &lt;- à compléter Modele.DurbinErreur &lt;- à compléter Correction à la section 10.6.1. Exercice 2. Réalisation d’un modèle GAM library(sf) library(mgcv) load(&quot;data/chap06/DonneesLyon.Rdata&quot;) # Ajout des coordonnées x et y xy &lt;- à compléter LyonIris$X &lt;- à compléter LyonIris$Y &lt;- à compléter # Construction du modèle avec formule &lt;- &quot;PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed&quot; Modele.GAM2 &lt;- gam(NO2 ~ à compléter à compléter, data = LyonIris) summary(Modele.GAM2) Correction à la section 10.6.2. Exercice 2. Réalisation d’un modèle GWR library(sf) library(spgwr) load(&quot;data/chap06/DonneesLyon.Rdata&quot;) # Ajout des coordonnées x et y xy &lt;- à compléter LyonIris$X &lt;- à compléter LyonIris$Y &lt;- à compléter # Optimisation du nombre de voisins avec le CV formule &lt;- &quot;PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed&quot; bwaCV.voisins &lt;- gwr.sel(à compléter) # Optimisation du nombre de voisins avec l&#39;AIC bwaCV.voisins &lt;- gwr.sel(à compléter) # Réalisation de la GWR Modele.GWR &lt;- gwr(à compléter) # Affichage des résultats Modele.GWR Correction à la section 10.6.3. "],["chap07.html", "Chapitre 7 Méthodes de classification non supervisée spatiale", " Chapitre 7 Méthodes de classification non supervisée spatiale Les méthodes de classification vise à regrouper des observations d’un jeu de données en plusieurs classes en fonction de leurs caractéristiques évaluées à partir de plusieurs variables. Appliquée à une couche spatiale (polygones, points, lignes), il s’agit alors de classifier les unités spatiales sur la base de plusieurs de leurs attributs mesurés à partir de variables. Dans le cadre de ce chapitre, nous aborderons deux principales familles de méthode de classification non supervisée : celles avec une contrainte spatiale (algorithmes AZP, SKATER, REDCAP) et celles avec une dimension spatiale (ClustGeo et classification floue c-means spatiale). Sommairement, les deux dernières méthodes sont des extensions spatiales de la classification ascendante hiérarchique (CAH) et de l’algorithme flou c-means. Par conséquent, la lecture de ce chapitre nécessite de bien maîtriser le fonctionnement de la CAH et du k-moyennes (k-means). Si ce n’est pas le cas, nous vous invitons vivement à lire le chapitre suivant (Apparicio et Gelb 2022). Bref retour sur les méthodes de classification Il existe de nombreuses méthodes de classification. Nous distinguons habituellement plusieurs familles de méthodes de classification, celles non supervisées versus supervisées et celles strictes versus floues : Les méthodes de classification non supervisées « […] relèvent de la statistique exploratoire multidimensionnelle et permettent de classifier automatiquement les observations sans avoir de connaissance préalable sur la nature des classes présentes dans l’ensemble de données (Lebart, Morineau et Piron 1995). Les méthodes les plus connues dans ce domaine sont l’algorithme de Classification Ascendante Hiérarchique (CAH) et la méthode des k-moyennes (k-means) » (Gelb et Apparicio 2021, 1). À cela s’ajoutent d’autres méthodes comme le k-médianes (Jain et Dubes 1988) ou encore le k-médoïdes (Kaufman 1990), la classification mixte combinant k-moyennes et CAH (Lebart, Morineau et Piron 1995). Aussi, pour regrouper les observations, ces méthodes (CAH, k-moyennes, k-médianes, k-médoïdes, classification mixte) sont basées sur la distance (proximité) entre elles tandis que d’autres méthodes sont basées sur la densité des observations (algorithmes DBSCAN, HDBSCAN, STDBSCAN, OPTICS abordés à la section 4.1). Les méthodes de classification supervisées « […] permettent d’affecter des observations à partir d’un échantillon déjà classifié, souvent appelé classes d’entraînement. Parmi les méthodes supervisées les plus connues, on retrouve les forêts d’arbres décisionnels, les réseaux de neurones artificiels et l’analyse factorielle discriminante » (Gelb et Apparicio 2021, 1). Qu’elles soient ou non supervisées, « on distingue généralement les méthodes strictes (ou de partition) des méthodes floues. […] Dans une classification stricte, chaque observation appartient à une seule classe : mathématiquement, l’appartenance à une classe donnée est binaire (0 ou 1), tandis que dans une classification floue, chaque observation a une probabilité d’appartenance variant de 0 à 1 pour chacune des classes » (Gelb et Apparicio 2021, 1‑2). Pourquoi recourir à des méthodes de classification non supervisée spatiales? Dans un article récent, Gelb et Apparicio (2021) identifie deux principales limites à l’application d’une méthode de classification non supervisée a-spatiale (comme le CAH et le k-means) sur des données spatiales : La non prise en compte de la dimension spatiale qui constitue une perte d’information : « […]  une partie de l’information propre aux données, à savoir leur localisation, n’est pas prise en compte dans le processus de classification. Or, la dimension géographique est souvent très structurante; par conséquent, l’occulter revient à perdre une quantité non négligeable d’information. Il convient toutefois de nuancer quelque peu ce propos. Généralement, la cartographie des méthodes de classification non supervisées a-spatiales (CAH et k-means) révèlent des effets de voisinage, d’autant plus que les variables introduites dans la classification sont fortement autocorrélées positivement » (Gelb et Apparicio 2021, 6). Limiter l’effet de mitage : « […] dans un contexte d’autocorrélation spatiale positive, des observations proches spatialement devraient plus vraisemblablement appartenir au même groupe. Avec les méthodes de classification a-spatiales, il est fréquent d’observer des phénomènes de mitage, c’est-à-dire des observations appartenant à un groupe b et isolées au milieu d’un ensemble d’observations appartenant au groupe a. Ce phénomène peut s’expliquer par la présence ici et là d’autocorrélation spatiale locale négative, c’est-à-dire des observations dont les caractéristiques sémantiques diffèrent de leurs voisines. Souvent, la dissimilarité sémantique entre ces observations est négligeable et ne justifie pas cette rupture spatiale » (Gelb et Apparicio 2021, 6). Dans ce chapitre, nous utilisons les packages suivants : Pour importer et manipuler des fichiers géographiques : sf pour importer et manipuler des données vectorielles. spdep pour construire des matrices de pondération spatiale. Pour construire des cartes et des graphiques : tmap est certainement le meilleur package pour la cartographie. ggplot2 pour construire des graphiques. ggpubr pour combiner des graphiques. Pour les méthodes de classification avec une contrainte spatiale : rgeoda pour les algorithmes AZP, Skater et REDCAP. spdep pour l’algorithme Skater. Pour les méthodes de classification avec une dimension spatiale : ClustGeo pour la méthode ClustGeo. geocmeans pour la classification k-moyenne floue et spatiale. References "],["sect071.html", "7.1 Méthodes de classification non supervisée avec constrainte spatiale", " 7.1 Méthodes de classification non supervisée avec constrainte spatiale Nous avons vu que l’objectif d’une méthode non supervisée appliquée à des données spatiales est de regrouper en n classes les unités spatiales d’une couche géographique. Prenons l’exemple de quatre variables environnementales cartographiés à la figure 7.1 au niveau des IRIS de la ville de Lyon, dont trois considérées comme des nuisances (bruit, dioxyde d’azote et particules fines) et une considérée comme avantageuse (végétation). Figure 7.1: Cartographie des variables environnementales Il est possible de regrouper les unités spatiales avec ou sans contrainte spatiale : Sans contrainte spatiale, nous cherchons à regrouper les IRIS (unités spatiales) avec des valeurs similaires pour les quatre variables retenues (Lden, NO2, PM2,5 et pourcentage de canopée). Cette approche est illustrée à la figure 7.2.a avec l’algorithme k-moyennes (k-means en anglais) avec cinq classes. Avec contrainte spatiale, nous cherchons à regrouper les IRIS (unités spatiales) avec des valeurs similaires pour les quatre variables retenues, tout en nous assurant que les regroupements forment des régions avec une absence de mitage. Cette approche est illustrée à la figure 7.2.b avec l’algorithme SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) avec cinq classes. Autrement dit, l’objectif des méthodes de classification non supervisée avec constrainte spatiale est d’agréger n unités spatiales en m régions non discontinues (avec n &lt; m) (Openshaw et Rao 1995, 428). Figure 7.2: Classification non supervisée avec et sans contrainte spatiale Intérêt et limites des méthodes de classification avec une contrainte spatiale Selon Gelb et Apparicio (2021, 7), le résultat d’une méthode de classification avec une contrainte spatiale est « la création de régions très cohérentes spatialement, c’est-à-dire avec une absence de mitage. Autrement dit, avec ces méthodes, il n’est pas possible d’identifier de groupes qui seraient spatialement discontinus, c’est-à-dire composés de plusieurs ensembles régionaux séparés. L’impossibilité d’obtenir du mitage au sein des différentes régions peut masquer la présence de valeurs fortement dissemblables localement, malgré la prise en compte de l’espace. Or, ces observations systématiquement différentes de leurs voisines doivent faire l’objet d’une attention particulière dans les exercices de classification intégrant l’espace, ce que ne permettent pas ces méthodes d’agrégation spatiale. […] Les limites de ces méthodes, particulièrement celles relatives au mitage, ont conduit plus récemment à la mise au point de nouvelles méthodes incluant l’espace dans le processus de classification, sans imposer une contrainte de contiguïté. Plus spécifiquement, ces nouvelles méthodes sont des modifications des algorithmes classiques, tels que la CAH ou le FCM, pour intégrer la dimension spatiale en parallèle à la dimension sémantique des données. En d’autres termes, l’espace n’est plus intégré comme une contrainte dans les algorithmes de classification, mais plutôt comme une donnée supplémentaire ». Les principaux algorithmes de classification supervisée avec contrainte spatiale (Spatially Constrained Clustering Methods en anglais) sont : La méthode de zonage automatique (Automatic Zoning Procedure en anglais) (AZP) proposée Openshaw (1977), puis améliorée par Openshaw et Rao (1995). L’algorithme SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) (Assunção et al. 2006). L’Algorithme REDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning) (Guo 2008). L’Algorithme du max-p-regions problem (Duque, Anselin et Rey 2012). Pour mettre en œuvre ces différents algorithmes, nous utilisons le package rgeoda (Li et Anselin 2023). Notez que l’algorithme Skater est aussi implémenté dans le package spded (fonction skater). 7.1.1 Algorithmes AZP L’algorithme AZP (Automatic Zoning Problem) est une approche itérative et heuristique visant à regrouper des polygones adjacents en m régions, tout en maximisant la variance intrarégionale (variance interclasse) et minimisant la variance intrarégionale (variance intraclasse) calculées sur les p variables. Autrement dit, il vise à créer des régions non discontinues les plus homogènes possibles et les plus dissemblables entre elles sur la base des p variables. Pour utiliser l’AZP, il faut spécifier le nombre de régions (m) désiré. Notez qu’il existe trois algorithmes pour l’AZP : AZP (Automatic Zoning Procedure), soit la première version par Stan Openshaw (1977). AZP-SA (A simulated annealing AZP method) (Openshaw et Rao 1995). AZP-TABU (A tabu search heuritic version of AZP) (Openshaw et Rao 1995). Pour une description détaillée de ces trois algorithmes, on pourra consulter Openshaw et Rao (1995) ou encore le lien suivant. Appliquons ces algorithmes aux 506 IRIS de la ville de Lyon avec les quatre variables environnementales préalablement centrées réduites (bruit, dioxyde d’azote et particules fines et pourcentage de végétation) et une matrice de contiguïté selon le partage d’un nœud. Le package rgeoda comprend trois fonctions pour l’AZP : azp_greedy (AZP), azp_sa (AZP-SA), azp_tabu (AZP-TABU). Pour l’exercice, nous fixons le nombre de régions à 5. Notez que par défaut, les variables seront centrées réduites (moyenne = 0 et écart-type = 1) avec le paramètre scale_method=\"standardize\". library(rgeoda) library(sf) library(tmap) ## Variables VarsEnv &lt;- c(&quot;Lden&quot;, &quot;NO2&quot;, &quot;PM25&quot;, &quot;VegHautPrt&quot;) ## Dataframe sans la géométrie et les quatre variables load(&quot;data/chap06/DonneesLyon.Rdata&quot;) Data &lt;- st_drop_geometry(LyonIris[VarsEnv]) ## Création d&#39;une matrice de contiguïté avec rgeoda queen_w &lt;- queen_weights(LyonIris) ## Calcul des trois algorithmes azp &lt;- rgeoda::azp_greedy(p=5, # Nombre de régions w=queen_w, # Matrice contiguïté df=Data, # Tableau de données scale_method = &quot;standardize&quot;) # cote z azp.sa &lt;- rgeoda::azp_sa(p=5, w=queen_w, df=Data, cooling_rate = 0.85) azp.tab &lt;- rgeoda::azp_tabu(p=5, w=queen_w, df=Data, tabu_length = 10, conv_tabu = 10) ## Création des trois champs dans la couche de Lyon LyonIris$Azp &lt;- as.character(azp$Clusters) LyonIris$Azp_sa &lt;- as.character(azp.sa$Clusters) LyonIris$Azp_tab &lt;- as.character(azp.tab$Clusters) Cartographions les résultats des trois algorithmes AZP (figure 7.3). ## Cartographie des résultats Carte.AZP1 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;Azp&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;a. AZP&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.AZP2 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;Azp_sa&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;b. AZP Simulated Annealing&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.AZP3 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;Azp_tab&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;c. AZP Tabu Search&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) tmap_arrange(Carte.AZP1, Carte.AZP2, Carte.AZP3, ncol = 2, nrow = 2) Figure 7.3: Regroupements des 505 IRIS en cinq régions selon les trois algorithmes AZP Par la suite, nous comparons les résultats obtenus des trois algorithmes en reportant : Les variances totale, intrarégionale et interégionale, et surtout le ratio entre les variances intergroupe et totale. Ce ratio varie de 0 à 1 et exprime la proportion de la variance des variables qui est expliquée par les différentes régions obtenues; plus il est élevé, meilleur est le résultat. Par conséquent, il peut être utilisé pour identifier la solution optimale entre les trois algorithmes. Le nombre d’observations par région. Les valeurs moyennes des variables centrées réduites par région. ## Calcul du ratio entre les variances intergroupes et totale cat(&quot;Ratio des variances interégionale et totale&quot;, &quot;\\nAZP : &quot;, round(azp$`The ratio of between to total sum of squares`, 3), &quot;\\nAZP-SA : &quot;, round(azp.sa$`The ratio of between to total sum of squares`, 3), &quot;\\nAZP-TABU : &quot;, round(azp.tab$`The ratio of between to total sum of squares`, 3) ) ## Ratio des variances interégionale et totale ## AZP : 0.436 ## AZP-SA : 0.518 ## AZP-TABU : 0.428 À la lecture des valeurs du ratio entre la variance interégionale et la variance totale ci-dessus, celle la plus élevée est obtenue pour l’AZP-SA (0,518), suivies de celles de l’AZP (0,436) et de l’AZP-TABU (0,428). Nous retenons alors l’AZP-SA. ## Nombre d&#39;observations par région table(LyonIris$Azp) ## ## 1 2 3 4 5 ## 186 176 104 27 13 table(LyonIris$Azp_sa) ## ## 1 2 3 4 5 ## 221 107 87 51 40 table(LyonIris$Azp_tab) ## ## 1 2 3 4 5 ## 191 186 91 27 11 ## Valeurs moyennes des variables centrées réduites par région Data$Azp &lt;- azp$Clusters Data$Azp_sa &lt;- azp.sa$Clusters Data$Azp_tab &lt;- azp.tab$Clusters aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp, data = Data, FUN = mean) ## Azp Lden NO2 PM25 VegHautPrt ## 1 1 58.05464 35.21961 18.85212 15.45333 ## 2 2 55.42159 26.45169 16.32884 13.98563 ## 3 3 53.23122 23.63965 14.95293 30.42529 ## 4 4 52.26742 22.99701 14.58065 20.39444 ## 5 5 48.71652 18.24301 13.07043 33.07154 aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp_sa, data = Data, FUN = mean) ## Azp_sa Lden NO2 PM25 VegHautPrt ## 1 1 57.41952 35.02151 18.80132 14.41317 ## 2 2 55.11019 22.52804 15.55562 14.12178 ## 3 3 51.08297 20.48809 14.16951 28.66000 ## 4 4 54.23625 24.47389 14.96836 22.30059 ## 5 5 58.40430 33.55155 17.08480 28.83775 aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ Azp_tab, data = Data, FUN = mean) ## Azp_tab Lden NO2 PM25 VegHautPrt ## 1 1 55.24285 26.04447 16.19300 14.67712 ## 2 2 58.05464 35.21961 18.85212 15.45333 ## 3 3 53.19298 23.97221 14.99671 31.90967 ## 4 4 52.26742 22.99701 14.58065 20.39444 ## 5 5 48.32861 17.74683 12.84853 31.68364 Les résultats finaux de l’AZP-SA sont présentés au tableau 7.1 et la figure 7.4. L’analyse conjointe du tableau et de la carte permet ainsi d’interpréter chacune des classes. En guise d’exemple, nous pouvons conclure que : La région 1 comprend 221 IRIS localisés au centre de la ville de Lyon et caractérisés par des niveaux moyens élevés de bruit (54,7), de dioxyde d’azote (35) et particules fines (18,8) élevés et un faible pourcentage de canopée (14,4%), tandis que la région 3 comprend 107 IRIS localisés à l’extrême ouest de la ville et caractérisés par les plus faibles niveaux de polluants (51,1, 20,5 et 14,2) et la forte moyenne pour la canopée (28,7%). Tableau 7.1: Valeurs moyennes des variables pour les quatre régions obtenus par l’AZP-SA Région Lden NO2 PM25 Végétation Nombre d’IRIS 1 57,4 35,0 18,8 14,4 221 2 55,1 22,5 15,6 14,1 107 3 51,1 20,5 14,2 28,7 87 4 54,2 24,5 15,0 22,3 51 5 58,4 33,6 17,1 28,8 40 Figure 7.4: Regroupement des IRIS en quatre régions selon l’AZP-TABU Nous avons vu que pour les algorithmes AZP, il faut spécifier le nombre de régions. Nous l’avons fixé arbitrairement à 5. Comme pour n’importe quelle méthode de classification non supervisée, déterminer le nombre de classes optimal est une étape cruciale qui peut s’appuyer sur différentes techniques, dont la méthode du coude basée sur l’inertie expliquée (ici le ratio entre la variance interégionale et totale), l’indicateur de silhouette et la méthode GAP. Pour une description détaillée de ces méthodes, consultez la section suivante (Apparicio et Gelb 2022). Le code ci-dessous permet de réaliser un graphique avec les valeurs du ratio (inertie expliquée) par l’algorithme AZP-TABU calculée de 2 à 10 régions. À la lecture de la figure 7.5, nous observons deux ruptures (coudes) très nettes à 5 et 8. nregions &lt;- 2:10 Data &lt;- data.frame(scale(st_drop_geometry(LyonIris)[VarsEnv])) queen_w &lt;- queen_weights(LyonIris) inertie &lt;- sapply(nregions, function(k){ # calcul de l&#39;AZP-TABU avec k resultat &lt;- azp_tabu(p=k, w=queen_w, df=Data, tabu_length = 10, conv_tabu = 10) # récupération du ratio ratios &lt;- resultat$`The ratio of between to total sum of squares` return(ratios) }) df &lt;- data.frame(k = nregions, ratio = inertie) ggplot(df) + geom_line(aes(x = k, y = ratio)) + geom_point(aes(x = k, y = ratio), color = &quot;red&quot;) + labs(x = &quot;Nombre de régions&quot;, y = &quot;inertie expliquée (%)&quot;) Figure 7.5: Méthode du coude reposant sur l’inertie expliquée pour l’AZP-TABU 7.1.2 Algorithme SKATER L’algorithme SKATER (Spatial ’K’luster Analysis by Tree Edge Removal) (Assunção et al. 2006) permet aussi de créer des régions sans discontinuité, en recourant à une technique de la théorie des graphes, soit celle de l’arbre couvrant de poids minimal (minimum spanning tree). Succinctement, la classification est obtenue avec les étapes suivantes : Création d’un graphe de connectivité pour les polygones de la couche géographique. Dans ce graphe, les nœuds sont les centroïdes des polygones et les arêtes représentent les liaisons entre deux entités spatiales voisines. Pour chaque arête, nous calculons la dissimilarité (appelée coût) deux polygones voisins en fonction des p variables. Pour chaque polygone, nous retenons l’arête avec le coût minimal. Autrement dit, pour chaque polygone, nous retenons son polygone voisin qui lui est le plus semblable selon les p variables. Nous obtenons ainsi l’arbre couvrant de poids minimal. Cet arbre est ensuite élagué en supprimant les arêtes avec les plus forts coûts et en créant ainsi des sous-graphes en m régions sans discontinuité. Pour une description plus détaillée de l’algorithme, consultez l’article d’Assunção et al. (2006). Le code ci-dessous permet de centrer et réduire les quatre variables (fonction scale) et de construire la matrice de voisinage entre les polygones de la couche LyonIris (fonction poly2nb de spdep). library(rgdal) library(spdep) library(tmap) ## Variables VarsEnv &lt;- c(&quot;Lden&quot;, &quot;NO2&quot;, &quot;PM25&quot;, &quot;VegHautPrt&quot;) ## Dataframe sans la géométrie et les quatre variables load(&quot;data/chap06/DonneesLyon.Rdata&quot;) Data &lt;- st_drop_geometry(LyonIris[VarsEnv]) ## Données centrées et réduites LyonIrisZscore &lt;- data.frame(scale(Data)) ## Matrice voisinage Lyon.nb &lt;- poly2nb(LyonIris) Calculons les coûts pour les arêtes reliant les nœuds avec la fonction nbcosts. Nous constatons que le polygone 1 est voisin des polygones 27, 26, 44 et 74 avec des coûts de 1,354, 1,74, 1,15 et 16,3. Par conséquent, parmi ses quatre voisins, le polygone 1 est le plus semblable au polygone 44 (coût minimal). ## Calcul des coûts pour les arêtes lcosts &lt;- nbcosts(Lyon.nb, LyonIrisZscore) head(Lyon.nb, n=1) ## [[1]] ## [1] 27 36 44 73 head(lcosts, n=1) ## [[1]] ## [1] 1.343210 1.735894 1.153787 1.632583 À partir de ces coûts, nous pouvons trouver l’arbre couvrant de poids minimal (minimum spanning tree), objet dénommée ici Lyon.mst qui comprend trois colonnes : la première pour l’identifiant du polygone la seconde pour l’identifiant du polygone voisin la troisième pour la valeur du coût minimal (similarité selon les variables retenues). ## Matrice de pondération spatiale avec les coûts Lyon.w &lt;- nb2listw(Lyon.nb, lcosts, style=&quot;B&quot;) ### Trouver l&#39;arbre couvrant de poids minimal Lyon.mst &lt;- mstree(Lyon.w) head(Lyon.mst, n=3) ## [,1] [,2] [,3] ## [1,] 499 478 0.5025378 ## [2,] 478 46 0.4618205 ## [3,] 46 7 1.3665949 Le code ci-dessous de visualiser le graphe de connectivité et l’arbre couvrant de poids minimal (figure 7.6). ## Visualisation du graphe de connectivité coords &lt;- st_coordinates(st_centroid(LyonIris)) plot(st_geometry(LyonIris), border=&quot;gray&quot;, lwd=.5, col=&quot;wheat&quot;) plot(Lyon.nb, coords, add=TRUE, col=&quot;red&quot;, lwd=1) ## Visualisation de l&#39;arbre couvrant de poids minimal plot(st_geometry(LyonIris), border=&quot;gray&quot;, lwd=.5, col=&quot;wheat&quot;) plot(Lyon.mst, coords, col=&quot;blue&quot;, cex.lab=0.7, add=TRUE) Figure 7.6: Graphe de connectivité et arbre couvrant de poids minimal Le code ci-dessous permet de réaliser une classification Skater avec cinq régions avec le package spdep. ## Skater avec le package spdep set.seed(123456789) skater5.spdep &lt;- spdep::skater(edges = Lyon.mst[,1:2], # deux première colonnes de l&#39;arbre data = data.frame(LyonIrisZscore), method = &quot;euclidean&quot;, ncuts = 4) # k-1 régions table(skater5.spdep$groups) ## ## 1 2 3 4 5 ## 48 215 89 57 97 Toutefois, il est plus simple d’utiliser la fonction skater de rgeoda qui ne nécessite pas de créer au prélable l’arbre couvrant de poids minimal. ## Skater avec le package rgeoda library(rgeoda) Data &lt;- st_drop_geometry(LyonIris[VarsEnv]) queen_w &lt;- queen_weights(LyonIris) skater5.rgeoda &lt;- rgeoda::skater(k = 5, # k-1 régions w = queen_w, # matrice de contiguité scale_method = &quot;standardize&quot;, df = Data) # dataframe table(skater5.rgeoda$Clusters) ## ## 1 2 3 4 5 ## 248 125 51 48 34 La figure 7.7 démontre que les résultats obtenus sont légèrement différents avec les deux packages. LyonIris$skater5spdep &lt;- as.character(skater5.spdep$groups) LyonIris$skater5rgeoda &lt;- as.character(skater5.rgeoda$Clusters) Carte.SkaterA &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;skater5spdep&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;a. Skater spdep&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.SkaterB &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;skater5rgeoda&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;b. Skater rgeoda&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) tmap_arrange(Carte.SkaterA, Carte.SkaterB) Figure 7.7: Résultats de l’algorithme Skater avec cinq classes obtenus avec les packages spdep et rgeoda Algorithme Skater avec un seuil minimal pour les classes Dans une classification non supervisée avec une contrainte spatiale, il est possible de fixer un seuil minimal pour chaque région à partir d’une variable. L’exemple le plus classique est l’obtention de p régions qui doivent au moins avoir un nombre d’habitants fixé par la personne utilisatrice. Pour ce faire, nous utilisons deux paramètres de la fonction spdep::skater, soit crit = 50000 pour fixer le seuil et vec.crit = df$Population pour indiquer le vecteur sur lequel est calculé le critère. `clus10_min &lt;- spdep::skater(edges = ct_mst[,1:2], data = dfs, # dataframe avec les variables centrées réduites crit = 50000, # seuil fixé vec.crit = df$Population, # variable population du dataframe ncuts = 4) Fonction skater : différences entre les packages rgeoda et spdep La fonction skater de rgeoda a deux principaux avantages : Comme décrit précédemment, l’avantage de la fonction skater de rgeoda est qu’elle ne nécessite pas de calculer au prélable l’arbre couvrant de poids minimal. scale_method = c(“raw”, “standardize”, “demean”, “mad”, “range_standardize”, “range_adjust”) permet de transformer directement les variables. La méthode par défaut est la cote z (moyenne =  et écart-type = 1). Avec la fonction skater de spdep, vous devez préalablement transformer vos variables et construire l’arbre couvrant de poids minimal. Par contre, elle intègre de nombreux types de distance pour évaluer la dissimilarité entre les unités spatiales avec le paramètre method = c(\"euclidean\", \"maximum\", \"manhattan\", \"canberra\", \"binary\", \"minkowski\", \"mahalanobis\") tandis que le paramètre distance_method = c(\"euclidean\", \"manhattan\") de rgeoda ne comprend que deux types de distance. 7.1.3 Algorithmes REDCAP Les différentes versions de l’algorithme REDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning) proposé par Diansheng Guo (2008) sont aussi basées sur la construction d’un arbre (spanning tree) dont l’élagage est obtenue de cinq différentes façons : Premier ordre et saut minimal (First-order and Single-linkage) qui fournit un résultats identique à l’algorithme skater. Ordre complet et saut maximal (Full-order and Complete-linkage). Ordre complet et saut moyen (Full-order and Average-linkage). Ordre complet et saut minimal (Full-order and Single-linkage). Ordre complet et critère de Ward (Full-order and Ward-linkage). Le code ci-dessous permet de calculer les cinq versions de l’algorithmes REDCAP avec cinq régions et de comparer leurs résultats à partir du ratio (entre les variances interrégionale et totale) et du nombre d’observations par région library(rgeoda) library(sf) ## Préparation des données Data &lt;- st_drop_geometry(LyonIris[VarsEnv]) queen_w &lt;- queen_weights(LyonIris) ## Algorithmes REDCAP redcap5.A &lt;- redcap(k = 5, w = queen_w, scale_method = &quot;standardize&quot;, df = Data, method = &quot;firstorder-singlelinkage&quot;) redcap5.B &lt;- redcap(k = 5, w = queen_w, scale_method = &quot;standardize&quot;, df = Data, method = &quot;fullorder-completelinkage&quot;) redcap5.C &lt;- redcap(k = 5, w = queen_w, scale_method = &quot;standardize&quot;, df = Data, method = &quot;fullorder-averagelinkage&quot;) redcap5.D &lt;- redcap(k = 5, w = queen_w, scale_method = &quot;standardize&quot;, df = Data, method = &quot;fullorder-singlelinkage&quot;) redcap5.E &lt;- redcap(k = 5, w = queen_w, scale_method = &quot;standardize&quot;, df = Data, method = &quot;fullorder-wardlinkage&quot;) ## Comparaison des résultats Ratios &lt;- data.frame(Methode = c(&quot;firstorder-singlelinkage&quot;, &quot;fullorder-completelinkage&quot;, &quot;fullorder-averagelinkage&quot;, &quot;fullorder-singlelinkage&quot;, &quot;fullorder-wardlinkage&quot;), ratio = c(redcap5.A$`The ratio of between to total sum of squares`, redcap5.B$`The ratio of between to total sum of squares`, redcap5.C$`The ratio of between to total sum of squares`, redcap5.D$`The ratio of between to total sum of squares`, redcap5.E$`The ratio of between to total sum of squares`) ) Nobs &lt;- data.frame(rbind(table(redcap5.A$Clusters), table(redcap5.B$Clusters), table(redcap5.C$Clusters), table(redcap5.D$Clusters), table(redcap5.E$Clusters)) ) names(Nobs) &lt;- c(&quot;C1&quot;, &quot;C2&quot;, &quot;C3&quot;, &quot;C4&quot;, &quot;C5&quot;) Ratios &lt;- cbind(Ratios, Nobs) Ratios ## Methode ratio C1 C2 C3 C4 C5 ## 1 firstorder-singlelinkage 0.4081577 248 125 51 48 34 ## 2 fullorder-completelinkage 0.4728026 173 156 115 47 15 ## 3 fullorder-averagelinkage 0.4993184 148 141 106 63 48 ## 4 fullorder-singlelinkage 0.3976055 227 102 73 53 51 ## 5 fullorder-wardlinkage 0.5185455 166 134 120 51 35 À la lecture des valeurs du ratio ci-dessous, la meilleure classification serait celle obtenue avec un ordre complet et le critère de Ward. Cartographions les résultats des quatre version de l’algorithmes RECAP avec un ordre complet (figure 7.8). ## Ajout des champs dans la couche LyonIris$RC5.FOsinglelinkage &lt;- as.character(redcap5.B$Clusters) LyonIris$RC5.FOaveragelinkage &lt;- as.character(redcap5.C$Clusters) LyonIris$RC5.FOsinglelinkage &lt;- as.character(redcap5.D$Clusters) LyonIris$RC5.FOwardlinkage &lt;- as.character(redcap5.E$Clusters) ## Cartographie des résultats Carte.RCb &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;RC5.FOsinglelinkage&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;a. Saut maximal&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.RCc &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;RC5.FOaveragelinkage&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;b. Saut moyen&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.RCd &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;RC5.FOsinglelinkage&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;c. Saut minimal&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.RCe &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;RC5.FOwardlinkage&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;d. Critère de Ward&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) tmap_arrange(Carte.RCb, Carte.RCc, Carte.RCd, Carte.RCe, ncol = 2, nrow = 2) Figure 7.8: Regroupements des 505 IRIS en cinq régions selon les quatre versions de l’algorithme REDCAP avec un lien complet 7.1.4 Algorithme du max-p-regions problem Cet algorithme proposé par Duque et al. (2012) n’est pas abordé dans le cours. Il peut être calculé avec trois fonctions du package rgeoda, soit maxp_greedy, maxp_sa et maxp_tabu. References "],["sect072.html", "7.2 Méthodes de classification non supervisée avec une dimension spatiale", " 7.2 Méthodes de classification non supervisée avec une dimension spatiale Nous avons vu que les méthodes de classification avec une contrainte spatiale visent à obtenir des régions non discontinues, c’est-à-dire sans mitage spatial. L’objectif des méthodes de classification non supervisée avec une dimension spatiale est quelque peu différent : classifier les observations en tenant compte de l’espace (proximité, voisinage entre les unités spatiales) afin de limiter les effets de mitage, sans obtenir l’interdire. Dans le cadre de cette section, nous décrivons deux de ces méthodes qui intègrent la dimension spatiale de manière différente : La méthode ClustGeo, qui est une extension de la classification ascendante hiérarchique, est une méthode de classification non supervisée, spatiale et stricte. Cette méthode repose sur deux matrices de dissimilarité : une matrice sémantique calculée sur les valeurs de plusieurs variables caractérisant les entités géographiques et une matrice de distances (euclidienne le plus souvent) entre les entités géographiques. Nous cherchons ainsi à regrouper les observations qui se ressemblent à la fois selon leurs attributs et selon leur proximité spatiale. La méthode k-moyenne spatiale et floue (Spatial fuzzy c-means), qui est une extension de la méthode k-moyennes, est une méthode de classification non supervisée, spatiale et floue. Cette méthode repose sur deux matrices de dissimilarité : une matrice sémantique calculée sur les valeurs de plusieurs variables caractérisant les entités géographiques et une matrice sémantique spatialement décalée. Nous cherchons ainsi à regrouper les observations qui se ressemblent à la fois selon leurs caractéristiques et celles de leurs unités spatiales adjacentes ou proches. Autrement dit, dans la méthode ClustGeo, l’espace est introduit sous la forme d’une matrice de distances entre les entités spatiales (agencement spatial) tandis que dans la méthode du Spatial fuzzy c-means, il est introduit sous la forme d’une matrice de données sémantiques spatialement décalée (information sémantique dans l’environnement immédiat). 7.2.1 Classification ascendante hiérarchique spatiale (ClustGeo) 7.2.1.1 Description de la méthode ClustGeo La méthode ClustGeo, proposée Marie Chavent et ses collègues (2018), est une extension de la classification ascendante hiérarchique (CAH) qui intègre la dimension spatiale des entités géographiques. Cette méthode repose sur une idée brillante, soit classer (regrouper) les observations (unités spatiales) en utilisant deux matrices de dissimilarité : Une matrice sémantique calculée sur p variables caractérisant les unités spatiales (\\(D_0\\)). Une matrice spatiale calculée à partir des distances euclidiennes entre les unités spatiales (\\(D_1\\)). Notez qu’un paramètre \\(\\alpha\\), variant de 0 à 1, permet de définir le poids de la matrice spatiale comparativement à celle sémantique : Avec \\(\\alpha=0\\), le poids accordé à la matrice spatiale est nul. Nous obtenons ainsi une CAH classique. Avec \\(\\alpha=1\\), le poids accordé à la matrice spatiale est maximal; la classification est alors purement spatiale. Par conséquent, « […] l’enjeu principal est de fixer la valeur du paramètre , considérant qu’une augmentation de  revient à améliorer l’inertie expliquée de la matrice spatiale, au détriment d’une perte de l’inertie expliquée sur le plan sémantique » (Gelb et Apparicio 2021, 16). 7.2.1.2 Calcul de la CAH classique Retour sur la classification ascendante hiérarchique (CAH) Pour une description détaillée de la CAH, consultez la section suivante (Apparicio et Gelb 2022). Le code ci-dessous permet de déterminer l’arbre de classification selon le critère de Ward à partir de la matrice sémantique. ## Variables pour la CAH VarsEnv &lt;- c(&quot;Lden&quot;, &quot;NO2&quot;, &quot;PM25&quot;, &quot;VegHautPrt&quot;) ## Dataframe sans la géométrie et les quatre variables load(&quot;data/chap06/DonneesLyon.Rdata&quot;) Data &lt;- st_drop_geometry(LyonIris[VarsEnv]) ## Centrage (moyenne = 0) et réduction des données (variance = 1) DataZscore &lt;- data.frame(scale(Data)) ## Matrice sémantique : dissimilarité des observations selon les variables Matrice.Semantique &lt;- dist(DataZscore, method = &quot;euclidean&quot;) # Calcul du dendrogramme avec le critère WARD Arbre &lt;- hclust(Matrice.Semantique, method = &quot;ward.D&quot;) plot(Arbre, hang = -1, label = FALSE, main = &quot;Dendrogramme \\n(arbre de classification selon le critère de Ward)&quot;, sub = &quot;&quot;, ylab = &quot;Hauteur&quot;, xlab = &quot;&quot; ) À la lecture de la figure 7.9, nous ne détectons de seuils marqués dans l’inertie expliquée en fonction du nombre de groupe. Par conséquent, nous fixons arbitrairement le nombre de groupes à 5. library(ggplot2) # Fonction pour l&#39;inertie expliquée par les classes prop_inert_cutree &lt;- function(K,tree,n){ P &lt;- cutree(tree,k=K) W &lt;- sum(tree$height[1:(n-K)]) Tot &lt;- sum(tree$height) return(1-W/Tot) } # Inertie expliquée par des CAH de 2 à 10 classes df.inertie &lt;- data.frame(NGroupes = 2:10, Inertie = sapply(2:10, prop_inert_cutree, tree=Arbre, n=nrow(DataZscore))) ggplot(df.inertie)+ geom_line(aes(x=NGroupes,y=Inertie))+ geom_point(aes(x=NGroupes,y=Inertie), color = &quot;red&quot;) + labs(y = &quot;Inertie expliquée&quot;, x = &quot;Nombre de groupes&quot;) Figure 7.9: Méthode du coude reposant sur l’inertie expliquée pour CAH Nous pouvons visualiser l’arbre avec une coupure à cinq classes. plot(Arbre, labels = FALSE, main = &quot;Partition en 2, 5 ou 9 classes&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;, sub = &quot;&quot;, axes = FALSE, hang = -1) rect.hclust(Arbre, 5, border = &quot;red&quot;) ## Coupure de l&#39;arbre à cinq classes LyonIris$CAH5 &lt;- as.character(cutree(Arbre, k=5)) ## Nombre d&#39;observations par classe table(LyonIris$CAH5) ## ## 1 2 3 4 5 ## 62 120 88 81 155 ## Valeurs moyennes des classes aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ CAH5, data = st_drop_geometry(LyonIris), FUN = mean) ## CAH5 Lden NO2 PM25 VegHautPrt ## 1 1 49.52246 19.11011 13.84968 34.64694 ## 2 2 54.57170 22.29326 15.17248 12.34458 ## 3 3 62.53950 37.18071 18.25763 18.25330 ## 4 4 55.21535 25.85652 15.83278 26.67333 ## 5 5 55.08405 34.17193 18.90686 13.44716 Les résultats de la CAH sont cartographiés à la figure 7.10 tandis que le nombre d’observations et les valeurs moyennes des classes sont reportés tableau 7.2. library(tmap) ## Cartographie des résultats Carte.CAH5 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;CAH5&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;CAH (critère de Ward)&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.CAH5 Figure 7.10: CAH avec le critère de Ward avec cinq classes Tableau 7.2: Valeurs moyennes des variables pour les cinq classes obtenues avec la CAH Classe Lden NO2 PM25 Végétation Nombre d’IRIS 1 49,5 19,1 13,8 34,6 62 2 54,6 22,3 15,2 12,3 120 3 62,5 37,2 18,3 18,3 88 4 55,2 25,9 15,8 26,7 81 5 55,1 34,2 18,9 13,4 155 7.2.1.3 Calcul de la méthode ClustGeo Calcul des deux matrices (sémantique et spatiale) Dans un premier temps, nous créons les matrices sémantique (\\(D_0\\)) et spatiale (\\(D_1\\)). library(sf) library(ClustGeo) ## Variables VarsEnv &lt;- c(&quot;Lden&quot;, &quot;NO2&quot;, &quot;PM25&quot;, &quot;VegHautPrt&quot;) ## Dataframe sans la géométrie et les quatre variables load(&quot;data/chap06/DonneesLyon.Rdata&quot;) Data &lt;- st_drop_geometry(LyonIris[VarsEnv]) ## Centrage (moyenne = 0) et réduction des données (variance = 1) DataZscore &lt;- data.frame(scale(Data)) ## Matrice sémantique : dissimilarité des observations selon les variables Matrice.Semantique &lt;- dist(DataZscore, method = &quot;euclidean&quot;) ## Matrice spatiale entre les unités spatiales xy &lt;- st_coordinates(st_centroid(LyonIris)) Matrice.Spatiale &lt;- dist(xy, method = &quot;euclidean&quot;) Optimisation de la valeur de \\(\\alpha\\) Pour la méthode ClustGeo (avec k = 5), nous évaluons l’impact du paramètre α pour des valeurs de 0 à 1, avec un saut de 0,05. Pour ce faire, nous utilisons la fonction choicealpha du package ClustGeo. À la lecture de la figure 7.11, nous constatons que : plus la valeur de \\(\\alpha\\) augmente, plus l’inertie expliquée par la matrice sémantique diminue (trait noir) et inversement, plus l’inertie expliquée par la matrice spatiale est forte. avec \\(\\alpha=\\text{0,30}\\), l’inertie expliquée par la matrice spatiale est de 50 % pour une perte d’inertie expliquée par la matrice sémantique de seulement 7 %. Avec \\(\\alpha=\\text{0,35}\\), nous constatons observe une chute importante de l’inertie sémantique expliquée.Par conséquent, nous retenons la valeur de 0,30 pour la méthode ClustGeo. alphas &lt;- seq(0, 1, 0.05) result &lt;- choicealpha(D0 = Matrice.Semantique, # matrice sémantique D1 = Matrice.Spatiale, # matrice spatiale range.alpha = alphas, # valeurs de alpha K = 5, # nombre de classes wt = NULL, scale = TRUE, graph = FALSE) # Graphique avec Alpha df.alpha &lt;- data.frame(result$Q) df.alpha$alpha &lt;- alphas ggplot(df.alpha)+ geom_line(aes(x=alphas,y= Q0), color = &quot;black&quot;)+ geom_point(aes(x=alphas,y= Q0), color = &quot;black&quot;, size=3) + geom_line(aes(x=alphas,y= Q1), color = &quot;red&quot;)+ geom_point(aes(x=alphas,y= Q1), color = &quot;red&quot;, size=3) + labs(y = &quot;Pseudo-Inertie&quot;, x = &quot;Paramètre alpha&quot;, subtitle = &quot;Matrice sémantique (noir) et matrice spatiale (rouge)&quot;) Figure 7.11: Impact des deux matrices dans la classification Réalisation de la méthode ClustGeo ## Dendrogramme avec ClustGeo Arbre.ClustGeo &lt;- hclustgeo(D0 = Matrice.Semantique, D1 = Matrice.Spatiale, alpha = 0.30) ## Coupure de l&#39;arbre à cinq classes LyonIris$ClustGeo5 &lt;- as.character(cutree(Arbre.ClustGeo, k=5)) ## Nombre d&#39;observations par classe table(LyonIris$ClustGeo5) ## ## 1 2 3 4 5 ## 101 73 102 145 85 ## Valeurs moyennes des classes aggregate(cbind(Lden,NO2,PM25,VegHautPrt) ~ ClustGeo5, data = st_drop_geometry(LyonIris), FUN = mean) ## ClustGeo5 Lden NO2 PM25 VegHautPrt ## 1 1 51.99446 22.63948 14.69971 32.70396 ## 2 2 55.46789 20.84670 15.00392 14.07849 ## 3 3 61.41468 36.20785 18.23707 18.77059 ## 4 4 54.98927 34.16558 18.93142 12.84883 ## 5 5 54.05419 24.32165 15.45733 16.14224 ## Cartographie des résultats Carte.ClusteGeo &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;ClustGeo5&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;ClustGeo avec alpha = 0,30&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte.ClusteGeo Tableau 7.3: Valeurs moyennes des variables pour cinq classes obtenues par la ClustGeo (alpha = 0,30) Classe Lden NO2 PM25 Végétation Nombre d’IRIS 1 52,0 22,6 14,7 32,7 101 2 55,5 20,8 15,0 14,1 73 3 61,4 36,2 18,2 18,8 102 4 55,0 34,2 18,9 12,8 145 5 54,1 24,3 15,5 16,1 85 La comparaison des typologies obtenues avec la CAH et la ClusteGeo démontre clairement que l’introduction d’une matrice spatiale dans la classification ClustGeo génére des classes qui sont moins discontinues spatialement (figure 7.12) tmap_arrange(Carte.CAH5, Carte.ClusteGeo, nrow = 1, ncol = 2) Figure 7.12: Impact des deux matrices dans la classification 7.2.2 Spatial fuzzy c-means La méthode SFCM (Spatial fuzzy c-means), proposé Weiling Cai et ses collègues (2007), est une extension de FCM, soit une version floue de l’algorithme de k-moyennes. Le principe de base est le suivant : « Comparativement au FCM classique, le SFCM introduit dans son calcul, en plus du jeu de données original (\\(D_0\\)), une version spatialement décalée (\\(D_s\\)) de ce dernier (Cai, Chen et Zhang 2007). En analyse d’image, cela revient à calculer \\(D_s\\) en appliquant un filtre moyen ou médian à \\(D_0\\) (la médiane étant moins sensible aux valeurs extrêmes locales). Ce processus peut facilement s’appliquer à des entités géographiques vectorielles, en créant une matrice de pondération spatiale \\(W_{kl}\\) (l étant les voisins de k et la diagonale de cette matrice valant 0) (Getis 2009) et en utilisant les poids de cette matrice dans le calcul d’une moyenne ou d’une médiane pondérée locale » (Gelb et Apparicio 2021, 8). Comme pour la méthode ClustGeo, il est possible de fixer une pondération à la dimension spatiale (\\(D_s\\)) avec un paramètre \\(\\alpha\\), qui varie de 0 à \\(\\infty\\). Une valeur de 0 signale qu’aucun poids n’est accordé à la dimension spatiale, ce qui revient à calculer un FCM classique. Si \\(\\alpha=\\) =2, alors la version spatialement décalée aura deux fois plus de poids dans la classification que le jeu de données original. Retour sur une variable spatialement décalée La notion de variable spatialement décalée a été abordée au chapitre 2 (figure 2.20). Retour la classification non supervisée k-moyennes (k-means) Pour une description détaillée de la classification k-moyennes, consultez la section suivante (Apparicio et Gelb 2022). 7.2.2.1 Calcul du c-moyennes floue classique (fuzzy c-means) À des fins de comparaison avec la CAH, nous proposons de calculer une classification c-moyennes floue classique (fuzzy c-means) avec cinq classes (k = 5). Puis, pour déterminer la valeur optimale de m (soit le degré de logique floue), nous calculons l’inertie expliquée et l’indice de silhouette pour des valeurs de m variant de 1,1 à 3 (avec un incrément de 0,1). Les résultats de ces deux indicateurs présentés à la figure 7.13 signale que : plus le paramètre m augmente, plus l’inertie expliquée diminue (figure 7.13.a). la valeur de l’indice de silhouette est maximal avec m = 1,8 (figure 7.13.b). library(geocmeans) library(ggpubr) ## Variables pour la CAH VarsEnv &lt;- c(&quot;Lden&quot;, &quot;NO2&quot;, &quot;PM25&quot;, &quot;VegHautPrt&quot;) ## Dataframe sans la géométrie et les quatre variables load(&quot;data/chap06/DonneesLyon.Rdata&quot;) Data &lt;- st_drop_geometry(LyonIris[VarsEnv]) ## Centrage (moyenne = 0) et réduction des données (variance = 1) DataZscore &lt;- data.frame(scale(Data)) ## Dataframe pour les différents paramètres avec k = 5 et degré de flou FCM_selection &lt;- select_parameters(algo = &quot;FCM&quot;, data = DataZscore, k = 5, # nous pourrions ici tester avec k=2:10 m = seq(1.1,3,0.1), classidx = TRUE, spconsist = FALSE, tol = 0.001, seed = 456, verbose = FALSE) ## [1] &quot;number of combinaisons to estimate : 20&quot; # Graphique avec l&#39;inertie expliquée G1 &lt;- ggplot(FCM_selection) + geom_line(aes(x = m, y = Explained.inertia)) + geom_point(aes(x = m, y = Explained.inertia), color = &quot;red&quot;)+ labs(title =&quot;a. Variation des données expliquées&quot;, y = &quot;Inertie expliquée&quot;, x = &quot;Paramètre m&quot;) # Graphique avec l&#39;indice de Silhouette G2 &lt;- ggplot(FCM_selection) + geom_line(aes(x = m, y = Silhouette.index)) + geom_point(aes(x = m, y = Silhouette.index), color = &quot;red&quot;)+ labs(title =&quot;b. Consistance des groupes&quot;, y = &quot;Critère de sihlouette floue&quot;, x = &quot;Paramètre m&quot;) # Combinaison des deux graphiques dans la figure ggarrange(G1, G2) Figure 7.13: Évaluation de la qualité de la classification FCM avec cinq classes selon le degré de flou (m) Réalisons la classification fuzzy c-means et cartographions les probabilités d’appartenance à chacune des classes (figure 7.14), puis l’appartenance à une classe (figure 7.15). ## Classification du c-means FCM &lt;- CMeans(DataZscore, k = 5, m = 1.8, tol = 0.0001, verbose = FALSE, seed = 456) ## calcul des indicateurs de qualité calcqualityIndexes(DataZscore, FCM$Belongings, 1.5) ## $Silhouette.index ## [1] 0.5502472 ## ## $Partition.entropy ## [1] 0.9663788 ## ## $Partition.coeff ## [1] 0.5143427 ## ## $XieBeni.index ## [1] 1.269992 ## ## $FukuyamaSugeno.index ## [1] 158.1419 ## ## $Explained.inertia ## [1] 0.3757482 ## Cartographie des probabilités d&#39;appartenance à chaque classe Cartes.FCM &lt;- mapClusters(LyonIris,FCM$Belongings) names(Cartes.FCM) ## [1] &quot;ProbaMaps&quot; &quot;ClusterPlot&quot; tmap_arrange(Cartes.FCM$ProbaMaps, ncol = 2) Figure 7.14: Cartographie des probabilités d’appartenance aux cinq classes avec la classification c-means Cartes.FCM$ClusterPlot Figure 7.15: Cartographie des des classes issues la classification c-means 7.2.2.2 Calcul du c-moyennes floue et spatial (spatial fuzzy c-means) Dans l’exemple ci-dessous, nous calculons une classification SFCM (spatial fuzzy c-means) avec : k = 5, soit cinq classes. m = 1,8, soit le degré de logique floue. alpha = 0,7, soit le poids accordée à la matrice spatialement décalée comparativement à la matrice de données originale. library(spdep) library(ggplot2) # Création d&#39;une matrice de contiguïté standardisée Neighbours &lt;- poly2nb(LyonIris, queen = TRUE) WMat &lt;- nb2listw(Neighbours, style=&quot;W&quot;, zero.policy = TRUE) # Calcul du SFCM SFCM &lt;- SFCMeans(DataZscore, WMat, k = 5, m = 1.8, alpha = 0.7, tol = 0.0001, standardize = FALSE, verbose = FALSE, seed = 456) Résultats des indicateurs de qualité du SFCM calcqualityIndexes(DataZscore, SFCM$Belongings, 1.5) ## $Silhouette.index ## [1] 0.4829962 ## ## $Partition.entropy ## [1] 1.040742 ## ## $Partition.coeff ## [1] 0.472018 ## ## $XieBeni.index ## [1] 2.214814 ## ## $FukuyamaSugeno.index ## [1] 397.8657 ## ## $Explained.inertia ## [1] 0.3346273 Cartographie des probabilités d’appartenance à chaque classe Cartes.SFCM &lt;- mapClusters(LyonIris, SFCM$Belongings, undecided = 0.45) Cartes.SFCM$ProbaMaps[[1]] Figure 7.16: Cartographie des probabilités d’appartenance aux cinq classes avec la classification SFCM Cartes.SFCM$ProbaMaps[[2]] Figure 7.17: Cartographie des probabilités d’appartenance aux cinq classes avec la classification SFCM Cartes.SFCM$ProbaMaps[[3]] Figure 7.18: Cartographie des probabilités d’appartenance aux cinq classes avec la classification SFCM Cartes.SFCM$ProbaMaps[[4]] Figure 7.19: Cartographie des probabilités d’appartenance aux cinq classes avec la classification SFCM Cartes.SFCM$ProbaMaps[[5]] Figure 7.20: Cartographie des probabilités d’appartenance aux cinq classes avec la classification SFCM Cartographie des probabilités d’appartenance à chaque classe du SFCM Cartes.SFCM$ClusterPlot Figure 7.21: Cartographie des des classes issues la classification SFCM References "],["sect073.html", "7.3 Quiz de révision du chapitre", " 7.3 Quiz de révision du chapitre Quelles sont les méthodes de classification non supervisée avec une contrainte spatiale Relisez l’introduction du chapitre 7. Algorithmes AZP (Automatic Zoning Problem). Algorithmes SKATER. Classification floue c-means spatiale. Algorithmes REDCAP. Méthode ClustGeo. Quelles sont les méthodes de classification non supervisée avec une dimensions spatiale Relisez l’introduction du chapitre 7. Algorithmes AZP (Automatic Zoning Problem). Algorithmes SKATER. Classification floue c-means spatiale. Algorithmes REDCAP. Méthode ClustGeo. La classification ascendante hiérarchique et le k-moyennes sont des méthodes de classification non supervisée qui ne prennent pas en compte l’espace : Relisez au besoin la section 6.1.2.2. Vrai Faux La méthode de classification avec contrainte spatiale produit des régions non discontinues (sans mitage spatial) : Relisez au besoin la section 7.1. Vrai Faux L’algorithme SKATER est basé sur : Relisez au besoin la section 7.1.2. Une matrice de distances. Un arbre couvrant de poids minimal. Le I de Moran. La méthode ClustGeo est basée sur deux matrices : Relisez au besoin la section ??. Une matrice sémantique calculée sur p variables caractérisant les unités spatiales (D0). Une matrice spatiale calculée à partir des distances euclidiennes entre les unités spatiales (D1). Une matrice spatialement décalée de la matrice sémantique original (Ds). La méthode c-means spatiale floue (Spatial fuzzy c-means): Relisez au besoin la section ??. Une matrice sémantique calculée sur p variables caractérisant les unités spatiales (D0). Une matrice spatiale calculée à partir des distances euclidiennes entre les unités spatiales (D1). Une matrice spatialement décalée de la matrice sémantique original (Ds). Le paramètre alpha de la méthode ClustGeo permet de définir le poids accordé à la matrice spatiale. Son intervalle de variation est de : Relisez le deuxième encadré à la section 7.2.1.1. 0 à 1 0 à l’infini Le paramètre alpha de la classification c-means spatiale floue permet de définir le poids accordé à la matrice spatialement décalée. Son intervalle de variation est de : Relisez le deuxième encadré à la section 7.2.1.1. 0 à 1 0 à l’infini Vérifier votre résultat "],["sect074.html", "7.4 Exercices de révision", " 7.4 Exercices de révision Exercice 1. Réalisation de classification avec contrainte spatiale avec des variables socioéconomiques library(rgeoda) library(sf) library(tmap) ## Préparation des données load(&quot;data/chap06/DonneesLyon.Rdata&quot;) VarSocioEco &lt;- c(&quot;Pct0_14&quot;, &quot;Pct_65&quot;, &quot;Pct_Img&quot;, &quot;Pct_brevet&quot;, &quot;NivVieMed&quot;) Data2 &lt;- st_drop_geometry(LyonIris[VarSocioEco]) queen_w &lt;- queen_weights(LyonIris) ## Classification avec k = 4 azp5_sa &lt;- à compléter azp5_tab &lt;- à compléter skater5 &lt;- rgeoda::skater(à compléter) redcap5 &lt;- rà compléter ## Cartographie des résultats LyonIris$SE.azp4_sa &lt;- à compléter LyonIris$SE.azp4_tab &lt;- à compléter LyonIris$SE.skater4 &lt;- à compléter LyonIris$SE.recap4 &lt;- à compléter Carte1 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.azp4_sa&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;a. AZP-SA&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte2 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.azp4_tab&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;b. AZP-TABU&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte3 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.skater4&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;c. Skater&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte4 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.recap4&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;d. RECAP&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) tmap_arrange(à compléter) Correction à la section 10.7.1. "],["chap08.html", "Chapitre 8 Méthodes d’interpolation spatiale ", " Chapitre 8 Méthodes d’interpolation spatiale "],["sect081.html", "8.1 Méthodes déterministes", " 8.1 Méthodes déterministes 8.1.1 Interpolation polynomiale locale 8.1.2 Interpolation de l’inverse à la distance - IDW "],["sect082.html", "8.2 Méthodes géostatistiques", " 8.2 Méthodes géostatistiques 8.2.1 Krigeage simple 8.2.2 Krigeage universel 8.2.3 Co-krigeage 8.2.4 Krigeage résiduel "],["sect083.html", "8.3 Quiz de révision du chapitre", " 8.3 Quiz de révision du chapitre "],["sect084.html", "8.4 Exercices de révision", " 8.4 Exercices de révision "],["chap09.html", "Chapitre 9 Analyse spatiale par l’analyse d’images ", " Chapitre 9 Analyse spatiale par l’analyse d’images "],["sect091.html", "9.1 Fonctions mathématiques, booléennes, relationnelles et conditionnelles sur les images", " 9.1 Fonctions mathématiques, booléennes, relationnelles et conditionnelles sur les images 9.1.1 Fonctions mathématiques 9.1.2 Fonctions booléennes 9.1.3 Fonctions relationnelles 9.1.4 Fonctions conditionnelles "],["sect092.html", "9.2 Fonctions locales, focales, zonales et globales sur les images", " 9.2 Fonctions locales, focales, zonales et globales sur les images 9.2.1 Fonctions locales 9.2.2 Fonctions focales 9.2.3 Fonctions zonales 9.2.4 Fonctions globales "],["sect093.html", "9.3 Fonctions de surface", " 9.3 Fonctions de surface 9.3.1 Élevation 9.3.2 Pente 9.3.3 Ombrage 9.3.4 Visibilité 9.3.5 Sky-view factor Index Modèle numérique de terraim Modèle numérique d’élévation "],["sect094.html", "9.4 Données spatiotemporelles en mode image", " 9.4 Données spatiotemporelles en mode image 9.4.1 Fichier netCDF 9.4.2 Exemples applicatifs "],["sect095.html", "9.5 Quiz de révision du chapitre", " 9.5 Quiz de révision du chapitre "],["sect096.html", "9.6 Exercices de révision", " 9.6 Exercices de révision "],["chap10.html", "Chapitre 10 Correction des exercices ", " Chapitre 10 Correction des exercices "],["sect1001.html", "10.1 Exercices du chapitre 1", " 10.1 Exercices du chapitre 1 10.1.1 Exercice 1 library(sf) ## Importation des deux couches Arrond &lt;- st_read(&quot;data/shp/Arrondissements.shp&quot;, quiet = TRUE) Rues &lt;- st_read(&quot;data/shp/Segments_de_rue.shp&quot;, quiet = TRUE) ## Création d&#39;un objet sf pour l&#39;arrondissement des Nations : requête attributive table(Arrondissements$NOM) Arrond.DesNations &lt;- subset(Arrondissements, NOM == &quot;Arrondissement des Nations&quot;) ## Découper les rues avec le polygone de l&#39;arrondissement des nations Rues.DesNations &lt;- st_intersection(Rues, Arrond.DesNations) 10.1.2 Exercice 2 library(sf) library(tmap) ## Importation des deux couches AD.RMRSherb &lt;- st_read(dsn = &quot;data/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;SherbAD&quot;, quiet = T) HotelVille &lt;- data.frame(ID = 1, Nom = &quot;Hotel de Ville&quot;, lon = -71.89306, lat = 45.40417) HotelVille &lt;- st_as_sf(HotelVille, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) ## Changement de projection avant de s&#39;assurer que les deux couches aient la même HotelVille &lt;- st_transform(HotelVille, st_crs(AD.RMRSherb)) ## Ajout d&#39;un champ pour la distance en km à l&#39;hôtel de Ville pour les secteurs de recensement AD.RMRSherb$DistHVKM &lt;- as.numeric(st_distance(AD.RMRSherb,HotelVille)) / 1000 ## Cartographie en quatre classes selon les quantiles tmap_mode(&quot;plot&quot;) tm_shape(AD.RMRSherb)+ tm_fill(col= &quot;DistHVKM&quot;, palette = &quot;Reds&quot;, n=4, style = &quot;quantile&quot;, title =&quot;Distance à l&#39;hôtel de Ville (km)&quot;)+ tm_borders(col=&quot;black&quot;) 10.1.3 Exercice 3 library(sf) ## Importation de la couche des divisions de recensement du Québec DR.Qc &lt;- st_read(dsn = &quot;data/gpkg/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DivisionsRecens2021&quot;, quiet = T) ## Importation du fichier csv des division de recensement DR.Data &lt;- read.csv(&quot;data/tables/DRQC2021.csv&quot;) ## Jointure attributive avec le champ IDUGD DR.Qc &lt;- merge(DR.Qc, DR.Data, by=&quot;IDUGD&quot;) ## Il y a déja deux champs dans la table pour calculer la densité de population : ## SUPTERRE : superficie en km2 ## DRpop_2021 : population en 2021 DR.Qc$HabKm2 &lt;- DR.Qc$DRpop_2021 / DR.Qc$SUPTERRE head(DR.Qc, n=2) summary(DR.Qc$HabKm2) 10.1.4 Exercice 4 library(sf) ## Importation du réseau de rues Rues &lt;- st_read(&quot;data/shp/Segments_de_rue.shp&quot;, quiet=T) unique(Rues$TYPESEGMEN) ## Sélection des tronçons autoroutiers Autoroutes &lt;- subset(Rues, TYPESEGMEN == &quot;Autoroute&quot;) ## Création d&#39;une couche sf pour le point avec les coordonnées ## en degrés (WGS84, EPSG : 4326) : -71.91688, 45.37579 Point1_sf &lt;- data.frame(ID = 1, lon = -71.91688, lat = 45.37579) Point1_sf &lt;- st_as_sf(Point1_sf, coords = c(&quot;lon&quot;,&quot;lat&quot;), crs = 4326) ## Changement de projection avant de s&#39;assurer que les deux couches aient la même Point1_sf &lt;- st_transform(Point1_sf, st_crs(Autoroutes)) ## Trouver le tronçon autoroutier le plus proche PlusProche &lt;- st_nearest_feature(Point1_sf, Autoroutes) print(PlusProche) Point1_sf$AutoroutePlusProche &lt;- as.numeric(st_distance(Point1_sf, Autoroutes[PlusProche,])) cat(&quot;Distance à l&#39;autoroute la plus proche :&quot;, Point1_sf$AutoroutePlusProche, &quot;m.&quot;) ## Zone tampon ZoneTampon &lt;- st_buffer(Point1_sf, Point1_sf$AutoroutePlusProche) ## Cartographie tmap_mode(&quot;view&quot;) tm_shape(ZoneTampon)+ tm_borders(col= &quot;black&quot;)+ tm_shape(Autoroutes)+ tm_lines(col=&quot;red&quot;)+ tm_shape(Point1_sf)+ tm_dots(col= &quot;blue&quot;, shape=21, size = .2) "],["sect1002.html", "10.2 Exercices du chapitre 2", " 10.2 Exercices du chapitre 2 10.2.1 Exercice 1 Figure 10.1: Exercice sur la contiguïté et les ordres d’adjacence ::: 10.2.2 Exercice 2 library(sf) library(spdep) library(tmap) ## Importation de la couche des secteurs de recensement SRQc &lt;- st_read(dsn = &quot;data/chap02/exercice/RMRQuebecSR2021.shp&quot;, quiet=TRUE) ## Matrice selon le partage d&#39;un segment (Rook) Rook &lt;- poly2nb(SRQc, queen=FALSE) W.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = &quot;W&quot;) ## Coordonnées des centroïdes des entités spatiales coords &lt;- st_coordinates(st_centroid(SRQc)) ## Matrices de l&#39;inverse de la distance # Trouver le plus proche voisin k1 &lt;- knn2nb(knearneigh(coords)) plusprochevoisin.max &lt;- max(unlist(nbdists(k1,coords))) # Voisins les plus proches avec le seuil de distance maximal Voisins.DistMax &lt;- dnearneigh(coords, 0, plusprochevoisin.max) # Distances avec le seuil maximum distances &lt;- nbdists(Voisins.DistMax, coords) # Inverse de la distance au carré InvDistances2 &lt;- lapply(distances, function(x) (1/x^2)) ## Matrices de pondérations spatiales standardisées en ligne W_InvDistances2Reduite &lt;- nb2listw(Voisins.DistMax, glist = InvDistances2, style = &quot;W&quot;) ## Matrice des plus proches voisins avec k = 2 k2 &lt;- knn2nb(knearneigh(coords, k = 2)) W.k2 &lt;- nb2listw(k2, zero.policy=FALSE, style = &quot;W&quot;) 10.2.3 Exercice 3 library(sf) library(spdep) library(tmap) ## Cartographie de la variable tm_shape(SRQc)+ tm_polygons(col=&quot;D1pct&quot;, title = &quot;Premier décile de revenu (%)&quot;, style=&quot;quantile&quot;, n=5, palette=&quot;Greens&quot;)+ tm_layout(frame = F)+tm_scale_bar(c(0,5,10)) ## I de Moran avec la méthode Monte-Carlo avec 999 permutations # utilisez la fonction moran.mc # avec la matrice W.Rook moran.mc(SRQc$D1pct, listw=W.Rook, zero.policy=TRUE, nsim=999) # avec la matrice W_InvDistances2 moran.mc(SRQc$D1pct, listw=W_InvDistances2Reduite, zero.policy=TRUE, nsim=999) # avec la matrice W.k2 moran.mc(SRQc$D1pct, listw=W.k2, zero.policy=TRUE, nsim=999) Les valeurs du I de Moran sont les suivantes : 0,69 pour la matrice Rook, 0,52 pour la matrice inverse de la distance au carré réduite et 0,75 pour la matrice selon le critère des deux plus proches voisins. 10.2.4 Exercice 4 #################### ## Calcul du Z(Gi) #################### SRQc$D1pct_localGetis &lt;- localG(SRQc$D1pct, W.Rook, zero.policy=TRUE) # Définition des intervalles et des noms des classes classes.intervalles = c(-Inf, -3.29, -2.58, -1.96, 1.96, 2.58, 3.29, Inf) classes.noms = c(&quot;Point froid (p = 0,001)&quot;, &quot;Point froid (p = 0,01)&quot;, &quot;Point froid (p = 0,05)&quot;, &quot;Non significatif&quot;, &quot;Point chaud (p = 0,05)&quot;, &quot;Point chaud (p = 0,01)&quot;, &quot;Point chaud (p = 0,001)&quot;) ## Création d&#39;un champ avec les noms des classes SRQc$D1pct_localGetisP &lt;- cut(SRQc$D1pct_localGetis, breaks = classes.intervalles, labels = classes.noms) ## Cartographie tm_shape(SRQc)+ tm_polygons(col =&quot;D1pct_localGetisP&quot;, title=&quot;Z(Gi)&quot;, palette=&quot;-RdBu&quot;, lwd = 1)+ tm_layout(frame =F) #################### ## Typologie LISA #################### ## Cote Z (variable centrée réduite) zx &lt;- (SRQc$D1pct - mean(SRQc$D1pct))/sd(SRQc$D1pct) ## variable X centrée réduite spatialement décalée avec une matrice Rook wzx &lt;- lag.listw(W.Rook, zx) ## I de Moran local (notez que vous pouvez aussi utiliser la fonction localmoran_perm) localMoranI &lt;- localmoran(SRQc$D1pct, W.Rook) plocalMoranI &lt;- localMoranI[, 5] ## Choisir un seuil de signification signif = 0.05 ## Construction de la typologie Typologie &lt;- ifelse(zx &gt; 0 &amp; wzx &gt; 0, &quot;1. HH&quot;, NA) Typologie &lt;- ifelse(zx &lt; 0 &amp; wzx &lt; 0, &quot;2. LL&quot;, Typologie) Typologie &lt;- ifelse(zx &gt; 0 &amp; wzx &lt; 0, &quot;3. HL&quot;, Typologie) Typologie &lt;- ifelse(zx &lt; 0 &amp; wzx &gt; 0, &quot;4. LH&quot;, Typologie) Typologie &lt;- ifelse(plocalMoranI &gt; signif, &quot;Non sign&quot;, Typologie) # Non significatif ## Enregistrement de la typologie dans un champ SRQc$TypoIMoran.D1pct &lt;- Typologie ## Couleurs Couleurs &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;lightpink&quot;, &quot;skyblue2&quot;, &quot;lightgray&quot;) names(Couleurs) &lt;- c(&quot;1. HH&quot;,&quot;2. LL&quot;,&quot;3. HL&quot;,&quot;4. LH&quot;,&quot;Non sign&quot;) ## Cartographie tmap_mode(&quot;plot&quot;) tm_shape(SRQc) + tm_polygons(col = &quot;TypoIMoran.D1pct&quot;, palette = Couleurs, title =&quot;Autocorrélation spatiale locale&quot;)+ tm_layout(frame = FALSE) "],["sect1003.html", "10.3 Exercices du chapitre 3", " 10.3 Exercices du chapitre 3 10.3.1 Exercice 1 library(sf) library(tmap) ## Importation des données Arrondissements &lt;- st_read(dsn = &quot;data/chap03/Arrondissements.shp&quot;, quiet=TRUE) Incidents &lt;- st_read(dsn = &quot;data/chap03/IncidentsSecuritePublique.shp&quot;, quiet=TRUE) ## Changement de projection Arrondissements &lt;- st_transform(Arrondissements, crs = 3798) Incidents &lt;- st_transform(Incidents, crs = 3798) ## Couche pour les accidents Accidents &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% c(&quot;Accident avec blessés&quot;, &quot;Accident mortel&quot;)) ## Coordonnées et projection cartographique xy &lt;- st_coordinates(Accidents) ProjCarto &lt;- st_crs(Accidents) ## Centre moyen CentreMoyen &lt;- data.frame(X = mean(xy[,1]), Y = mean(xy[,2])) CentreMoyen &lt;- st_as_sf(CentreMoyen, coords = c(&quot;X&quot;, &quot;Y&quot;), crs = ProjCarto) # Distance standard combiné CentreMoyen$DS &lt;- c(sqrt(mean((xy[,1] - mean(xy[,1]))**2 + (xy[,2] - mean(xy[,2]))**2))) CercleDS &lt;- st_buffer(CentreMoyen, dist = CentreMoyen$DS) head(CercleDS) 10.3.2 Exercice 2 library(sf) library(tmap) ## Importation des données SR &lt;- st_read(dsn = &quot;data/chap03/Recen2021Sherbrooke.gpkg&quot;, layer = &quot;DR_SherbSRDonnees2021&quot;, quiet=TRUE) ## Couche pour les accidents pour l&#39;année 2021 Acc2021 &lt;- subset(Incidents, Incidents$DESCRIPTIO %in% c(&quot;Accident avec blessés&quot;, &quot;Accident mortel&quot;) &amp; ANNEE==2021) ## Nous nous assurons que les deux couches aient la même projection cartographique SR &lt;- st_transform(SR, st_crs(Acc2021)) ## Calcul du nombre d&#39;incidents par SR SR$Acc2021 &lt;- lengths(st_intersects(SR, Acc2021)) ## Calcul du nombre de méfaits pour 1000 habitants SR$DensiteMAcc2021Hab &lt;- SR$Acc2021 / (SR$SRpop_2021 / 1000) ## Cartographie tm_shape(SR)+ tm_polygons(col=&quot;Acc2021&quot;, style=&quot;pretty&quot;, title=&quot;Nombre pour 1000 habitants&quot;, border.col = &quot;black&quot;, lwd = 1)+ tm_bubbles(size = &quot;DensiteMAcc2021Hab&quot;, border.col = &quot;black&quot;, alpha = .5, col = &quot;aquamarine3&quot;, title.size = &quot;Nombre&quot;, scale = 1.5)+ tm_layout(frame = FALSE)+tm_scale_bar(text.size = .5, c(0, 5, 10)) 10.3.3 Exercice 3 library(sf) library(spatstat) library(tmap) library(terra) ## Importation des données Arrondissements &lt;- st_read(dsn = &quot;data/chap03/Arrondissements.shp&quot;, quiet=TRUE) Incidents &lt;- st_read(dsn = &quot;data/chap03/IncidentsSecuritePublique.shp&quot;, quiet=TRUE) ## Changement de projection Arrondissements &lt;- st_transform(Arrondissements, crs = 3798) Incidents &lt;- st_transform(Incidents, crs = 3798) ## Couche pour les méfaits pour l&#39;année 2021 M2021 &lt;- subset(Incidents, DESCRIPTIO == &quot;Méfait&quot; &amp; ANNEE==2021) ## Pour accélérer les calculs, nous retenons uniquement l&#39;arrondissement des Nations # Couche pour l&#39;arrondissement des Nations ArrDesNations &lt;- subset(Arrondissements, NOM == &quot;Arrondissement des Nations&quot;) # Sélection des accidents localisés dans l&#39;arrondissement Des Nations RequeteSpatiale &lt;- st_intersects(M2021, ArrDesNations, sparse = FALSE) M2021$Nations &lt;- RequeteSpatiale[, 1] M2021Nations &lt;- subset(M2021, M2021$Nations == TRUE) ## Conversion des données sf dans le format de spatstat # la fonction as.owin est utilisée pour définir la fenêtre de travail fenetre &lt;- as.owin(ArrDesNations) ## Conversion des points au format ppp pour les différentes années M2021.ppp &lt;- ppp(x = st_coordinates(M2021Nations)[,1], y = st_coordinates(M2021Nations)[,2], window = fenetre, check = T) ## Kernel quadratique avec un rayon de 500 mètres et une taille de pixel de 50 mètres kdeQ &lt;- density.ppp(M2021.ppp, sigma=500, eps=50, kernel=&quot;quartic&quot;) ## Conversion en raster RkdeQ &lt;- terra::rast(kdeQ)*1000000 ## Projection cartographique crs(RkdeQ) &lt;- &quot;epsg:3857&quot; ## Visualisation des résultats tmap_mode(&quot;plot&quot;) tm_shape(RkdeQ) + tm_raster(style = &quot;cont&quot;, palette=&quot;Reds&quot;, title = &quot;Gaussien&quot;)+ tm_shape(M2021Nations) + tm_dots(col = &quot;black&quot;, size = 0.01)+ tm_shape(ArrDesNations) + tm_borders(col = &quot;black&quot;, lwd = 3)+ tm_layout(frame = F) "],["sect1004.html", "10.4 Exercices du chapitre 4", " 10.4 Exercices du chapitre 4 10.4.1 Exercice 1 library(sf) library(tmap) library(dbscan) library(ggplot2) ## Importation des données Collissions &lt;- st_read(dsn = &quot;data/chap04/collisions.gpkg&quot;, layer = &quot;CollisionsRoutieres&quot;, quiet = T) ## Collisions impliquant au moins une personne à vélo en 2020 et 2021 Coll.Velo &lt;- subset(Collissions, Collissions$NB_VICTIMES_VELO &gt; 0 &amp; Collissions$AN %in% c(2020, 2021)) ## Coordonnées géographiques xy &lt;- st_coordinates(Coll.Velo) ## Graphique pour la distance au quatrième voisin le plus proche DistKplusproche &lt;- kNNdist(xy, k = 4) DistKplusproche &lt;- as.data.frame(sort(DistKplusproche, decreasing = FALSE)) names(DistKplusproche) &lt;- &quot;distance&quot; ggplot(data = DistKplusproche)+ geom_path(aes(x = 1:nrow(DistKplusproche), y = distance), size=1)+ labs(x = &quot;Points triés par ordre croissant selon la distance&quot;, y = &quot;Distance au quatrième point le plus proche&quot;)+ geom_hline(yintercept=250, color = &quot;#08306b&quot;, linetype=&quot;dashed&quot;, size=1)+ geom_hline(yintercept=500, color = &quot;#00441b&quot;, linetype=&quot;dashed&quot;, size=1)+ geom_hline(yintercept=1000, color = &quot;#67000d&quot;, linetype=&quot;dashed&quot;, size=1) ## DBSCAN avec les quatre distances set.seed(123456789) dbscan250 &lt;- dbscan(xy, eps = 250, minPts = 4) dbscan500 &lt;- dbscan(xy, eps = 500, minPts = 4) dbscan1000 &lt;- dbscan(xy, eps = 1000, minPts = 4) ## Affichage des résultats dbscan250 dbscan500 dbscan1000 ## Enregistrement dans la couche de points sf Coll.Velo Coll.Velo$dbscan250 &lt;- as.character(dbscan250$cluster) Coll.Velo$dbscan500 &lt;- as.character(dbscan500$cluster) Coll.Velo$dbscan1000 &lt;- as.character(dbscan1000$cluster) Coll.Velo$dbscan250 &lt;- ifelse(nchar(Coll.Velo$dbscan250) == 1, paste0(&quot;0&quot;, Coll.Velo$dbscan250), Coll.Velo$dbscan250) Coll.Velo$dbscan500 &lt;- ifelse(nchar(Coll.Velo$dbscan500) == 1, paste0(&quot;0&quot;, Coll.Velo$dbscan500), Coll.Velo$dbscan500) Coll.Velo$dbscan1000 &lt;- ifelse(nchar(Coll.Velo$dbscan1000) == 1, paste0(&quot;0&quot;, Coll.Velo$dbscan1000), Coll.Velo$dbscan1000) ## Extraction des agrégats Agregats.dbscan250 &lt;- subset(Coll.Velo, dbscan250 != &quot;00&quot;) Agregats.dbscan500 &lt;- subset(Coll.Velo, dbscan500 != &quot;00&quot;) Agregats.dbscan1000 &lt;- subset(Coll.Velo, dbscan1000 != &quot;00&quot;) ## Cartographie des résultats tmap_mode(&quot;view&quot;) tm_shape(Agregats.dbscan250)+tm_dots(col=&quot;dbscan250&quot;, size = .05) tm_shape(Agregats.dbscan500)+tm_dots(col=&quot;dbscan500&quot;, size = .05) tm_shape(Agregats.dbscan1000)+tm_dots(col=&quot;dbscan1000&quot;, size = .05) 10.4.2 Exercice 2 library(sf) library(tmap) library(dbscan) library(ggplot2) ## Importation des données Collissions &lt;- st_read(dsn = &quot;data/chap04/collisions.gpkg&quot;, layer = &quot;CollisionsRoutieres&quot;) ## Collisions impliquant au moins une personne à vélo en 2020 et 2021 Coll.Velo &lt;- subset(Collissions, Collissions$NB_VICTIMES_VELO &gt; 0 &amp; Collissions$AN %in% c(2020, 2021)) ## Coordonnées géographiques xy &lt;- st_coordinates(Coll.Velo) Coll.Velo$x &lt;- xy[,1] Coll.Velo$y &lt;- xy[,2] ## Conversion du champ DT_ACCDN au format Date Coll.Velo$DT_ACCDN &lt;- as.Date(Coll.Velo$DT_ACCDN) ## ST-DBSCAN avec eps1 = 500, esp2 = 30 et minpts = 4 Resultats.stdbscan &lt;- stdbscan(x = Coll.Velo$x, y = Coll.Velo$y, time = Coll.Velo$DT_ACCDN, eps1 = 500, eps2 = 30, minpts = 4) ## Enregistrement des résultats ST-DBSCAN dans la couche de points sf Coll.Velo$stdbscan &lt;- as.character(Resultats.stdbscan$cluster) Coll.Velo$stdbscan &lt;- ifelse(nchar(Coll.Velo$stdbscan) == 1, paste0(&quot;0&quot;, Coll.Velo$stdbscan), Coll.Velo$stdbscan) ## Nombre de points par agrégat avec la fonction table table(Coll.Velo$stdbscan) ## Sélection des points appartenant à un agrégat avec la fonction subset Agregats &lt;- subset(Coll.Velo, stdbscan != &quot;00&quot;) ## Conversion de la date au format POSIXct Agregats$dtPOSIXct &lt;- as.POSIXct(Agregats$DT_ACCDN, format = &quot;%Y/%m/%d&quot;) ## Tableau récapitulatif library(&quot;dplyr&quot;) Tableau.stdbscan &lt;- st_drop_geometry(Agregats) %&gt;% group_by(stdbscan) %&gt;% summarize(points = n(), date.min = min(DT_ACCDN), date.max = max(DT_ACCDN), intervalle.jours = as.numeric(max(DT_ACCDN)-min(DT_ACCDN))) ## Affichage du tableau print(Tableau.stdbscan, n = nrow(Tableau.stdbscan)) ## Construction du graphique ggplot(Agregats) + geom_point(aes(x = dtPOSIXct, y = stdbscan, color = stdbscan), show.legend = FALSE) + scale_x_datetime(date_labels = &quot;%Y/%m&quot;)+ labs(x= &quot;Temps&quot;, y= &quot;Identifiant de l&#39;agrégat&quot;, title = &quot;ST-DBSCAN avec Esp1 = 1000, Esp2 = 21 et MinPts = 4&quot;) ## Création d&#39;une couche pour les agrégats stdbcan.Agregats &lt;- subset(Coll.Velo, stdbscan != &quot;00&quot;) ## Cartographie tmap_mode(&quot;view&quot;) tm_shape(stdbcan.Agregats)+ tm_dots(shape = 21, col=&quot;stdbscan&quot;, size=.025, title = &quot;Agrégat&quot;) "],["sect1005.html", "10.5 Exercices du chapitre 5", " 10.5 Exercices du chapitre 5 "],["sect1006.html", "10.6 Exercices du chapitre 6", " 10.6 Exercices du chapitre 6 10.6.1 Exercice 1 library(sf) library(spatialreg) # Matrice de contiguïté selon le partage d&#39;un segment (Rook) load(&quot;data/chap06/DonneesLyon.Rdata&quot;) Rook &lt;- poly2nb(LyonIris, queen=FALSE) Rook &lt;- poly2nb(LyonIris, queen=FALSE) W.Rook &lt;- nb2listw(Rook, zero.policy=TRUE, style = &quot;W&quot;) # Modèles formule &lt;- &quot;PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed&quot; Modele.SLX &lt;- lmSLX(formule, listw=W.Rook, data = LyonIris) # dataframe Modele.SAR &lt;- lagsarlm(formule, listw=W.Rook, data = LyonIris, type = &#39;lag&#39;) Modele.SEM &lt;- errorsarlm(formule, listw=W.Rook, data = LyonIris) Modele.DurbinSpatial &lt;- lagsarlm(formule, listw = W.Rook, data = LyonIris, type = &quot;mixed&quot;) Modele.DurbinErreur &lt;- errorsarlm(formule, listw=W.Rook, data = LyonIris, etype = &#39;emixed&#39;) # Résultats des modèles summary(Modele.SLX) summary(Modele.SAR) summary(Modele.SEM) summary(Modele.DurbinSpatial) summary(Modele.DurbinErreur) 10.6.2 Exercice 2 library(sf) library(mgcv) load(&quot;data/chap06/DonneesLyon.Rdata&quot;) # Ajout des coordonnées x et y xy &lt;- st_coordinates(st_centroid(LyonIris)) LyonIris$X &lt;- xy[,1] LyonIris$Y &lt;- xy[,2] # Construction du modèle avec formule &lt;- &quot;PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed&quot; Modele.GAM2 &lt;- gam(PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed+ s(X, Y, k= 40), data = LyonIris) summary(Modele.GAM2) 10.6.3 Exercice 3 library(sf) library(spgwr) load(&quot;data/chap06/DonneesLyon.Rdata&quot;) # Ajout des coordonnées x et y xy &lt;- st_coordinates(st_centroid(LyonIris)) LyonIris$X &lt;- xy[,1] LyonIris$Y &lt;- xy[,2] # Optimisation du nombre de voisins avec le CV formule &lt;- &quot;PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed&quot; bwaCV.voisins &lt;- gwr.sel(formule, data = LyonIris, method = &quot;cv&quot;, gweight=gwr.bisquare, adapt=TRUE, verbose = FALSE, RMSE = TRUE, longlat = FALSE, coords=cbind(LyonIris$X,LyonIris$Y)) # Optimisation du nombre de voisins avec l&#39;AIC formule &lt;- &quot;PM25 ~ Pct0_14+Pct_65+Pct_Img+Pct_brevet+NivVieMed&quot; bwaCV.voisins &lt;- gwr.sel(formule, data = LyonIris, method = &quot;AIC&quot;, gweight=gwr.bisquare, adapt=TRUE, verbose = FALSE, RMSE = TRUE, longlat = FALSE, coords=cbind(LyonIris$X,LyonIris$Y)) # Réalisation de la GWR Modele.GWR &lt;- gwr(formule, data = LyonIris, adapt=bwaCV.voisins, gweight=gwr.bisquare, hatmatrix=TRUE, se.fit=TRUE, coords=cbind(LyonIris$X,LyonIris$Y), longlat=F) # Affichage des résultats Modele.GWR "],["sect1007.html", "10.7 Exercices du chapitre 7", " 10.7 Exercices du chapitre 7 10.7.1 Exercice 1 library(rgeoda) library(sf) library(tmap) ## Préparation des données load(&quot;data/chap06/DonneesLyon.Rdata&quot;) VarSocioEco &lt;- c(&quot;Pct0_14&quot;, &quot;Pct_65&quot;, &quot;Pct_Img&quot;, &quot;Pct_brevet&quot;, &quot;NivVieMed&quot;) Data2 &lt;- st_drop_geometry(LyonIris[VarSocioEco]) queen_w &lt;- queen_weights(LyonIris) ## Classification avec k = 4 azp5_sa &lt;- azp_sa(p=4, w=queen_w, df=Data2, cooling_rate = 0.85) azp5_tab &lt;- azp_tabu(p=4, w=queen_w, df=Data2, tabu_length = 10, conv_tabu = 10) skater5 &lt;- rgeoda::skater(k=4, w=queen_w, df=Data2) redcap5 &lt;- redcap(k = 4, w = queen_w, df = Data2, method = &quot;fullorder-wardlinkage&quot;) ## Cartographie des résultats LyonIris$SE.azp4_sa &lt;- as.character(azp5_tab$Clusters) LyonIris$SE.azp4_tab &lt;- as.character(azp5_sa$Clusters) LyonIris$SE.skater4 &lt;- as.character(skater5$Clusters) LyonIris$SE.recap4 &lt;- as.character(redcap5$Clusters) Carte1 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.azp4_sa&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;a. AZP-SA&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte2 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.azp4_tab&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;b. AZP-TABU&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte3 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.skater4&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;c. Skater&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) Carte4 &lt;- tm_shape(LyonIris)+tm_borders(col=&quot;gray&quot;, lwd=.5)+ tm_fill(col=&quot;SE.recap4&quot;, palette = &quot;Set1&quot;, title =&quot;&quot;)+ tm_layout(frame=FALSE, main.title = &quot;d. RECAP&quot;, main.title.position = &quot;center&quot;, main.title.size = 1) tmap_arrange(Carte1, Carte2, Carte3, Carte4) "],["sect1008.html", "10.8 Exercices du chapitre 8", " 10.8 Exercices du chapitre 8 "],["sect1009.html", "10.9 Exercices du chapitre 9", " 10.9 Exercices du chapitre 9 "],["sect1011.html", "10.10 Exercices du chapitre 11", " 10.10 Exercices du chapitre 11 "],["sect1012.html", "10.11 Excercice du chapitre 12", " 10.11 Excercice du chapitre 12 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
